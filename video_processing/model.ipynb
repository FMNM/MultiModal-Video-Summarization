{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import VivitConfig, VivitModel, BlipProcessor, BlipForConditionalGeneration, pipeline, AutoModelForSeq2SeqLM, AutoTokenizer  # BlipForConditionalGeneration, BlipProcessor, pipeline\n",
    "import whisper\n",
    "import yt_dlp as youtube_dl\n",
    "from ultralytics import YOLO  # Ensure YOLOv5 is installed\n",
    "from PIL import Image\n",
    "from google.cloud import vision\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./industry-project-uts-d68073cc0d1d.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"../video summarization/dataset/youtube.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download video using yt-dlp\n",
    "def download_video(video_link, output_path):\n",
    "    ydl_opts = {\"format\": \"best\", \"outtmpl\": output_path}\n",
    "    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([video_link])\n",
    "\n",
    "\n",
    "# Step 1: Download the video\n",
    "video_link = dataset[\"link\"][299]\n",
    "video_path = \"downloaded_video - Trim.mp4\"\n",
    "output_folder = \"video_frames\"\n",
    "# download_video(f\"https://www.youtube.com/watch?v={video_link}\", video_path) # Uncomment this line to download the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract frames from the video using FFMPEG backend in OpenCV\n",
    "def extract_frames(video_path, output_folder, frame_rate=1):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path, cv2.CAP_FFMPEG)  # Force OpenCV to use FFMPEG\n",
    "\n",
    "    count = 0\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if count % frame_rate == 0:\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_{frame_count}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            frame_count += 1\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "extract_frames(video_path, output_folder, frame_rate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess frames for ViViT (Video Vision Transformer) model\n",
    "def preprocess_frame(frame, size=(224, 224)):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, size)\n",
    "    frame = frame / 255.0\n",
    "    frame = np.transpose(frame, (2, 0, 1))  # Now shape is (C, H, W)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def preprocess_all_frames(output_folder, size=(224, 224)):\n",
    "    preprocessed_frames = []\n",
    "    for frame_file in sorted(os.listdir(output_folder)):\n",
    "        frame_path = os.path.join(output_folder, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        preprocessed_frame = preprocess_frame(frame, size)\n",
    "        preprocessed_frames.append(preprocessed_frame)\n",
    "    preprocessed_frames = np.stack(preprocessed_frames)\n",
    "    return preprocessed_frames\n",
    "\n",
    "\n",
    "preprocessed_frames = preprocess_all_frames(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transcribe audio from video using Whisper\n",
    "def transcribe_audio(video_path):\n",
    "    model = whisper.load_model(\"medium\")  # You can use different sizes: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
    "    result = model.transcribe(video_path)\n",
    "    transcription = result[\"text\"]\n",
    "    return transcription\n",
    "\n",
    "\n",
    "# Function to summarize text in smaller chunks with truncation\n",
    "def summarize_text(text, summarizer, chunk_size=256, max_length=1024):\n",
    "    tokens = text.split()\n",
    "    summaries = []\n",
    "\n",
    "    # Wrap the loop with tqdm to show the progress bar\n",
    "    for i in tqdm(range(0, len(tokens), chunk_size), desc=\"Summarizing\"):\n",
    "        chunk = \" \".join(tokens[i: i + chunk_size])\n",
    "        # Dynamically set max_length based on the length of the chunk\n",
    "        chunk_length = len(chunk.split())\n",
    "        adjusted_max_length = min(max_length, int(chunk_length * 0.5))  # Set max_length to 50% of the chunk length\n",
    "        adjusted_min_length = int(adjusted_max_length * 0.2)  # Set min_length to 20% of max_length\n",
    "        \n",
    "        summary = summarizer(\n",
    "            chunk,\n",
    "            max_length=adjusted_max_length,\n",
    "            min_length=adjusted_min_length,\n",
    "            do_sample=False\n",
    "        )\n",
    "        summaries.append(summary[0][\"summary_text\"])\n",
    "\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Advanced Feature Extraction using ViViT\n",
    "def extract_vivit_features(preprocessed_frames, num_frames_per_segment=3):\n",
    "    torch.cuda.empty_cache()\n",
    "    configuration = VivitConfig(image_size=224, num_frames=num_frames_per_segment)\n",
    "    model = VivitModel(configuration)\n",
    "    model.eval()\n",
    "\n",
    "    # Convert frames to tensor\n",
    "    frames_tensor = torch.tensor(preprocessed_frames, dtype=torch.float32)\n",
    "    batch_size = frames_tensor.shape[0] // num_frames_per_segment\n",
    "    frames_tensor = frames_tensor[: batch_size * num_frames_per_segment]\n",
    "    frames_tensor = frames_tensor.view(batch_size, num_frames_per_segment, 3, 224, 224)\n",
    "\n",
    "    frames_tensor = frames_tensor.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frames_tensor)\n",
    "\n",
    "    # outputs.last_hidden_state has shape (batch_size, seq_length, hidden_dim)\n",
    "    print(\"outputs.last_hidden_state.shape:\", outputs.last_hidden_state.shape)\n",
    "\n",
    "    # Extract the CLS token representation\n",
    "    video_features = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_dim)\n",
    "    print(\"video_features.shape:\", video_features.shape)\n",
    "\n",
    "    return video_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Scene Transition Extraction with Advanced Contextual Feature Changes\n",
    "def compute_feature_differences(features):\n",
    "    \"\"\"\n",
    "    Compute the differences between consecutive feature vectors across time.\n",
    "    Args:\n",
    "        features: tensor of shape (num_frames, feature_dim)\n",
    "    Returns:\n",
    "        diff: a tensor representing the difference between consecutive frame features\n",
    "    \"\"\"\n",
    "    # Convert features to numpy array\n",
    "    features_np = features.cpu().numpy()\n",
    "\n",
    "    # Compute pairwise feature distance (Euclidean) between consecutive frames\n",
    "    feature_diffs = np.linalg.norm(np.diff(features_np, axis=0), axis=1)\n",
    "\n",
    "    return feature_diffs\n",
    "\n",
    "\n",
    "def detect_scene_transitions(features, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Detect significant scene transitions based on feature differences.\n",
    "    Args:\n",
    "        features: tensor of shape (num_frames, feature_dim)\n",
    "        threshold: threshold value for detecting scene change\n",
    "    Returns:\n",
    "        scene_transition_indices: list of indices where scene transitions occur\n",
    "    \"\"\"\n",
    "    feature_diffs = compute_feature_differences(features)\n",
    "\n",
    "    # Plot differences for visualization\n",
    "    plt.figure()\n",
    "    plt.plot(feature_diffs)\n",
    "    plt.title(\"Feature Differences Between Consecutive Frames\")\n",
    "    plt.xlabel(\"Frame\")\n",
    "    plt.ylabel(\"Feature Difference\")\n",
    "    plt.show()\n",
    "\n",
    "    # Scene transitions are where the feature difference exceeds the threshold\n",
    "    scene_transition_indices = np.where(feature_diffs > threshold)[0]\n",
    "\n",
    "    return scene_transition_indices\n",
    "\n",
    "\n",
    "# Step 6: Apply Temporal Coherence via Agglomerative Clustering\n",
    "def cluster_scene_transitions(features, scene_transition_indices, num_clusters=5):\n",
    "    \"\"\"\n",
    "    Cluster the detected scene transitions into coherent groups using Agglomerative Clustering.\n",
    "    Args:\n",
    "        features: tensor of shape (num_frames, feature_dim)\n",
    "        scene_transition_indices: indices of detected scene transitions\n",
    "        num_clusters: number of scene clusters\n",
    "    Returns:\n",
    "        clustered_scenes: list of indices corresponding to clustered scene transitions\n",
    "    \"\"\"\n",
    "    # Extract the transition features\n",
    "    transition_features = features[scene_transition_indices].cpu().numpy()\n",
    "\n",
    "    # Use Agglomerative Clustering with temporal constraint (Ward linkage)\n",
    "    clustering_model = AgglomerativeClustering(n_clusters=num_clusters, linkage=\"ward\").fit(transition_features)\n",
    "\n",
    "    clustered_scenes = []\n",
    "    for cluster_label in range(num_clusters):\n",
    "        cluster_indices = np.where(clustering_model.labels_ == cluster_label)[0]\n",
    "        # Select the first frame in the cluster\n",
    "        clustered_scenes.append(scene_transition_indices[cluster_indices[0]])\n",
    "\n",
    "    return clustered_scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.last_hidden_state.shape: torch.Size([366, 197, 768])\n",
      "video_features.shape: torch.Size([366, 768])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAHFCAYAAAAHcXhbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAADGe0lEQVR4nOydd5wU5f3HP7P1CsfROTooLQjYUdSooCJYYk00JooaE5NoomI0MRpFk4ixRaPRFBXFnzWxRI2iIIrGioqAHaX3Xq7f7j6/P3afme/zzDO7s1fYneP7zot4N7c780x7ns/zbY8lhBBgGIZhGIYJKKFCN4BhGIZhGKYlsJhhGIZhGCbQsJhhGIZhGCbQsJhhGIZhGCbQsJhhGIZhGCbQsJhhGIZhGCbQsJhhGIZhGCbQsJhhGIZhGCbQsJhhGIZhGCbQsJgpIh588EFYlmX896tf/apNjvnZZ59h6tSpWLZsWZvsvyUsW7ZMuQbRaBRdu3bFgQceiMsuuwyffvqp6zuvv/46LMvC66+/rmy/6667MHjwYMRiMViWhW3btgEArrnmGvTv3x+RSASdOnVq+5MKOAMHDlTuSUlJCQYPHowpU6Zg06ZNzdpnMT+DrUkqlcLDDz+Mo48+Gt26dUM0GkWPHj1wwgkn4Pnnn0cqlSp0EwvCjTfeiGeffda13etdbmv0fof+O+CAA3ZpWxj/RArdAMbN9OnTMXz4cGVb79692+RYn332Ga6//noceeSRGDhwYJsco6X84he/wFlnnYVUKoVt27Zh/vz5eOCBB3DXXXdh2rRpuOKKK+zP7rfffnjnnXcwYsQIe9vHH3+MX/7yl7jgggswefJkRCIRVFRU4D//+Q/++Mc/4uqrr8akSZMQj8cLcXqB49BDD8Wtt94KAKirq8MHH3yAqVOn4o033sAHH3yQ9/6C8Ay2lPr6epx88sl45ZVXcOaZZ+Lee+9FVVUVNm7ciJkzZ+K73/0unnjiCZx00kmFbuou58Ybb8Tpp5+Ok08+Wdluepd3JbLfoXTo0KEgbWFyw2KmCBk5cmTgZwBNTU2wLAuRSMsfsf79++Pggw+2fz/uuOMwZcoUnHrqqbjyyisxcuRITJo0CQDQsWNH5bMAbAvOj3/8Y4wZM8be/sknnwAAfvnLX6JHjx4tbicA1NbWoqysrFX2Vax06tRJucbjxo3Dzp078fvf/x5fffUVhg4dWsDWFSdTpkzByy+/jIceegjnnHOO8rdTTz0VV1xxBerq6grUuuLE9C7vSvR+JxtCCNTX16O0tLSNW8V4IpiiYfr06QKAmDdvXtbPPf744+Lggw8WZWVlory8XEyYMEF89NFHymfmzZsnzjjjDDFgwABRUlIiBgwYIM4880yxbNky1/H0f9OnTxdCCDFgwAAxefJk1/GPOOIIccQRR9i/v/baawKAmDFjhpgyZYro3bu3sCxLfP7550IIIWbNmiXGjx8vKioqRGlpqTjkkEPE7Nmzc16PpUuXCgDilltuMf599erVIhqNinHjxrna8tprr9lt1c9v8uTJYsCAAa7t1113XV7XePLkyaK8vFwsXLhQHHPMMaJDhw7i4IMPFkII0dDQIH7/+9+LYcOGiVgsJrp16ybOPfdcsWHDBmUfAwYMEMcff7x46aWXxL777itKSkrEsGHDxP333+8631WrVokf//jHom/fviIajYpevXqJ0047Taxbt87+zPbt28Xll18uBg4cKKLRqOjdu7e45JJLRHV1tbKvJ598UowZM0Z07NhRlJaWikGDBonzzjsv5z2R7dW59dZbBQCxZMkSZfu8efPEiSeeKDp37izi8bjYZ599xBNPPGH/PdszePfddwvLssT69etdx/n5z39ub0smk6JTp05iypQp9ja/11+I/O714sWLxaRJk0R5ebno27evmDJliqivr896zdauXSui0ag49thjs36Osnz5cvGDH/xAdO/eXcRiMTF8+HBx6623imQyaX+Gvh+33XabGDhwoCgvLxcHH3yweOedd5T9ffPNN+KMM84QvXr1ErFYTPTo0UOMHz9ezJ8/P+9rIYQQ7777rjjhhBNEly5dRDweF3vssYe45JJLlOs1YMAA1/euu+46QYcd072XfYv+Lv/5z38WAMTixYtd+73yyitFNBoVGzdutLe1Vb8j233RRReJe++9VwwfPlxEo1Fx7733CiGEmDp1qhgzZozo3LmzqKioEPvuu6+47777RCqVUvYh36Xnn39e7LPPPqKkpEQMHz5cPP/880KI9LsxfPhwUVZWJg488EDjuJDr/RJCiJqaGrtPiMfjonPnzmL//fcXjz76aM5rESRYzBQRsmN/9913RVNTk/JP8sc//lFYliXOP/988cILL4inn35ajB07VpSXl4tPP/3U/ty//vUvce2114pnnnlGzJ07Vzz++OPiiCOOEN27d7df+A0bNogbb7xRABB//etfxTvvvCPeeecdu8PPV8z06dNHnH766eK5554TL7zwgti8ebN4+OGHhWVZ4uSTTxZPP/20eP7558UJJ5wgwuFwzo7FT6dy8MEHi3g8bl8jvQP89NNPxTXXXGMPkO+88474+uuvxUcffSR+9KMfCQBi5syZ4p133hErV67M6xpPnjxZRKNRMXDgQDFt2jTx6quvipdfflkkk0kxceJEUV5eLq6//noxa9Yscd9994k+ffqIESNGiNraWnsfAwYMEH379hUjRowQM2bMEC+//LL47ne/KwCIuXPn2p9btWqV6NWrl+jWrZu4/fbbxezZs8UTTzwhzj//fFs01tTUiH322Uf5zJ133ikqKyvF+PHj7c707bffFpZliTPPPFO8+OKLYs6cOWL69Oni7LPPzno/ZHuPO+44+7ncuXOnmDNnjujbt6849NBDlc/OmTNHxGIx8e1vf1s88cQTYubMmeLcc89VBHO2Z/CLL74QAJROd+LEiaK0tFQMGTLE3vbee+8JAOLFF18UQoi8rn8+9zoWi4lvfetb4tZbbxWzZ88W1157rbAsS1x//fVZr9mjjz4qANiDXS42bNgg+vTpI7p37y7+9re/iZkzZ4qLL75YABA/+9nP7M/J92PgwIFi4sSJ4tlnnxXPPvusGDVqlOjcubPYtm2b/dlhw4aJwYMHi4cffljMnTtXPPXUU+Lyyy+335N8rsXMmTNFNBoVo0ePFg8++KCYM2eOeOCBB8SZZ56pXC8/Yuadd94RpaWl4rjjjrPvvTyW/i5v3LhRxGIxcfXVVyv7TCQSonfv3uLUU0+1t7VGv/OnP/3J1Q/Ld0j2d6NHjxaPPvqomDNnjvjkk0+EEEKce+654v777xezZs0Ss2bNEr///e9FaWmp6zmR7/7IkSPFY489Jl588UVx0EEHiWg0Kq699lpx6KGHiqefflo888wzYujQoaJnz57Ks+vn/RJCiAsvvFCUlZWJ22+/Xbz22mvihRdeEDfddJO46667sl6HoMFipojwmqUCEE1NTWLFihUiEomIX/ziF8r3du7cKaqqqsT3vvc9z30nEglRXV0tysvLxZ133mlv/9e//qV0GJR8xczhhx+ufK6mpkZ06dJFnHjiicr2ZDIp9t57bzFmzJgsV8OfmDnjjDMEAHv2rneAQnhbvGTHSmdz+VzjyZMnCwDigQceUD772GOPCQDiqaeeUrbPmzdPABD33HOPvU1azpYvX25vq6urE126dBEXXnihve38888X0WhUfPbZZ57XYtq0aSIUCrnO89///rcy2EvrBh3s/GKyaAEQY8aMEWvXrlU+O3z4cLHvvvsqYlwIIU444QTRq1cv28qQ7Rns27evOP/884UQaWtLeXm5+PWvfy0A2Nfsj3/8o4hGo7b1ye/1b869fvLJJ5XPHnfccWLYsGFZr9lNN91ki2Y//OY3vxEAxHvvvads/9nPfiYsyxJffvmlEMJ5P0aNGiUSiYT9uffff18AEI899pgQQohNmzYJAOKOO+7wPGY+12LPPfcUe+65p6irq/Pcn18xI4QQ5eXlxn7G9C6feuqpom/fvoqF6sUXXxQAbItGa/U7pn+zZs0SQqTFTGVlpdiyZUvWfSWTSdHU1CRuuOEG0bVrV8U6M2DAAFFaWipWrVplb/v4448FANGrVy9RU1Njb3/22WcFAPHcc8/Z2/y+XyNHjhQnn3xy1na2BzibqQiZMWMG5s2bp/yLRCJ4+eWXkUgkcM455yCRSNj/SkpKcMQRRyhR/9XV1fj1r3+NwYMHIxKJIBKJoEOHDqipqcHnn3/eJu0+7bTTlN/ffvttbNmyBZMnT1bam0qlMHHiRMybNw81NTUtOqYQokXf18nnGkv0837hhRfQqVMnnHjiico+9tlnH1RVVbn2sc8++6B///727yUlJRg6dCiWL19ub3vppZcwbtw4fOtb3/Js+wsvvICRI0din332UY577LHHKlkhBx54IADge9/7Hp588kmsXr06r2t02GGH2c/lW2+9hfvvvx8bN27E+PHj7Yymr7/+Gl988QV+8IMfAIDSnuOOOw5r167Fl19+mfNYRx11FGbPng0g/TzV1tZiypQp6NatG2bNmgUAmD17NsaOHYvy8nL7Ovi5/vnea8uycOKJJyrbRo8erdyn1mDOnDkYMWKEEt8FAOeeey6EEJgzZ46y/fjjj0c4HFbaBMBuV5cuXbDnnnvilltuwe2334758+e7Mqf8XouvvvoK33zzDX70ox+hpKSkVc/bD+eddx5WrVplPxNAOmGiqqrKjptrrX7nkksucfXDBx10kP338ePHo3Pnzq7vzZkzB0cffTQqKysRDocRjUZx7bXXYvPmzdiwYYPy2X322Qd9+vSxf5fv95FHHqnE3snt8p7m836NGTMGL730En7zm9/g9ddfb7exWRwAXIR861vfMgYAr1+/HoAzGOmEQo42Peuss/Dqq6/id7/7HQ488EB07NgRlmXhuOOOa7OHuVevXsb2nn766Z7f2bJliz0INYfly5cjHo+jS5cuzd4HJZ9rDABlZWXo2LGjax/btm1DLBYz7kNPYe7atavrM/F4XLlPGzduRN++fXO2/euvv0Y0Gs163MMPPxzPPvss/vKXv+Ccc85BQ0MD9tprL1x99dX4/ve/n/UYAFBZWak8n4cccghGjBiBsWPH4rbbbsO0adPs6/irX/3Ks6yAn1Tuo48+Gg899BAWL16M2bNnY99990WPHj0wfvx4zJ49G2eddRbefvttXH311cp18HP9m3Ov9QE8Ho+jvr4+6zlIobp06dKsn5Ns3rzZmNUlMxo3b96sbNefH5mVJ58fy7Lw6quv4oYbbsDNN9+Myy+/HF26dMEPfvAD/PGPf0RFRYXva7Fx40YAyPksthWTJk1Cr169MH36dEyYMAFbt27Fc889h0suucQWdK3V7/Tt2zdrIobe3wHA+++/jwkTJuDII4/EP//5T/Tt2xexWAzPPvss/vjHP7r6Xr3fks+s13b5rOXzfv3lL39B37598cQTT+BPf/oTSkpKcOyxx+KWW27BkCFDPM8vaLCYCRDdunUDAPz73//GgAEDPD+3fft2vPDCC7juuuvwm9/8xt7e0NCALVu2+D5eSUkJGhoaXNs3bdpkt4ViWZaxvXfddZdnVkDPnj19t0dn9erV+PDDD3HEEUe0StYU4P8aS/Rzlvvo2rUrZs6cafxORUVF3u3q3r07Vq1alfUz3bp1Q2lpKR544AHPv0tOOukknHTSSWhoaMC7776LadOm4ayzzsLAgQMxduzYvNsnrQELFixQjnXVVVfh1FNPNX5n2LBhOfd71FFHAUhbX2bNmoVjjjnG3n7NNdfgjTfeQENDA44++mjlPP1c/3zvdXMZN24cotEonn32Wfz0pz/N+fmuXbti7dq1ru1r1qwBAOO7l4sBAwbg/vvvB5C2rjz55JOYOnUqGhsb8be//c33tejevTsA5HwWs/UdLSEcDuPss8/GX/7yF2zbtg2PPvooGhoacN5559mfaet+R2J69x9//HFEo1G88MILivA11dFpCfm8X+Xl5bj++utx/fXXY/369baV5sQTT8QXX3zRqu0qJCxmAsSxxx6LSCSCb775xuXaoFiWBSGEq27Kfffdh2QyqWzTZ3GUgQMHYuHChcq2r776Cl9++aWvDvXQQw9Fp06d8Nlnn+Hiiy/O+fl8qKurwwUXXIBEIoErr7yy1fbr9xpn44QTTsDjjz+OZDKpmKVbwqRJk/Dwww/jyy+/9BQBJ5xwAm688UZ07doVgwYN8rXfeDyOI444Ap06dcLLL7+M+fPnN0vMfPzxxwBgp7gPGzYMQ4YMwYIFC3DjjTfmbANgfgZ79eqFESNG4KmnnsKHH35o7+uYY47BhRdeiNtvvx0dO3ZULAp+r39r3Gs/VFVV4YILLsC9996LGTNmuFKzAeCbb75BTU0NRo8ejaOOOgrTpk3DRx99hP3228/+zIwZM2BZFsaNG9ei9gwdOhTXXHMNnnrqKXz00UcA/F+LoUOHYs8998QDDzyAKVOmeNZmGjhwIDZs2ID169fbwqGxsREvv/yy67O6FTIX5513Hm6++WY89thjePDBBzF27FilLldb9ju5kOUoqNuvrq4ODz/8cKseJ5/3i9KzZ0+ce+65WLBgAe644452VUqCxUyAGDhwIG644QZcffXVWLJkCSZOnIjOnTtj/fr1eP/9920F3rFjRxx++OG45ZZb0K1bNwwcOBBz587F/fff76pyO3LkSADAP/7xD1RUVKCkpASDBg1C165dcfbZZ+OHP/whfv7zn+O0007D8uXLcfPNN9uzs1x06NABd911FyZPnowtW7bg9NNPR48ePbBx40YsWLAAGzduxL333ptzPytWrMC7776LVCqF7du320Xzli9fjttuuw0TJkzI+1p64fcaZ+PMM8/EI488guOOOw6XXHIJxowZg2g0ilWrVuG1117DSSedhFNOOSWvdt1www146aWXcPjhh+O3v/0tRo0ahW3btmHmzJmYMmUKhg8fjksvvRRPPfUUDj/8cFx22WUYPXo0UqkUVqxYgVdeeQWXX345DjroIFx77bVYtWoVjjrqKPTt2xfbtm3DnXfeiWg0iiOOOCJnW7Zt24Z3330XQLqe0Oeff44bb7wR8XgcF110kf25v//975g0aRKOPfZYnHvuuejTpw+2bNmCzz//HB999BH+9a9/Acj+DAJpK8xdd92F0tJSHHrooQCAQYMGYdCgQXjllVfwne98R7HM+b3+rXGv/XL77bdjyZIlOPfcc/Hyyy/jlFNOQc+ePbFp0ybMmjUL06dPx+OPP47Ro0fjsssuw4wZM3D88cfjhhtuwIABA/Df//4X99xzD372s5/lXcdn4cKFuPjii/Hd734XQ4YMQSwWw5w5c7Bw4ULbcpvPtfjrX/+KE088EQcffDAuu+wy9O/fHytWrMDLL7+MRx55BABwxhln4Nprr8WZZ56JK664AvX19fjLX/7imkwBwKhRo/D666/j+eefR69evVBRUZHVajd8+HCMHTsW06ZNw8qVK/GPf/xD+Xtr9TvN4fjjj8ftt9+Os846Cz/5yU+wefNm3HrrrW1SkNPv+3XQQQfhhBNOwOjRo9G5c2d8/vnnePjhhzF27Nh2I2QAcJ2ZYsJvnZlnn31WjBs3TnTs2FHE43ExYMAAcfrppysph6tWrRKnnXaaXetg4sSJ4pNPPjFmKN1xxx1i0KBBIhwOK2l9qVRK3HzzzWKPPfYQJSUl4oADDhBz5szxzGb617/+ZWzv3LlzxfHHHy+6dOkiotGo6NOnjzj++OM9Py/RswrC4bBdI+HSSy9V0kX1tjQ3mymfayxrj5hoamoSt956q9h7771FSUmJ6NChgxg+fLi48MILlToZXnVb9GsshBArV64U559/vqiqqrJryHzve99T6rBUV1eLa665xq6vUllZKUaNGiUuu+wyux7NCy+8ICZNmiT69Olj1xw57rjjxJtvvmk8F4qezRQOh0X//v3F6aef7qpZIoQQCxYsEN/73vdEjx49RDQaFVVVVWL8+PHib3/7m/I5r2dQCCH+85//CADimGOOUb7z4x//WAAQf/nLX1zH9Xv9hWjZvTZl53iRSCTEQw89JMaPHy+6dOkiIpGI6N69u5g0aZJ49NFHlQyd5cuXi7POOkt07dpVRKNRMWzYMHHLLbd41pnRAambtH79enHuueeK4cOHi/LyctGhQwcxevRo8ec//1nJgvJ7LYRIp1RPmjRJVFZWing8Lvbcc09x2WWXKZ958cUXxT777CNKS0vFHnvsIe6++27j9fr444/FoYceKsrKyrLWmaH84x//EABEaWmp2L59u/F6t7Tf8VNnxsQDDzwghg0bZtffmTZtmrj//vsFALF06VL7c17vvmnfXm3y83795je/EQcccIBdi2aPPfYQl112mdi0aVPW6xA0LCFaOR2EYRiGYRhmF8Kp2QzDMAzDBBoWMwzDMAzDBBoWMwzDMAzDBBoWMwzDMAzDBBoWMwzDMAzDBBoWMwzDMAzDBJp2XzQvlUphzZo1qKioMJafZhiGYRim+BBCYOfOnejdu7drrTSddi9m1qxZg379+hW6GQzDMAzDNIOVK1fmXNy03YsZuajcypUrXasbMwzDMAxTnOzYsQP9+vXztThvuxcz0rXUsWNHFjMMwzAMEzD8hIhwADDDMAzDMIGGxQzDMAzDMIGGxQzDMAzDMIGGxQzDMAzDMIGGxQzDMAzDMIGGxQzDMAzDMIGGxQzDMAzDMIGGxQzDMAzDMIGGxQzDMAzDMIGGxQzDMAzDMIGGxQzDMAzDMIGGxQzDMAzDMIGGxQzDMIGkvikJIUShm8EwTBHAYoZhmMCxamsthv9uJn75+MeFbgrDMEUAixmGYQLHw+8uBwA8v2BNgVvCMEwxwGKGYZjAUduQLHQTGIYpIljMMAwTOGoaEoVuAsMwRQSLGYZhAkdNI4sZhmEcWMwwDBM4ahvZzcQwjAOLGYZhAge7mRiGobCYYRgmcLBlhmEYCosZhmECB8fMMAxDYTHDMEzgqGPLDMMwBBYzDMMEjmqOmWEYhsBihmGYwFHflCp0ExiGKSJYzDAMwzAME2hYzDAMwzAME2hYzDAMwzAME2hYzDAMEyhSKWH/HA1bBWwJwzDFAosZhmECBa0xUxaLFLAlDMMUCyxmGIYJFDUNTo2ZaJi7MIZhWMwwDBMw1BozwvNzDMPsPrCYYRgmUFAxI1jLMAwDFjMMwwQMumI2axmGYQAWMwzDBAzVMsNyhmEYFjMMwwQMtswwDKPDYoZhmEBRwzEzDMNosJhhGCZQVJPUbHYzMQwDsJhhGCZgNCSomClgQxiGKRpYzDAMEyjIagYcM8MwDAAWMwzDBA1ijmE3E8MwAIsZhmEChvD4mWGY3RcWMwzDBIqUYpkpYEMYhikaWMwwDBMohBIzw2qGYRgWMwzDBAzFzcRahmEYsJhhGCZgKG6mAraDYZjigcUMwzDBgrqZ2DTDMAxYzDAMEzDYzcQwjA6LGYZhAkUqxW4mhmFUWMwwDBMoVMsMyxmGYVjMMAwTMAQvZ8AwjAaLGYZhAgUXzWMYRofFDMMwDMMwgaagYmbatGk48MADUVFRgR49euDkk0/Gl19+qXxGCIGpU6eid+/eKC0txZFHHolPP/20QC1mGKbQ6HEyHDfDMExBxczcuXNx0UUX4d1338WsWbOQSCQwYcIE1NTU2J+5+eabcfvtt+Puu+/GvHnzUFVVhWOOOQY7d+4sYMsZhikUKU27sJZhGCZSyIPPnDlT+X369Ono0aMHPvzwQxx++OEQQuCOO+7A1VdfjVNPPRUA8NBDD6Fnz5549NFHceGFFxai2QzDFBB9PaaUEAjBKlBrGIYpBooqZmb79u0AgC5dugAAli5dinXr1mHChAn2Z+LxOI444gi8/fbbxn00NDRgx44dyj+GYdoPuiWGDTMMwxSNmBFCYMqUKTjssMMwcuRIAMC6desAAD179lQ+27NnT/tvOtOmTUNlZaX9r1+/fm3bcIZhdinsZmIYRqdoxMzFF1+MhQsX4rHHHnP9zbJUE7IQwrVNctVVV2H79u32v5UrV7ZJexmGKRRaADDbZhhmt6egMTOSX/ziF3juuefwxhtvoG/fvvb2qqoqAGkLTa9eveztGzZscFlrJPF4HPF4vG0bzDBMwXC5mVjLMMxuT0EtM0IIXHzxxXj66acxZ84cDBo0SPn7oEGDUFVVhVmzZtnbGhsbMXfuXBxyyCG7urkMwxQBKVYvDMNoFNQyc9FFF+HRRx/Ff/7zH1RUVNhxMJWVlSgtLYVlWbj00ktx4403YsiQIRgyZAhuvPFGlJWV4ayzzipk0xmGKRBsmWEYRqegYubee+8FABx55JHK9unTp+Pcc88FAFx55ZWoq6vDz3/+c2zduhUHHXQQXnnlFVRUVOzi1jIMUwzo2oVjZhiGKaiY8VO507IsTJ06FVOnTm37BjEMU/To3Yae3cQwzO5H0WQzMQzD+IGXM2AYRofFDMMwgcLtZmIYZneHxQzDMIHCbZkpUEMYhikaWMwwDBMoXDEyLGYYZreHxQzDMIGCs5kYhtFhMcMwTKBgNxPDMDosZhiGCRS8ajbDMDosZhiGCRS6W4mXN2AYhsUMwzCBgpczYBhGh8UMwzCBQrfEcAAwwzAsZhiGCRQuSwxrGYbZ7WExwzBMoGAtwzCMDosZhmECBadmMwyjw2KGYZhA4U7NZjXDMLs7LGYYhgkULjcTaxmG2e1hMcMwTKDQs5m4zgzDMCxmGIYJFFxnhmEYHRYzDMMECtYuDMPosJhhGCZQcDYTwzA6LGYYhgkUnM3EMIwOixmGYQKFLl7YMsMwDIsZhmECRSql/s5ahmEYFjMMwwQKt2WG5QzD7O6wmGEYJlDo2iXFWoZhdntYzDAMEyjchhhWMwyzu8NihmGYQMEBwAzD6LCYYRgmULhTsxmG2d1hMcMwTKDQ12JiywzDMCxmGIYJFK5Vs9k2wzC7PSxmGIYJFLzQJMMwOixmGIYJFLw2E8MwOixmGIYJFLp20WNoGIbZ/WAxwzBMoGDtwjCMDosZhmECBdeZYRhGh8UMwzCBwr3QJKsZhtndYTHDMEygcKVms5ZhmN0eFjMMwwQKVzZTgdrBMEzxwGKGYZhA4a4zw3KGYXZ3WMwwDBMoXAHABWoHwzDFA4sZhmECBVtmGIbRYTHDMEyg4IUmGYbRYTHDMEygcC80yTDM7g6LGYZhggUvNMkwjAaLGYZhAoXbzcRqhmF2d1jMMAwTKNjNxDCMDosZhmEChTubqTDtYBimeGAxwzBMoHC5mdg2wzC7PSxmGIYJFC5LDGsZhtntYTHDMEygSbGYYZjdHhYzDMMECnYzMQyjw2KGYZhAwQHADMPosJhhGCZQ8EKTDMPosJhhGCZQyBiZkJX+LxfNYxiGxQzDMIFC2GImrWZYyjAMw2KGYZiAkZYvUsywmmEYptli5uuvv8bLL7+Muro6AGzqZRhm12C7mTK9F2czMQyTt5jZvHkzjj76aAwdOhTHHXcc1q5dCwC44IILcPnll7d6AxmGYShy4hTOWGZSqUK2hmGYYiBvMXPZZZchEolgxYoVKCsrs7efccYZmDlzZqs2jmEYRkfaYThmhmEYSSTfL7zyyit4+eWX0bdvX2X7kCFDsHz58lZrGMMwjIlUxs8UyqQzsYubYZi8LTM1NTWKRUayadMmxOPxVmkUwzCMF45lRv2dYZjdl7zFzOGHH44ZM2bYv1uWhVQqhVtuuQXjxo1r1cYxDMO40FOzWc0wzG5P3m6mW265BUceeSQ++OADNDY24sorr8Snn36KLVu24K233mqLNjIMw9jItZksmZrNthmG2e3J2zIzYsQILFy4EGPGjMExxxyDmpoanHrqqZg/fz723HPPvPb1xhtv4MQTT0Tv3r1hWRaeffZZ5e/nnnsuLMtS/h188MH5NplhmHaElC5hmZrNWoZhdnvytswAQFVVFa6//voWH7ympgZ77703zjvvPJx22mnGz0ycOBHTp0+3f4/FYi0+LsMwwYUrADMMo5O3mJk+fTo6dOiA7373u8r2f/3rX6itrcXkyZN972vSpEmYNGlS1s/E43FUVVXl20yGYdop0s0kxUyKTTMMs9uTt5vppptuQrdu3Vzbe/TogRtvvLFVGkV5/fXX0aNHDwwdOhQ//vGPsWHDhqyfb2howI4dO5R/DMO0Hxw3EwcAMwyTJm8xs3z5cgwaNMi1fcCAAVixYkWrNEoyadIkPPLII5gzZw5uu+02zJs3D+PHj0dDQ4Pnd6ZNm4bKykr7X79+/Vq1TQzDFBh91ezCtYRhmCIhbzHTo0cPLFy40LV9wYIF6Nq1a6s0SnLGGWfg+OOPx8iRI3HiiSfipZdewldffYX//ve/nt+56qqrsH37dvvfypUrW7VNDMMUFt3NxEXzGIbJO2bmzDPPxC9/+UtUVFTg8MMPBwDMnTsXl1xyCc4888xWbyClV69eGDBgABYvXuz5mXg8zsX7GKYdYxfNk6YZhmF2e/IWM3/4wx+wfPlyHHXUUYhE0l9PpVI455xz2iRmhrJ582asXLkSvXr1atPjMAxTvAjbMiN/L2BjGIYpCvIWM7FYDE888QR+//vfY8GCBSgtLcWoUaMwYMCAvA9eXV2Nr7/+2v596dKl+Pjjj9GlSxd06dIFU6dOxWmnnYZevXph2bJl+O1vf4tu3brhlFNOyftYDMO0D9wLTbKaYZjdnWbVmQGAoUOHYujQoS06+AcffKAsgTBlyhQAwOTJk3Hvvfdi0aJFmDFjBrZt24ZevXph3LhxeOKJJ1BRUdGi4zIME1xcdWZYyzDMbk/eYiaZTOLBBx/Eq6++ig0bNiCVSil/nzNnju99HXnkkVmD915++eV8m8cwTDuG9hcyNTvFYoZhdnvyFjOXXHIJHnzwQTvLyFkfhWEYpm2hcx8nZobVDMPs7uQtZh5//HE8+eSTOO6449qiPQzDMJ7Qar8ym4mlDMMwedeZicViGDx4cFu0hWEYJitUuMiYGVYzDMPkLWYuv/xy3HnnnWzaZRhml2N0M7GaYZjdnrzdTP/73//w2muv4aWXXsJee+2FaDSq/P3pp59utcYxDMNQFDcTZzMxDJMhbzHTqVMnrvPCMEzBcerMMAyzu5O3mJk+fXpbtINhGCYn1ArDq2YzDCPJO2YGABKJBGbPno2///3v2LlzJwBgzZo1qK6ubtXGMQzDUEzZTClWMwyz25O3ZWb58uWYOHEiVqxYgYaGBhxzzDGoqKjAzTffjPr6evztb39ri3YyDMNo2UzubQzD7J7kbZm55JJLcMABB2Dr1q0oLS21t59yyil49dVXW7VxDMMwFGEIAGY/E8MwzcpmeuuttxCLxZTtAwYMwOrVq1utYQzDMDopJTWbA4AZhkmTt2UmlUohmUy6tq9atYoXgGQYpm0xLmdQmKYwDFM85C1mjjnmGNxxxx3275Zlobq6Gtdddx0vccAwTJtCC+Q5dWZYzTDM7k7ebqbbb78d48ePx4gRI1BfX4+zzjoLixcvRrdu3fDYY4+1RRsZhmEAqG6mMK/NxDBMhrzFTJ8+ffDxxx/j8ccfx4cffohUKoUf/ehH+MEPfqAEBDMMw7Q21ArD8b8Mw0jyEjNNTU0YNmwYXnjhBZx33nk477zz2qpdDMMwLkwLTXKdGYZh8oqZiUajaGhogCWnRAzDMLsQKVwsy7HMMAzD5B0A/Itf/AJ/+tOfkEgk2qI9DMPsQpqSqUI3IT8yRhgr8w9gNxPDMM2ImXnvvffw6quv4pVXXsGoUaNQXl6u/J1XzWaYYPDNxmp8567/4YwD++PaE0cUujm+kLolZFm2hVhwCDDD7PY0a9Xs0047rS3awjDMLuSlRWtR05jEA28txW+PG45IuFlLte1SFDdTZhtbZhiG4VWzGWY3pV+XMvvnBau2Yf8BXQrYGn8I283kqBnWMgzD8KrZDMNg7lebCt0EX9jCxcoIGrBlhmGYZoiZ5cuXY9SoUTjppJNw0UUXYePGjQCAm2++Gb/61a9avYEMw7QNVATM/Wpj4RqSB6lM1bwQyWbimBmGYXjVbKbo+GT1dvx34VokUzxItSW0PsuiVdsK15BmYMHitZkYhrHhVbOZomLKkx/j6Y/Sz9H//eggHDakW4Fb1H6hWjEl0tV1i72GlB0zo7iZWM0wzO4Or5rNFBVSyADA9rqmArak/aNXzg2CIUy2OZ2and7GWoZhGF41myka9Bk2l6lvW4J4vWULLdCYGYZhdnfydjP9+c9/xrhx43jVbKbV0S0DQRhcJX+b+w2+Wr8Te/WuxI8OG1To5vhCv95BuNzCyc2W/xeIdjMM07bkLWZ69+7Nq2YzbYIuXoIySK3YXIubXvoCQNpNNmFET6WGS7HidjMV/wWXAkxxM7FthmF2e3yJmf322w+vvvoqOnfujBtuuAG/+tWveNVsptUJ4uAKAHVNagxZbaM7pqwYCaJlRjqVuAIwwzAUXzEzn3/+OWpqagAA119/PRfHY9oEfVAKQkAqEFwRFsiYGbrQJMfMMAyTwZdlZp999sF5552Hww47DEII3HrrrejQoYPxs9dee22rNpDZfQiqKNDr4QSlPk4qFbzrTReaDHE6E8MwGXyJmQcffBDXXXcdXnjhBViWhZdeegmRiPurlmWxmGGajdvtEYxBSm9mQJptCLguTDvywbTQZBDazTBM2+JLzAwbNgyPP/44ACAUCuHVV19Fjx492rRhzO6H2+1RoIbkSVAtSu6A6+Jvt9NEyy7wxwHADMP4ipnZb7/9sHXrVgDAdddd5+liYpiWENTUbL2dyYC0O4gxSrLNIcu9jWGY3Ze8A4BvuOEGDgBm2oTgWmbU34Ng4QCCaVFS3EwcAMwwTAYOAGaKhvYjCgrUkDwJqiUMSK/LZHHRPIZhMnAAMFM0uERBQFSB3s7AZDMFsEghdTNx0TyGYSQcAMwUDWzh2LUEMTXbcTNZdjYTaxmGYfJeziCVSrVFOxjGEJAajFFKd4cFpNmBTM2mTQyFLNc2hmF2T3yJmeeeew6TJk1CNBrFc889l/Wz3/nOd1qlYczuRxDdHoA7eymobqYguPWkcAyFSJ2ZALSbYZi2xZeYOfnkk7Fu3Tr06NEDJ598sufnLMtCMhmMdWmY4iOw7pqAtjuIFiV5rS04VfMC0GyGYdoYX2KGupbYzcS0Fe4YjgI1JE+CalEKpgijFYA5m4lhmDS+6swwTCEIxuDqtnAE1s0UgOvtZDNZnM3EMIxNXgHAqVQKDz74IJ5++mksW7YMlmVh0KBBOP3003H22Wfb5cUZpjkEsbw+AOjGyiCIAsBQ16cwzcgLx83kxMwE5HIzDNOG+LbMCCHwne98BxdccAFWr16NUaNGYa+99sLy5ctx7rnn4pRTTmnLdjK7AUHMrgHcAcBBabc7Zqb4G263kdSZYRiG8W2ZefDBB/HGG2/g1Vdfxbhx45S/zZkzByeffDJmzJiBc845p9UbyeweBNHtAZiWYQhGu4NY10c2MWTRCsABaDjDMG2Kb8vMY489ht/+9rcuIQMA48ePx29+8xs88sgjrdo4ZveivazNFBwxo/9e/O22i+bBWWyy+FvNMExb41vMLFy4EBMnTvT8+6RJk7BgwYJWaRSze8JrM+1a3HVmCtSQfJBeJgu2nykIIoxhmLbFt5jZsmULevbs6fn3nj17YuvWra3SKGb3JKhuJj17KShF3IJYcVl1M2W2FX+zGYZpY3yLmWQyaVxcUhIOh5FIJFqlUczuiTsrqDDtyJcgigIgmPVxaJstdjMxDJPBdwCwEALnnnsu4vG48e8NDQ2t1ihm9ySolpl242YKwPW2k5mUAOACNohhmKLAt5iZPHlyzs9wJhPTEvRBKSiDVBADaYFgtttxM9HU7OJvN8MwbYtvMTN9+vS2bAfDuCq5BiX2JIgLNgLBzB6zs5ksLprHMIwDL2fAFA1BLZoX2DWltBilQGSPkYUm7ZiZADSbYZi2hcUMUzQEMYYDCKa7BghmrI+03qXdTJayjWGY3RcWM0zREMTy+gCLsF2JbU0iC00GQYQxDNO2sJhhioaguplcsScBaXgQl2GQLUwvNMnZTAzDpGExwxQN7tiTYIxSrqJ5wWi2a4HMIFxuQQOA7TozAWg4wzBtSrPEzMMPP4xDDz0UvXv3xvLlywEAd9xxB/7zn/+0auOY3YugWmaC6K4Bgtlu2WZaAZi1DMMweYuZe++9F1OmTMFxxx2Hbdu2IZlMAgA6deqEO+64o7Xbx+xGcMzMriWIFYClciFLM7GWYRgmfzFz11134Z///CeuvvpqhMNhe/sBBxyARYsWtWrjmN0L90KThWlHvriXMyhMO/IlkDEzZKFJJ2am+NvNMEzbkreYWbp0Kfbdd1/X9ng8jpqamrz29cYbb+DEE09E7969YVkWnn32WeXvQghMnToVvXv3RmlpKY488kh8+umn+TaZCQjtxcKhx9AUK+46M4VpRz6kbDFjsWWGYRibvMXMoEGD8PHHH7u2v/TSSxgxYkRe+6qpqcHee++Nu+++2/j3m2++GbfffjvuvvtuzJs3D1VVVTjmmGOwc+fOfJvNBAB9UAqIJjAE0gaj4UEUj4K4mextxd9shmHaGN/LGUiuuOIKXHTRRaivr4cQAu+//z4ee+wxTJs2Dffdd19e+5o0aRImTZpk/JsQAnfccQeuvvpqnHrqqQCAhx56CD179sSjjz6KCy+8MN+mM0WOO4YjGKNUUN1MQQy4pm6mUMY0EwQRxjBM25K3mDnvvPOQSCRw5ZVXora2FmeddRb69OmDO++8E2eeeWarNWzp0qVYt24dJkyYYG+Lx+M44ogj8Pbbb7OYaYcEMYYDcKeUB8XNFMTrLdsYYjcTwzCEvMRMIpHAI488ghNPPBE//vGPsWnTJqRSKfTo0aPVG7Zu3ToAQM+ePZXtPXv2tNPBTTQ0NKChocH+fceOHa3eNqZt0GM4AqIJDIHLwWh4UC1hgLrQJKsZhmHyipmJRCL42c9+ZouFbt26tYmQocj1VyRCCNc2yrRp01BZWWn/69evX5u2j2k9ghjDAbhjZoIqwoLQbtvNBIvXZmIYxibvAOCDDjoI8+fPb4u2KFRVVQFwLDSSDRs2uKw1lKuuugrbt2+3/61cubJN28m0HsFNzdbcTAFpeBDFox0ATCsAF3+zGYZpY/KOmfn5z3+Oyy+/HKtWrcL++++P8vJy5e+jR49ulYYNGjQIVVVVmDVrlp0K3tjYiLlz5+JPf/qT5/fi8Tji8XirtIHZtQQxhgMIpigAghm4LF2RFqkAHJDLzTBMG5K3mDnjjDMAAL/85S/tbZZl2e4fWRHYD9XV1fj666/t35cuXYqPP/4YXbp0Qf/+/XHppZfixhtvxJAhQzBkyBDceOONKCsrw1lnnZVvs5kAEMTy+kBwLUpBjJmhC02C3UwMw2TIW8wsXbq01Q7+wQcfYNy4cfbvU6ZMAQBMnjwZDz74IK688krU1dXh5z//ObZu3YqDDjoIr7zyCioqKlqtDUzx4LZwFKgheRLYonkBtCgJO5sJbJlhGMYmbzEzYMCAVjv4kUcemXU2aFkWpk6diqlTp7baMZniJYiWAiC4q327LGEp8+eKCafOjEXqzBSwQQzDFAV5i5kZM2Zk/fs555zT7MYwDCUog1RQ3UxBjFEShoUmOTebYZi8xcwll1yi/N7U1ITa2lrEYjGUlZWxmGGaTRDdHkCQ263+HoRmU8sMu5kYhpHknZq9detW5V91dTW+/PJLHHbYYXjsscfaoo3MbkJQi+bpgynHzLQdzkKT4ArADMPY5C1mTAwZMgQ33XSTy2rDMPkQ1JgZKV7k4BoQLWO3MxySWUHFj+JmythmgvKcMAzTdrSKmAGAcDiMNWvWtNbumN0Qd92TYAxSsp3RUPp1CsrgKtspxUwQrrdsYoisZ1D8rWYYpq3JO2bmueeeU34XQmDt2rW4++67ceihh7Zaw5jdD5fbIwDZNYBm4UgGz80UCVloRDAsSlKAWZyazTAMIW8xc/LJJyu/W5aF7t27Y/z48bjttttaq11Fz8xP1uLZ+WtwyOCuOGfswEI3p10Q1KJ5coCNhC2gKRiiAACSGbFou5kCcL3tonkWyNpMDMPs7uQtZlJBmS63MUs31WLmp+vQoSTvS8h44I6ZKVBD8kRaYiIBEgUAEWHSzRQAFabWmZHbir/dDMO0LXnHzNxwww2ora11ba+rq8MNN9zQKo0KAqXR9KWra/K/fAOTHVrdFQiOZUZqgEg4/UwEbaFJ2e4AaBm7zbTOTEAuN8MwbUjeYub6669HdXW1a3ttbS2uv/76VmlUECiNhQEA9Y0sZloLWxSE5OAajFFK2AHAwapI61zv4AUAp+vM8NpMDMOkyVvMyAUldRYsWIAuXbq0SqOCQEk0LWbYMtN6uLNrCtka/0gREA4HRxQApN22e6yQrfGHbGLIYssMwzAOvgM+OnfunJ4NWRaGDh2qCJpkMonq6mr89Kc/bZNGFiOlLGZaHd1SEJRYCJdFKSAqTATSMuO4mZxthWkLwzDFg28xc8cdd0AIgfPPPx/XX389Kisr7b/FYjEMHDgQY8eObZNGFiNlsfSlq2M3U6vhtnAUsjX+SeqBtAEZXXXLTBCut+JmstjNxDBMGt9iZvLkyQCAQYMG4ZBDDkE0Gm2zRgWB0hgHALc2QbQUADQ1OziBtACtMxOcGCW7AjDXmWEYhpB3XvERRxxh/1xXV4empibl7x07dmx5qwKAHTPDlplWI4iWAsAp7hekFGfAaXeQ6szYazPB4rWZGIaxyTsAuLa2FhdffDF69OiBDh06oHPnzsq/3QWOmWl99NiTIAyuAE1xDqZFKUji0XEzZZY0AFqsZhas3IbfPrMIW2oaW7YjZpeRSgms2uouEcLsvuQtZq644grMmTMH99xzD+LxOO677z5cf/316N27N2bMmNEWbSxK7NRsFjOthtsyE4DRFeqyAOnfC9ka/+gLTQbheks3U4i4mVra7pP++hYefW8F/vDCZy1sHbOruG3WlzjsT6/h9S83FLopTJGQt5h5/vnncc899+D0009HJBLBt7/9bVxzzTW48cYb8cgjj7RFG4sSaZlpSgo0JbkqcmsQREsBENz6OEEUYaIN3UzfbKpppT0xbc2Sjel7tZTvGZMhbzGzZcsWDBo0CEA6PmbLli0AgMMOOwxvvPFG67auiJExMwBbZ1qLIFoKgOC6mfTrHQS3Hl1oUtpmWqvd8Uje3SFTIOQSIkFZ1JVpe/J+e/fYYw8sW7YMADBixAg8+eSTANIWm06dOrVm24qaeCRkzww5bqZ10C0FARhbARgq6QbEUCc0ERaE601jZlrbMsNiJjhIEROUiQPT9uT99p533nlYsGABAOCqq66yY2cuu+wyXHHFFa3ewGLFsiyUReWSBgEZvYocEVDLjOMeC+baTOEAucfsbCbLavXU7HgknPtDTFEg3zE2zDCSvFOzL7vsMvvncePG4YsvvsAHH3yAPffcE3vvvXerNq7YKY2FUdOYRG1TotBNaRe4VnEOwOAKOLPEaDg47hrAtDZTARvjE7vODECK5rUO8ShbZoICu5kYnbzFDKW+vh79+/dH//79W6s9gYJrzbQurpiZgBi8grj6NGBam6n4G664mfSNzdqf8112MwUH+ewGpaYT0/bk/fYmk0n8/ve/R58+fdChQwcsWbIEAPC73/0O999/f6s3sJjhWjOti16RNgiDKxDM1aeBYFZcls9EyLKQeUxaZJlpSDiKmd1MwSGRzFhmAvDMMruGvMXMH//4Rzz44IO4+eabEYvF7O2jRo3Cfffd16qNK3a41kzr4s5mKmBj8sCVUh6Qhgex4rJsooV0ejZgFmEfLNuCe17/OqcborrBcRHHwlaWTzLFRBAtM3fOXox/vPFNoZvRbslbzMyYMQP/+Mc/8IMf/ADhsDOTGT16NL744otWbVyx47iZAuIPKXL07JogWAoAd8xMUPpXd52Z4m84XWhS+plMzT79b+/g5plf4j8fr866vxoiZhItvHFCCFzw0Af4zVMLW7QfJjd2zEwAnlkA2FrTiD/P/go3vviFIqCZ1iNvMbN69WoMHjzYtT2VSrnWaWrvsJupdQmipQAIctG89H/DtluvgI3xiby2fheaXJajqBodWKTrorls2NmA2Z+vx+PzVgbGRRpU5K1qrXqlN730Bcbf9jq217XNGEZF18adDW1yjN2dvMXMXnvthTfffNO1/V//+hf23XffVmlUUGAx07rYg6sVnIBUwO1mCkqGhTTRB8oyk/lvugJw7mwm+Rkvahqcd7ephRHnjST+pqVWHiY7ycy9aq0+4m9zv8GSjTU5LXnNhbrDNuyob5Nj6AghsGZbXWD60ZaSdzbTddddh7PPPhurV69GKpXC008/jS+//BIzZszACy+80BZtLFrKZMxMgLOZNlc3YFtdE/bs3qHQTQnw2kzp/0YDVHwOMFcuXrKxGuc88D5+cvgeOGfswAK2zowpmylbZy2fpfv/txRdy2M4ed8+yt9rWtEyQ5c1aUqmEA0XZ3bUzvomlEbDdvZdEJGXujUmDlSE0srurQm1zKzfRZaZu+d8jdtmfYVrjv8WLvj2HrvkmIUk76f5xBNPxBNPPIEXX3wRlmXh2muvxeeff47nn38exxxzTFu0sWgpyYiZ2gCLmf3/MBtH3Ta3KFagtbNrAhp7EqTic4C5zsxtr3yFVVvrcO1/Pi1gy7xRFpr0Ea8bDllYsbkWv3/hM1z6xMcu4aO4mVpomWkiYqiphcKorVi5pRb7/2E2Lnz4w0I3pUWkWjFmZj2xlMgJamuTLIBl5rZZXwEA/vDfz3fJ8QqNb8vMkiVLMGjQIFiWhWOPPRbHHntsW7YrELQnN9OCldvRt3NZQdugV9INiijQA4CDEJRIB/UwSYWPFHlGDw0AtpDbEhayLDQknPezrimJspjT7VHLTEsFiOJmKtLFZ+95/Rs0JlJ49YtgrzZtVwBuhRnP6m119s/1TW1z3+gzur4ZYua9JZvRq7IU/bsWto8uZnxbZoYMGYKNGzfav59xxhlYv359mzQqKEgx0x5Ss4tBODgxM5kNhW+SL5x6LUEKpHV+lt6GVAroXBYzf6FIsBeaRNo6AzjWGhMhC4q7Z2e9mkmiBgC3MGZGcTMV50Mwf8XWvL/z+dodeGLeCsWN1hKWbKzGF+t2tGgfLclmem7BGvz8kQ9tIbt2OxUzbdOXU8vM+h35uZm+3lCNM/7xLg6/5bW8vufHctme8C1mdPPsiy++iJqa3Xv5dVlnpj1UAC4OMRNMy0wQV81OGSwzKSHQqSxqby9GkW5KzZbjhCl+IhyylHPdWa9mq9AA4JYG7eoxM8WGEAJfrNuZ9/d++8wi/PqpRTj2jjdanFa8bFMNxt82F6fe83aL+k1nOQNgS02jYn3LxS8fm48XF63DI+8tBwCs2eZYStpMzNCYmTwtMytJCMD2Wv/ZVh1Lork/1I4IbgRYEVDSjtxMxTD+BnGtIMAduByEbCY6wNPrTWMGijGF1FloEsTNJDDzk3X41rUz8dyCNYrrIWTpYkYdjGsaqZuppTEzuy6bqb4piVtf/hIXPvyB7wH46w3VzTrWJ6u3AwCWbKzBk/NWNmsfQPo+Xf3sIgDpOMOdDf4G5lRK4Jn5q/C9v72DlxatBeC8Y+u212G/38/CuFtez7s9DRmX0pptzbPMbNhR7ztTSMlmyvO9Kidu0c/W+rdoVZS0aLUiF7WNCXy9IX8xvKvwLWYsy3KlOeZKe2zvtKeYmWKwJtgxMwGycABuERaEZtM20rWZ6CC8qdrpdFdtrcX6HJ33puoG/OXVxcrg0NqoC03KbcDvX/gMjYkUfvnYfCXFOmSptUiyu5nUc/tszQ6ccs9beOvrTb7atqssM0IInDd9Hu5+7Wu8/Ol6LFi5zdf35i1TXUx+RXevylL75621jfbPjYlUXoP/p2t24K2vN9u/+423vnfuN7jsiQV4f9kWPPlBWkzJvuGNxel7s2a7P2sHjWvqVJ52qa7dTi0z5kat2FyL5ZtrbFHy/II1GHPjq7jhhc98HVe3zOSTLk3dn7qYqW9KelrLvCwzW2saccnj8/G2z+da8v1/vIujb38DHy7P31W5K/At3YQQOPfccxGPxwGkF5n86U9/ivLycuVzTz/9dOu2sIgpa8PlDD5asRXrt9dj0qheANIda4+OcXTrEG/1YwHFYQXRK9IGQRQATruj4eC4x0yWGQF1QJeWmbe+3oQf3PceAODkfXrjjjPd9aSEEDjkpjloTKSwtbYR1524V5u0WzY7ZFlkoUmgX5dSO5CznlTkDocsZdB2WWYavC0zj72/AvNXbMOz81fj0MHdcratMUGzmdyD4sxP1uGuOYtx2/f2xvCqjvb2z9bswP+9txznjB2gbE+lBEIh94Rxc00j3lniiIJGn8JpW12j8ntjImW7yrNBB1N5La9//lNMf2sZYpEQHj5/DA7ao2vu42suEpk99ubijRjWswI9OpYYv/fNRseiJNfSkqI73xoqVKBXxNPDHxXfpokpff6P/lZP3Df5ANz0Urra/fS3lvl61ukzWNuYxAF/mI1/nHMA9h/QOed3m8h3P1vjiJlkSuCwP72GSMjCm78e5yoFQC0z9U1J25NwzbOf4L+L1uI/H6/BspuOdx2vpiGBhkQKXcqd+LlEMoUFq9IWulc/X++r3bsa35aZyZMno0ePHqisrERlZSV++MMfonfv3vbv8t/uRHNWzX78/RWYeMcbOOWetzxNdkIInHrP2/jZIx9h2aYarNxSi+P+8iYO+MNsLFy1rTWaDkA1fRbDAOxem6nwbfJDMN1Mzs/UEkYtMxszHf/i9c5z+uZi82zumfmr7VnvV+vzM0ULIXxbMuwAYAtK0bwhPSrsz3y00pk5hrSYmeoGPWbGO5vpo0ywrN/SC4qbyRAAfPUzi/Dpmh2YeIdTdPT1LzfguL+8iUffW4GH3l5mb5/5yVqMuG4mXli4Jutx9GNt3Nngef2TWpv8xpnQwVRaGF75NJ380ZhI4UOfQcV6u1MpYNGq7Tj7/vcx5sZXPYWJKZha9l35vmrUdSqf9VxuJiqmPly+BUD+LhzdCrW5phEzP1nr67telpmv1u/EpuoGrNtRr1jMJNRlTIXk+8u2ZD3e9/7+Dvb7/SwltudTIqL2KIKaZCZ835Hp06e3ZTsCiR0AnIdl5p9vLsE3G9OB0zM/WYeLx1e4PrOVPHg76xNKp3Pe9Hl4+6rxrhV+F63ajj/N/AK/mTQcI/v4E5XU9FkMVSJtUWAFTMxk+pogrc3kFTNDO85NO9MdJBU4XrEgz8x3Kqf26VRq/IyJ+qYkzps+D4s37MSrlx+JytLsQYvy6IqbSQjlfN78ShVc2SwzXnVmahoSdrBsbaO/oFfqwjCJsy7lMWyuSV/TpZtqMKhbOR5+Z7n99x2ZtqVSAj/9v48AAH95dTFOGN1b2Y8ulKRlJpFM4cA/zgYAvHvVUaiqVC0dTdq9o+3NBn0m7PouZF8NPtOZdQtSIpXCYjKh+3jlNuzb3z3jTxhikZpb/kARM8kUahoS9nUHzG6mpOH57xDPT8yY2huL+LMlUDG3eP1ONCSSiEfCttjW2+gc0/l5S02j/TzkioWTwuWR91ZgyjFDAQDvL3UEULEu7skBwC2gJPMw5mOZoQ+mV2eyfLOTJRYJq2byzTWN2FztVuHPL1yD/329Cc8bZnJeJBXLjO+vtRnyfQ8HSBQA7vo4xSAMcyHIo0ezmZKKZSY9M6PbvKxOdEabT/Drb59ZhHeWbMam6kYszbGOEqDXmcls09pFY1xSKaEMJDtcbibSbvJuLli1zd5ncywzJmtC/y5OjZBH3k2LGDoRCmXU2dzFTgmMIT3dkx2XKMgca/bnTu2YlYYimHrquXTZCCHw4fKtnoMcvS525V1yTev9Wnh0y4wQSs2fJzyCi+m1NIkpLz5ZvR03PP8ZfvvMIvv5pMG3TSmhBIADZsuM6fnP1zJD91ESDWW2+fsuFdmJlMCGTGo3jV0xWQKT5Hsmy00u3vjKeQ7fW+q4NVu67EdbwWKmBchy4Pl03vShbvSoRbFii9MRJVPC9eKaXmQpjPIpyU5ns8VgBdFjZoBgCAPZsQepaJ4xZkYIZeAwWWa83EGK9UZ7BrfWmDvSjTsb8PRHjkWHrrfjdd+VhSbJqtn0+NRtkEwJZSbpTs0mbibSSc9fsc3+2a/lVc1mcl8n2sZVW+tc35HnTzOG4obZu8vNlPkedVOZUjP0fkqKmRte+Ayn3fs2Ln1ivuFb6vdkG+k19WuZcbdb7dueX7DG2LeZssSMlght2w/vfw8PvLUUj763AnMzA7NumdG/k0vM2JaZPNOe5XM7sGsZJmeWCUl6iIKnP1qFO2cvxtvfbMq0U22jvB4fETFjei/pu7zF8A6WGpZuoO/dxyu32V4BGjze0mU/2goWMy0g0owYCdrJeQ0Myzc7YiaREq5OyCSeklleci+KzTKjr+JMtxUzrlifADSaipmQ3W61g5UxM+rM3HxuXtabe17/Gvv+fhaene9ewE933yRT6cFk4h1vYuR1L+NHD85zWRMcN5OTXZkSqmCpJvtNCrU91VlSs+l50oHCr2WGTk7MM2U6KKY8v0ODVE0WnqaEuq0pKbCpukEJCjb1EXp/05BIYtZn6zH9rWUAoGQaUWifJYU6FewNPt1VeruTKaG69hqTRjFBPyOf21yiJ5USSpyIY5lx4kASSeG6TybhSs9VHpe6mfzEHsnvhUKW3U+Y7tHKLbWY8uQC/Hn2Vzjrn+9h3fZ6owjcVN2AZdo44XVMwLHMUC+C7obUvwOk3UuplFBWEy/GGkoAi5kWkSvgM5FMKR1T+rPOz14PxTLiZkqm3LMHk6LPNmPxgu6mGCwgwmCZKQaLUS6cdgevArBlqTFKNK5CzmKTmpnb9KzQQUE+1/VNSdw880sAwP+9u9z1Hf1ZTaRSWL65Fl+u34maxiRe/WIDvt6o1kZxspnoQpNqZ06bl9Jm/3rMjBrnIjLfF0o8gl83Mn2fTRlG6t+F6/iynYolzCAUdDN/IplynZdJTOnbGhIpLCIJBQMMpfJ1a53tZiLb/AcSq+02WZ3NIswt+Ez9gmJB0v4uv7dRcTOlXMczWmY0MS+EQDkJrqUDvRdSbIcty4lRM5yrXlBvc02Dq41NyZSrZpBpLKHf21qTbiNdusG0DpV+rDXb6nxNposBFjMtwBYzHqPXr/61AAff+CpmfeYs+5BMuTtPnRVUcSdNbib3d+R+83Fx0M8WgzXBjpkJmJiRly6S43koJqQgCVmWvSxASqgdtzRN651XLhO//JkGBe/Vu2PW78jfdfcMjWmh7abZTF5tAjJxQLRoXoOeHuy2lizdVKME4fsNAG5S1mbKYZnJvMS5XChG94EmcJpSwmXBMru51G2NiVRe1iTAvMCjtMzUNyXx2ZodnoO73u6kweqcy+KSzQJtSiHXf9+guJmEa2JYZwoAFu590S07fIgZuY9wyLItzyZR4EpfTxrubVK44i3N76Q7ZoYuKJzrWqd/d7+TxbruGIuZFpDLMvPsx2uQSAn8eMYHdhnqXB0VACynMTPC5GYyRdyn/5uPKNEDywqNOWamUK3xj7ynwVrOIP3fkOUEngohlNmztC74mT3TDl9ad6hryXRFTDM+/Vi6kDAGAAt3h0vblc3NpAqM9M8fZeJlqjJ1T5qVmp0jZkYeyzRQK1auHJaK9OdTrm3mgcptmcmnzQBxMxliZn5w33s47i9vYv/fzzIWVjO10WSdc7WBWkYy8VSm7ipb1p38XY+ZccURGSwzep+qP6e6ADFhu5ksy+4nTPdIF4JNhnubSKWMokMnYZiYyFgteR7ZvpP+3X2NvGI9Cw2LmRYgBwEvMUOzFx6bt8L1WZOYqW1MKC+c3wBg2zLTTDdTMYmZ4Flm0m10iuYVsjX+cAJpLTuQVs9mkjOw/C0z6e+pac+5Z/3JpFu460LCrgBsqRWAPS0zKTVt2+WOMbyPciA+bEi6UF5Dwu3qNUE7eVOmovLuy5iZhFtMKJ/z5WYyzJ59XO+GpmTO2jj6foyp2Rk30+eZGiiJlDDWw9Jdb821zHi79ckz6BJOKQghNDeTO2bGHLOTXYT5ETO0b5PjhtEyo4mZRkOQclNSGAVttnZLy8xqLTg+23fk7yaBU4ywmGkBuQKAo2FnUN4sgylziJnVW9VS8AmD6d2oqLOYX71QZtM+g/jaEt1dQ7cVM7p7rBjij3KRst1MjihPCXVASAl3zAng9fy53acmYUQx7TeXZUb+2cr8D0hff+/AZNUtq6dmmzJV5MrSh5Gqv36qfKvCJPugnNUykyNJwO1m8muZ0dxM2qzfPFPXBIgpALgpPeBS4WkKpNWPr2eapY/nQ8x4FtfztjIlUgJ1TUlFUJmsDrkCgOW+6DZdgJiQhw2FrKzjxnYthTqRFK0iXqX1jMbk+HFFGt1MRdops5hpAbncTOrsJZPSSF6CxoT7ezXaTDSZFC7rRLaZcT7xGrQjMZmzdzXO2kw0m6nw7cqF7h4LQgVguiyATB5LGdw16SDJ3D5zPUgSyG72N21LCfcs0B0zk/6vaplxz/Dttgg9ANgZeIT2NxncKQPw9+nXyf6bH1dTkzZQutpimMiYBFDOmBnXTNkUM5PbhdDQlMrZZreLQ2RS58l+EklDvZbsAdCAlws9+3OS1TKTcj+D9t+SwpgFJs9Z9uWmduuCSxdhfgKAZXvCVvZxw+RmcqVmG91M2YWJ/JmWItCtV4CHm8kjNbzYYDHTAnJmMxmUcS7LjD4DNJv5Wskyk2PmvKsxWWZE4ZuVE1vMBMjNRH34TsyMeRDw4+ZMGJ4lUzxKtv34ipmBY1Gyt2WxzOhupoZEyhYQXnEl8u9l8bCzmGyeYsYUV2AUM0bLDP1cbguLycJgdjuo29wxM9ktboDZUteQSLlikUyxJyZhpAfgGjM1FeuRt8vPVCmY/t6Q1PtW57rJVOu6pqTLsqpfSvo9wG1NMUHdTNliZnQrT1rMuC0zJkGrY5pgKEUifVjvmkyWWY6ZaX/kymbS/cqplDqjMZn5dDHjP2YmvS0fS4afMvW7En05A7qtmJG3MUjF/mjxObrNPOvPPXs2uWuU+iQ5vpP+3W0F8rTMwFLdYz4tM4ATy2P6TkMiaYvRWDhkp6/WNuXOaNJdGDqm982UdpzbMuMecEy1SNzfU7c1JpK5xYzBfWUqvqev3Gxyy+lxRCnDvkxtUAWf92rbTYbB2/ndEEhLhLpaNya7wNItM/7cTM7kwakz4z4RPf6mKSlcVvN0ULB+v/09b/Q++XknEz6PVQywmGkB1DJjrL2huZl00aObPQG3mTORcn8v24Pb3ArAxWA6lJcrFNAA4AiJkSp2V5NsHg1I1AOAgbRJ28/MzJSpQx9T0/PlxwrkzmZyRJgjxLxdD7plBnBcTabvUHdSLBKy11/z52bKPjlQLAeZc9XdXPp3/bmZ3K4As/VMdamkrVTq8fV+zOQK1K9nQ1PSFVjtx82USAmXq8NcH4eKGXd/6OzPWzwnUu505nQgbXqbIma0tptq1tBtvurMUMtM3m4mt0vJ7frJ/k7KY9H3yY/gNRVt9SopUmhYzLQA1YLg/rseM6M/vKbCWnoBKtMAYxrg7TLj+dSZyeEG2NU4axyBZNgUsEE+cWJmglO5WK0z41g4TCsy+ysNIFx/z2WZ0fdjskLqMWTyrzQLK3sAsHufcuA1deZUtESJZcaXm4kMlqZsJnq8xiwzXtUyYxpwmhekKfclC77pbibT91wBwB5uphrNMuMnANivZUavQO1lGcgmJpNG65XTJ5dEQ7bI0Nuud9O6dSqf1Oxc2UxSzHTMrP1kTM1Ous/F+H4ZrG7qWmR+AoDdwqkYQhJMsJhpAWEyEzfXR3C2NSSShhQ7HzEzJjN/lsCt5sbMFINlRglKJbVPih15GYNUudi2glkk/sQgCpoMqaG5hIkxkNWPm8kg3HURIf9sAU42k0eb5D690r3pd6QwkoOyZaXvZ2lmIcS8A4BzCb5kypiqrO/Hj5spYcpmMg1UmhWi0SBmTGnAyt+F281T35T05WYyx8zoz5bhfDVR7OXlyFY0z2SZoUI9Eg6hJBMf5Xb1uwVfS9xMWWNmMvE33SriALyEqvt+57J8yp9zlUswupldSQHF2bexmGkBimXG8ILpqXGmstQ6euedb8xMPgYW1c1U+AeU1j4JBcQyQzs16mYqfjHjXGu6xpHeUfktDWCy8qlm7txuJlM8hj7jN7mZhEGwSFIpd/qvHKzkeVlWOj4GcGbl0XAIlmWhLCrdTPnGzGQ3+5uquLYomymPe1SWETMNWsyM6XimAGBTILEeAFxvsEz5qTNjTs0mz5HwTs02VXOmx9KPT60O4ZBlr2btxzJD79G2PAOAZQVgl8U95ayB1K2DFDNml5Kv7DXD9aDPsa+YGcOSD2yZaYfQ4m6mF0yPmfFTU0HvBPSaBvp+9W1eK7GaMLkGCgmtfUIH2GKGti8aDo6bSa0z42xzzUINlhm/iyjSeAiTWPYXM2N2M4VoBWB4P/emwU8OVvJYkZBl3zspnuKZ3/NxM1FxYnIh00HAlF7b3GwmYxG1bG4mW8zkrk9jcjGa+iM5CEfsFGeDZcZQgt8UXEsxpc97VTnPlprdlEy5jk+FeiRkeVpm9D5Iz6hat70+pwXZrjND1mbSr211Y8LuN7oTMWOu+Ox+T93HJNcjKTLilb6nhhgpkwvTR3xOMcBipgUoYiZHZ92QSLoeXlOH19Jspua6mYohZsZxfVjKAFvM0MsdpMrFJpdeSrifgyYfMTPpIFvnd9OgnCudW37GZZnxEQAshHnwTu/TXSekXhMzNF1WipZoJN015hcAnJ9lRk8AMFlmci1Ymf5efgHAHeLpczK5mXKl/KY8sok2Z8rlS4uCKTXbz/3OFWxqCkh12uqdmeVlmbEFreJmym71SGoTzIZECpuqs1tnnLWZvEt6yCVvSqNh2xXoFQ/lznByi0A9m0nPDATcky6XZTbpzjAshpAEEyxmWgB1M5ksM/oD7y9mxjB78DHrktvyMbAUX8wMtRY4tU+KGSpalJiZIjfNOJYZp2ieyV1jquthinWh+K0A7J7xOjN1afKv9UzNdv5fCO/Zuqkuii5mIqGQHbwtA45jumXGRwVgddabK0hauIL9ZTaRXzeTrDBudDNliasri1HLTHbLiKkCsLzfpgrnXTvEAPisAGyIkco1cKaDvXMHAJssTMbg9qRjnSv1sswY9qXvny4TYMJeNTtk2eOG/q7JQOLK0qgtrr3cTG4rU/b7mEwJl8vW9D2XZcY0/hTBxNcEi5kWEAo5GRWm2apSJbPJkJptcjMZXiQ/haVaump2MYgZ+f5ZlpouXMzQ5kUC5WZK/9dSXHrNMzN7iZ1cC5m6rQmOf76iJArAZJmB3fCQwTJDi+kBcvBVt9VrBSzDIcsemOsyx4tG0r+X2QHAuWNmlKJ5mtXFJBR1a4/JpWfK1JLHkYOvyRWRLZDWyWZKur5nqmGj7tdJp46GQ/Z10y0zptRsU4xQrpgZ08Bpqpye3p+3ZSxpjFFKKc+AFNAm67j+u94v6cvQ6NAA4LAdAKy2R7rqOpVFbbensaaMoUhiLjdjIiXsd0nef9O5meJjcgnOYoHFTAuRKlu/vyaXku6KMq2HpM/WTDEzJt3hFCrzP4oW60KTIWXxwwI2yAdJL8tMkYswxTJDhKOfzixXB9iUTLlWNvYTbJhMOdsqMqmp7mwmR7RIESbIvqT4sD+f8g4ApjEzciasW2bycTMpAcA5ZsqmfZrcLoApRialtC1tvfUREKrFzDSSashe7dT3myKWmbBlIR5Jt2FTxjLjiBlvy4wUQKmUu86MS7gZBk6T642en/E8UsJVlbkp6UwUo2EnZsYVAGyoM0MDhwFg9bZaY5sk5joz6me21aUFYWVpFLGMm5NmXMUj2QRO7gmGtMx0LI063/MhJv2s0F0MsJhpIV7VHF21GJqSbsuMsQKw2wrj9tl6m7DzcW8UrWUGCIxlhrZPEWFFrsJUl156W9oyYzDPG1JDKaYByY9L1fRcy+N39LLMZP7rLDOpWj1KyawTyB4ATAekaMbNJAWGnBmXNXM5Az+uYWntsQcuw3UD3IO3Uy/GqUViiutwt08VM+Y6Mz7iRaSVIWTZA+zmaplSnHYzZUvNLok4IixXcoPpuTHF4+ifdffHbuFGU5zDIe+YmWyWmf5dygD4t8yESQVgvR/fUeeIDSl4aD0i+WybKj6b0sf1c5UxM5VUzLieU7dlji0zuwnywXRbZtzpi/oD58fNZKySmWXWlU8AMB1wi8EPag+wIRAXQuHblQ26dpS+AnUxI9un1/SRz1aU+OxzW2Zyuw+MlhnDQCYHCTl7rG9Sj2/HzCgLTVLLjCZmjDEzKaVN1DJTK7OZmhMATNwfXjVkKNIKJN1FnpYZV+VazTLjc/acsN1M2cRM9pk6HcgjVMzUpC0zMgvHlJotj1WSabcpnskrc45aPb0sA1mzmYgAkNebZs9FQ1ksM4bnXW7rJ8VMjpiZpN23WXZ8ln6t5XFLo2HFzSSvgdNu0+KT2S1q1DJTURLxDI8wFchz77s4OzcWMy3Er2UmHfCX/ox8L80dbe4OztQxyg6muWszFUMhJLXOTFBEgWqZCQfFopSS15pWW3YCSe3ZczNiZgD3+jbG1OwsdUdkBVRAjVehFiW7aJ7I4mYyBJnqdWbCYWeAqW3SLDN5FM3LtjaTSaTI+Bx7kNLcRSHL/F05KJfZbiZTNpPBemtbZmgF4BxuBlKLJ71f53kIhSzENWuGdDM1Zql4LmNTzKXyzcJNCo30fsz3Ims2ExF88ro1JVO21TEcslAaVdPzJfq7TPvtARkxsyqHZcYOAFYsM+bnsiQaUtxM8nNOjJSTmeV8Lvf4IwvmlcUitiUy58SEuOJixM1VjLCYaSG2ZcZHLRhpqqYvpv5gyE5BmoLpbDXbvpsTM5Mr22RXQ9OFg1JnhloXaEB4PhayQmCyzKSEM+CVkIEy35gZwB375Sc1m7pZymJh+92iQkJ+wwJZzoCszVTuyzLjzmaKapaZqCubyUcAcMJ7MKXvl7RmyPOi1iQ5SIUs2PEo7jWF1AE+vRhhbvFkBwBLN1OTUzTP8hROQmlzkqzNlI6ZUYcQKWYA9zOgW0ZMdY3c7sn07zFyHNNSEfSzpv3QCsBlceLmkpafsIVB3ToAAL5ct1P9rsEyntTdTLksM5kmh0JkoUltvw1Nzvggn8dGIrhKom7xSgWO3kaKEE713w7xiGcb5H6kkG4irriSiNmiVCywmGkhXgWQ5MtEVyXWzcqAQcxkOgBZC8KvZaY5MTNFu9AkjeMofLOyQgNS0/8NRkq5eW0mQTpOOQtzrDV2KrDHAESfdX2xPpOf3TTjtd0KZF0kOlO23Y7kWOlsJtX14uzTfRzHMpMZlEOWnYkmBUZMczP5KpqnZDOZr1GIVBuuNUxu5GQmEgop6bkUObhkt8wYxKMeAEziMcqIC0PdT/p3KayoayhMLDMSmZpNz8Vud8YNVxL1tvp5WWoiJHBWt/o5381imUk5y0eURSPO8YmgHd23EgCwcPU29btGy0z6Z+lm2lmfwI5672UNaJ2ZiMcEWLrmSqJh21JIswnLiFvRft6JwNHbqCOzpcrjYc/FLhOa4FQsQ+T4xUhRi5mpU6fa5dblv6qqqkI3S0EOBF6z1RhJX5RmZdp56Q+GbpmhqZBe30lvy8QB5JOaXawxMwFMzZbt9bLUFRvy1lsWjU9yngnaScpt8Yi5M5N/l64pwJyVp2MOLnUGrzJDvAq1KPmJmaFuJn3JAiVmRrMC6ZaZ/NdmMvcHkbAjUqT7jAowed0iYctur1f6tHSBpUWBLjj1mbkzcJcTgSY/RjOj1GNJAeJYZqgI1C0zlaVRu916/J9uUTLVmfHqR6PhEEIyKNaHZcZUG0a3zDSRgnDhkIWRfdJiZuWWOmypacy6L/mcdiyJ2MJ3R5Y1mkxuJv1a226mSMgu2kir/doBwEknM6vMQ2AkyLMtkWKmLBax08O9ntMSkvafcFkCi3OGWdRiBgD22msvrF271v63aNGiQjdJwUvh2ubLkNMpyWjyaNh5oPUHQ5oaZZCe0TJjKtCX+UxeAcDUMlMEJhAnZkYdYIsZmuIMkNiCIm+4mgbvPMPO4OV0nHpqqKdwJwObq/ijMWbGvR86UJYb4lXkN2jGG10gU4+ZSRKXgBzEzHVmZNE8NQBYXgcvawA9Dr0sXjEM6tIJbjeTtGjRNuWKI2lKpuzBzYl18nY7yYkSXU/Ja9Yt9yPvbYpYukxipkNJBHGPei2NupgxrfvjYZmJhh3B6RUArBcl1P/mjjUSyqBfWRrFoG7lAIBFq7dn2Zcj8EMhC1EPl43yHSUAOHvMTDwaRowWzdMERtoS53Y1UuTf6f2R2VId4hFvy4zrWCRTkMVMy4hEIqiqqrL/de/evdBNUgh5PhROpyRNsXImFiJFuvSsB/lAdyiJ2Ptxm+PdD1OiGWKGvnzFYJlRZ93BsHDoLpagrPZti5kQjHE+NAA4aejgKHZGSNjbFWBcVT7LIBEJWbb4qDEEANPA5SSpaWOyzMhZsRRHTswMcWHYMTPOhAPwDvDXyZVNJP9ORUqtwVIrr1skZNmF+3K7mRzLTImHhYW+3/I6VJPr6uWucISs4/amLrN4RL3e5bGIZ1aQEzMjRbH67KTbaRZukXDIvheNCbOVTF012y2mbcuMbdFKkZiZdJtGZawzC1dus7/rillMOs8bdVFme0aMlhmPeEnqZmokVjfq+nHXGvK4b+TZctxMEc/JdELfL8mcou7BYqToxczixYvRu3dvDBo0CGeeeSaWLFmS9fMNDQ3YsWOH8q8t8bTMELOyHvAXIXUt9E5PdgDUzaSny5pM9s2xzKh1Zgr/gJpK7Be7mBGkU6P/LdIYORvT2kxUWMejTgfttsyYhUqYBDfqtUBMHaApsN2u5BuynEwiw5oy6YUm3e+e2c2U/rnctsyodWZoumxtk1r7Rb6nuTpw07o/FGMaeKY/iEdCtjCTbqZwKOTZR7gsDMR6ZruDyDVpSCTt1GnAsVDRy+81UBkDgJWYGWcIKY2GtUq6ZuucaplRY3K8RFiUihkPywDNyJT7oSnIRhGouWOcuBnHMqMLrnSfTMVpdosR3Qd9R7JlM6luJtUy0mjY5jXBiIWpZYbGzJitrLpbkQaX29uKwIpvoqjFzEEHHYQZM2bg5Zdfxj//+U+sW7cOhxxyCDZv3uz5nWnTpqGystL+169fvzZto7dlxm2KlQHA4ZDzsHplM8kAYCU1LvNg6uZ56g/PZ/BX6swUwQPqDLBBqteiupmCs0Cm7OydZQHos0izJOyOMWIeXNWsoPRn3JYZwwq9xpgZZ3ApNwQAU0sYDTiWmNxM8lzlBMFUZybqssykzyPiY6AC3LVgssbMaPE5sYizrcEOAM7tZiqlKcbSMmMQBcfc/gYO+9Nr9u9yEUMJPZZXgU7qYnT6tpASJ1VVWaK0gQralMFdQS0zMQ8XpsnN5BUzo1pmhNKWZMopjWEKpA3bYqYTAGDRKreYoYJLdpdhy/IUBhTqZvKMmUk491B1M7mtJe5aQ+bnL0KumwxQLo9F7OfaSwQpMXOuOjfF2bcVtZiZNGkSTjvtNIwaNQpHH300/vvf/wIAHnroIc/vXHXVVdi+fbv9b+XKlW3aRllXxBTxDsjCUjLgLt0ph0NqUTKJEMLOZlJiZuTsyEOF01+bbZnJEROwK1BjZoLirkn/Vw6sNP6kmKGuAtsyQ54BZ6bu+Ofj0bDyXQkV7rKjlrNMZWV5r9kjGciSdKC041WcQVEKm/JYBAYt47bMkAFTDuLyHfOTzeSVwaWTj2XGiZnJWIGI1UEOuOFsbqaEe1Zup+rG1HskhMCKLU6pfctS3Vrpc3TElHuhR4ObiWTmUMvMnt07KG2oJ/eNzuZNMTNxj7RfefxIyDI+p6bPAiB9prNfPXC6ibg15X3eq3dHhCxg3Y56bNhRb58z4PWcWp5ZZxTqZpLixyvLzu1m0tw89H57CAzqQpXPFs1m8rIOudLAk84K3fT4xdgvF7WY0SkvL8eoUaOwePFiz8/E43F07NhR+deWeD0UtrncckyximXGkKnQmEzZ1gnbzUTqOngHYHpnUWSDWmaKrWheUNZm0i0zYVuEte5xvt5QjcP+NAf/9+7yVtmfGp+U/pl2xrSwmTPL9Zq9k45dG5RpAKKXC8E066fVZamVZyepYmpSM64KwGQ5gzItzVqxKGXaLd280goaDpmtoTpN2uKHXtYUGjNTRwr0ycGLZjOZ+gjAGXBo8TdXscPMuelxK9FQyBW0Gw07A7JX3yL7MLrWlV5nZs8e5UobqJuJnoMpmymXC5MKrgYP0UD7Qfd+3QHAQlDxmP5ceTyCwT3Somxhxjojn58YsV45gs7bqkXxY5lp8HQzqTEztNifvc1DBNJ3ksbMeIlXd/0ap3I9LSlSjLVmAiVmGhoa8Pnnn6NXr16FboqNl5ixlXHYeeHrSMxMzGBCpi9/B1I0L9fshR47lUpXevSzym+xFc2T71WQUrNp/ADg7WZqSqbwzjebjWvW+OHmmV9g1dY6XPPsJ81vLEHOrMLkWtMOUCmnLp8/j1okdI0jOSjSWab9OZdF0W2+TxKzf8wkZjIZOB1KImR1JodSUzaTrK+iBQCrlhl1X/LYdgZNnpYZr/R1U8xMLOJhmQm5+wj6eylJzW7SLRGZz2zX0oVpf0TPNeIRn+MSnESAhIjVGXAsM7INtDZPk2L1I+I1R3Cp42YK2enE3m4m0p8ZXEN6ADDgPAs0hVm6mhau2gbAecfjUUdwmQR8NsuM/FPaMuNMeOiEkgYARw3ZTNQ9lsvNRIW6XnwyXZDSY2KsZUlRK1AJmSgUo6upqMXMr371K8ydOxdLly7Fe++9h9NPPx07duzA5MmTC900G+/iQ87DLl8oKTDCFhksyIsplXnaFOzMCHUzp6nugaQxmcLRt83FcXe+mdMUSL+XEu79FoqQFaTYk/R/ZXu93Ey3vvIlvv/Pd3HTS1806zitPROi7rGQFnyizDaT7tmzl5iOkEBak2XGnYrtHY9B3bNUzMh04o4lUVfMTIi8N855OrNod8wMzWbSrRUh5b+5Om8vweGca+a8wk5gL6027MTMkCQB324mElehuaJdYiaUzhSkafSRUIj0Yz7cTAbrGQDbomEv2EjdTAbLFN1XTjdT2FkqxEs0qBWA3c+WbpkBSJHCMBUzlQCcIGBaM0z+7rhpSTZTlmfEmfTAFmX6+crrVRJVLfeO4JMBuG63oledmbDh2Y6Fw8QyY/4edV/p29JtKPzkV6eoxcyqVavw/e9/H8OGDcOpp56KWCyGd999FwMGDCh002y8AoBlB0pf+NpGJ44gYkjNtpV5JEzS/dxixjWYkAe5KSmwbkc9lm2u9Yz6l7isBwV+QJVspoBU0qWuMcA7m+nvc9NZeA++vaxZx6kgaxX5qUabC/Vaq3+L0tlmyl3nIlsZAvlcS3cJHTT158v0XNMZr2OZSe+rKZmy3SYd4hGXCIuEQq5U4SRxi8gsnsZkCrSKa5jUCpHIY1OXQLbJQa7UbKNlxnZpWQbLjNkVnW6L7mZy1weSx9tWq4oZuU8aBByNWJ6BzroooOIwZFmKm2LPbqqYWbmlzrZ8NNoWFtXNktSsc16TQiWbKY8KwHE7KydlX9tSRcx4W2Y+yYiZlPacJpPOM0WDx7PFVSluJvLc0vO168xE1IUm3XEsBteTYZVwQL3ekmiEZFR5WOKcFbppTZuQ63PFRCT3RwrH448/Xugm5CRXanY45CwaRsWMacZHlTldJt5d6yF75oSkKSkQz3KHda2T6/NtDQ0ADkqdGfdyBun/eg183Svixu25oM/Xii21GFZV0az9SNQ6M27LjBTTdFkCr9kzXUXZSc1WAxDpLFw/JxovYYqZkYMXzWrqUBJRir7Jc9FdKDRgtQNxLzQkkoor2Nsyow48ujtKIgeXeCSEhkTKNbAZ68w0uLOZ6n1kMzUaBmU5UNNYJ8BtmZH3p0M8Yle5pTE77oBQ3cWiCs412+rtz1aWpVc6l3Vk/jb3G3y8cise/8lYWyRRKxRdGoGWAjBdNzoo+3Ez6RlI1DJDLQxSHNP7LxeP3FTdmI4ZEaqY0Ys75irmJ88VUOvMAGoSBnUzSQFf3+TEUaqrpLstKKZrQdsnyRrwrQknIagQDSFkpSdqxRCWoFPUlpkgYC9n4JF2Sjtl2RmHPWNmaDQ7mb3kETOjtCFPy0yhH1DHZWMRN1Ph2uMHGudD/0vvyVZSGn1IxhSfL1trnX3Q7JTmIgzXWhIlS3DQTCL7+csSMyNdKFKYUxeGl+uFugLkTDEcDrncTDJeRprhdTdThEwcJHQ5Azr41zUmlRRjz5gZMshlc/U1JlRXVlNSteSY0sDpIBHWrrdav8Rs9VHcJdo6T1Ko6WJGCqRyMmuJhb3XgbIrAIfT+00JNeA7ahB3NE7q3SVblP2mB0Snb9NdjV4xM+nYD5nhk4dlhvSZUgTFIs7zXU8qLks6lkbtd2JrbaMxmFip6OvDzUQDhqm4oJYRpc6MYUkIaoXRiyR6ujVJzIyEZs95BeVTK4y8RtFwyD7XXFb/QlDUlpkg4JUFkCQdvBMz4/aHG91MUTV1Ti8Xny2biZLrgfMqmFQo6NpMdqX6wFhmMmLG4GaipdH1wdYvW2qcQWn55ppm7YOi1pkxWGakKFEsM9kLdKU7zvTnaVn+SMhCA7ytl/S5tgUGieto1MRMh3jaAqAHAJvK69M6M9FwWuw0JlKoT6jVhqUIk8g6H3TgaUqmXGnNkkbDrD+REq5ClxFiBZGo2UyOJUIOaF6rZpssDHogrb5ekOyvZB0reXzPrEwtsJgePxyycMnRQ7Fqax0mHzLQ/rvpGjWS85JtSAlHvHqVnTDVS8nPMkNjZjLPW+Z6NyWTSoySJByy0Kkshi01jdha00SeU1pnxnlOfbmZUs77RsWF/I4QgljXwnZb60xihhTNK/N0/Wa3zNC4JVM7lWeLehRCFhrBbqZ2Sa6FJiMhJzVbiplQyJx26fhMVbNvSnsx/Vtmsj9wfkXRrsKxzARn9WnqrgHMbiYqZvTVpP1CrTutYZkxXWtJlMzU6czQVF0WUDtOe8ZLUozTs7mkIfZDdQXQbCZTanZ1gwz+zXRbLsuMZbDMqJaEEilmmpJqh58jm8l03pQmO1NGzfiQY4KpLgk9lhMzY3BFk+PSApmKpYksUki/YwoABrSYmbAj5lwBuFrfA8BeBypsWejTqRSP/vhg4zGU/ZgsM8nc637J6xqjC0169GtKBWAtcJnGmcQimee7yexmAoDOZVFsqWnElhrHMmMXLSXuUCr8s7qZ7OxBR9BQl12Dku0VJktuOPssoW4mvZCea0xwXKiumBkat+RhCVPiihKO4JPvcqHHChPsZmohXlHhtPNyAoAT9ncihrRL+QCXxsyWGc8gudYSMwVW22qdmaDEzKT/63IzkXZ/uoaIGY91ZbIhhMCWVnYzpQxWMIkiSpqom8m8Doxt4Qi7Y2bozFB/3lyBlSkB+QjSAGC5Fs/OTAVTuW6ZK5sppKYK2/skgb72ukGNSc31Y46ZoQNBtsHKzpQhIoEGZTruEkspMQ9IS4Eea0QCgBN0P04b4pGwfQ1sN5NWNM+Pm4m6ubyKr1Fri2xPyCBaAODLdTvJvtV4Enqu1AXoZfVrIs+WY5nxvzYTjfVxrENuN44uwLqUxwCk3UyySfGoW7TQdyWbmz5FnkHAqUclz1eZNERCrucxvd1xu+pxNG73oPNs65ZAWoXaayyhlZ1pSRE/SzcUChYzLcTOXvGwcqTrOmQ60CbHMhOLuF8AWs5aifhPOTMKuY3ilVKd081kqIVSSOR50FWziz1mRuhuJsMyDFuIVUVfr8YPdU1JxbS+YnNrxMzIdrsHpfTAEbKPDaTvSTRiHvBMaaB0wUTPeAxtxkuX7qD1UHTLjMzs0odSPVUYUN1MYcuyO/+GRDJrkKRsk+XTjSDftTLDoC/bkT4vd3wOzS5R6swYrhv9OUbSvB3LjCo4TXVmADU7Lhp2igZ6VwCmlplMG03rSQA4ZHA313knqGWG9G051/2iMTOWeo10FDeTVmi0KSXsdtOAa3nddOtF57K0mNlS0+jE9RiW6qAVfbMVHqUxNvR4SVvMEPEfdouZCBkzaDajdwVg+WznFzPjLOzpvBNyXKIu0kJPfE2wmGkhIcv8UNCHSb5QUjvQmSA1mTpBfKpy1mNmstWZUbfnCADOI2bmxUVr8fRHq7Lur6WYFj8sdsuMU28i/btpgUza+fmxzNQ1JnHbK19iQWblXiqGAGDt9nrDt/JD3nq6NpOEzsBodo135p4zuDkVgDPCPcvaNXomS0oIRWDoMTM77JgZaZnJHTNDZ/+hkKVUp6UuLVeJf7IfL5M8xbY8RMwDBXU7Z6v70ZCgs2B3H0HbQONPXOnzSS83U8YyE1PdTGEPN1OCCAD7XKXg8sjs+uHB/XHDSXsBcNx8drBzRH2O9GfAyxVJr6tnnRljBWDHUtVoiEdygls9LDM1jbZVRX8e5TWI+LLMOIIacGfB1mtuQr09pgkG4L3aeZPyvLndTE6sp9nKqhTAbEy6vlfoMh4mWMy0EBrMRjHFzEj0CqsSJzVbtcy4Y2bMD6COXmJdxx0AbH5AkymBnz/yEaY8uQBrttVl3aeJR95bjqNvn4sjbnlNSa/VoWnOptiTYsTLzUSFYqMiZnJ3Av/6cCXumvM1TvrrWwCArZngX9lx1TUlm+WuUttNr7UuCpysBbrGUsRrwMtmmVHqipifWxoLphZkU7OZqrUAYJMIK9WXM0jRVGLHDaNnM3XKpBVL4kRweMWTUGhMiCl7S8kAyhLDoFpm3H2EFAWWZbYo5UrNloOk7mbycpXoLm7aBi/LTDwSxqn79bV/pwGrerCxvmq23ic1Gix8vioAa1k5VFDFDNaxsOaK6ZwRM1tqG11WHvru0QKofrKZbMuMJkJp3w/AZZmJGjLuAOpm0iYKSUdwUsuMfG7CHnE+tK6PnplIax+xZaYdQoPZKGrMjNrBelUANqVmm+vM+LPM5FLPXunkOvTlXZlnvMba7XW4+plP8PWGaizfXIslG70zceTRQ0rMTF6H2+XobiZTu/MVMxt3Ntg/76hvsuNlBnQts2Mk9EEqX6gI07tIukKxnV1DOtNsMTNOvRSnA3Rmrh6WGfJcU4Gh15mRMTOOm8ltmakoieJXE4bitMxgqlhmLMue+dYnkkq75Uzcvga0Qq6PmTetM+OUXaCDKx2UtYGKBPw3kIq0McNxbdEUCmVcYOq+9KJ5XpYZxc1ErB7zlm3FHbO/so+TIBYnSSMRXF5QywJd1VsXM7nKTjjueidw2HuhSW/LDOC4Z6ibSeKKmSFuJj3rjh4/FPK35IU9HnhaZtTYJJebKey+1yGLumjNIjSqCd50SQNvK2tTyi0e64hlxk98UKFgMdNCaDAbxVRnxv5O2OwPdx5o8wvvWQHY4yXKPwDYvB+agbOputH4GS/0zrQ+i0VBXTVb3VasOO6a9H/DhnbT2KWGptwWFTrQfLhsq53J1LVDDB1L0hYEPeU2X9Q0eLco0AMk6QzPu0BXyCWCqHvKa6CKEYtj0vDeSDHtipnRxlLZvovHD8FPj9gDgFoTRA8AphYlGSMhoUG6tBq3F6bUY/o+KcsZaINSPGzKZnI+R91MtIhcen9my4z83PZac8yMu85M+nsfr9yGO2YvxoNvLcu0W4onxx0pn2fdokehqe5NZC2hWNiJfVGTG7xiZjJ9n5/UbINbj/a9Ttq7OSaFIi0zm0l/Z4sZcl8jxIqZdTkD8gzS/8rra2eyRp3Ac9okGrSrbLPdPt5hDlR0yufa852kxQ2zWGeLYWFiHRYzLcRzOQNSi0FPF1VMyOR7tJw1rV+TK33R6yXKFdDrt84MtSas25FfvIaeipxtoUVqLTAF0hYjSa8AYNJweg38WGaqG5xr9O7SzXbMTKeyGCpL02KmxZYZQ7C1hFphaLZH1CP2hVYAdr5HAho93DSuOjNJtbKq7Nj1onlU7FGoq4C+lzReQbrqaJ2ZsGWwzFAx4yNmhhbAixgsM6aiefaxIs71VgOn3W4m211jp457W2aEEL5Ts/WBct6yLcrxaPAnzczyIkREbFMypYg9O/YlJew4ubhHIKu8rhEfqdkJk2Um6h7iYhG3y0b/vUt5+j3bVO1YSWXhQPo+hyz4slbQuC0AiqADaMyMY0miFryoyaJHxpFspUHoM6KLYK9q8tQdagcAh7zdkcUAi5kWkmvBLur7l3i7mRxTIw3Ic8XMeJjrdXKJGbc1ycMyQ6wp+cbM6IN3tmweai1w6swUt5rRZ1wmEaZYZhKpnOdE44reW7IF2zJupi6tKWYMwlFCBQjN9qCDEMVUc6NBqU1h7gDNMTNkEpAZPDyL5hliZuxzIPeBxitIy0V9o1MrIxxyi5mYwc2UzY0g49OiESczqMlgmQlrgwugul7oLDhbNpPchy6M6GrHtcT65JxL+ntUzJgyrLbVNeHTNdvt5ywStuzgdnmuXqnZzn4dS4opZqZBSfv3CkCmg7Lcn3lCpLj1tFgcCi1SKHHFzGQsddTlK4WRY5lCxmWT21php2bLNdzCuphxrPIS3TpoFMFEtNN+xbaWajEzeskBr4BvusRHYw6BXSxw0bwW4pWabcpmcr4Tsv3h5gDgXKZYf7Eu+bqZvB5QKkjyFzNqx1PnyzLjDFTF6mZasHIbvt5Qjc7lajCqKZtJN4s3JlPGTlZCxcxna3ZgZJ+OANKmbylm9AUE8yVbADB1g9aTuie5MiAiIQspe6Cilhlzx6nHNaREdstMdYNWZ8YQM6P/TC0ztKieXGxSbi+JhlEWC9uFLWPKrDi3G4G6UaTVRBEzSsyM2m6lzgwZOGKGyYtzHDnD1iwzJP7IJHhNAcAxgwvj/aVbcPxf/mf/Hgk5CyTaAcA5psLRcAj1TemYP9s9RuJzqMgv8awA7LgiTd+j0P5MF8qUeMQtDLzqzGwmmYQx1+CuCsqslhl70iOPp/blDVoAMN0voGYzOW0OKfe/KSns9O0kcQ/qMTOm40vsWBuDwKUxccVYZ4bFTAvxVLgkg8KdzQSjP5wGAKt1ZjRzvDAPCjq53Uz65z3cTE0tEDN5uZmk64O4mYpvAgAAdqbReYcOBOAE/ppSynUx05DILmaqiZhpTKaIRSJsL+bXUssMTYO3tP7eFKRKLTPZFrXTYz+omdurPg2NBVOEhzZ46G4mPQCVdtq2m0mo6+jQDCE6ewXSs/HaxjqlTfQ42Uod0FgWmfa8k9xHWpbfVTTPUGeGBnw2KpYZ3c1kjplJeIgZeS/cbqbsyiQSom4edTD3gq4/p8TMGGJfvGJm5JpXdI2hfFbNls8k7SOpdcg+P23g7qxZ6gB3ALA8fa8Vxyk0CF22i7aT9v1Om1RBrQuwWFh1WSZSKcSgWrj0mBn5+Zx1Zohb2W4PzWYqwo6Z3UwtJGwYvADdMqO5mUIho7lODvwlEToLzp2+6J3NlF09uxeaNH+eBu2u3pZnzIw+kHuIGSEEGWCDEwA8f8U2AHTVbPV5SKWEayaZa0kDPX29psGxFrSem8ktHCURUkTN3kZEiVedGSqC5DmGqJspR2q2d8yMFgAcd8QMFQbU7WE/PylSAVhLo6WWGQBKerY6K85tmaExM/Ie0SBt51juGW86e1Fet+wLdNKsIPpfui/JZkOwvrM2k5qa7bUauPMZ96rV2QKAadsaEynbXeO1+rWXC50WH7Wt4B63wRijpLlZQpYasyjRhVlFPOISinEtANjJTMo9wOvuaO9sJrObKRyylCyk9Lmp7jKv86f31rHMZHf9mp5Tuq9itMywmGkhnsWmsmUzhcwpbkbLTDJlWye868yYX6KmHMGmXiZdHTWbqSGvGie6JcYrZoZqFjVmxvehCoIcsOyYGdvtmP67ySSe6/rVNCS139ODeCwSboOYGXO9FpNlxklB9bJCmt0lXhYdU5Ye7UxpinMqJYhlxhEdtK6MOWaGBAAT101TMqW0G1AHeGqZ8VMBmK77Y7pHTcR6pV/b7hVxY52ZmMFdpQff6gMOXSBwY3V64lHVscQ5F1k0jyw0aYrj0YmQLKQmn24m2bZn5q/G399YAgD4Vq+Oxkq+novo2rE2lmddG+ez1K3nfibpcXKlZltWerFJitsyY9lto8c0oQcA68VWTQHAUUWEuO83dfukj+9lmaLPcvaYGSfg2/2cZrOyFgMsZlqIvN96zEzKMMOUhJSOiriZiN/USfkmwWyyAJSHuV6ntSoA64Pvujwq0OqWGa+YGWqBsSwU9dpMNNBODliOmym9Xbabihk50OTKaKrWLDNyTa9o2HIGylaLmTFbZtxFxSwStGiezYVDJOjTHvByr9BLY8FUy4zTsdc2JW3h2LHUER108FZm4Ib3J0RmtnrMDOAu8a/vN2vMjAwAptazWmqZoeZ7p52dy6Lp7EWDCDQtRktFEwCXK4D2NZt2pi0zVZWOmJH3sAM518ZkKmtmkmyP7WbKsTaTRIrRR99bAQD47v59cc7Yga7id9RSovdlapZY9uNRkU3vrSkA1iTWdeiioV4uVLovP24m25qjvUtykhf3cDPJn+n9lotvyqYrqek0Rouef0Q9f7eV33GXuqyztLgiu5naH/4sM6qbSS1V7pXN5MyC5K5ldoffVbNzmQKbEwAMAKu3+o+b0YWQV8wMbQotsZ/DU1YQ6PWQYsbLzURN6XIAyelmalTFTLVtmWlFN5M9UzRnBbn85WHLMxDWZJmh+3KyoFLG71GLIzWPUzP7p6u3I5ESqCiJKJaGXJYZtX1qzAwtbAlolpk8KwDT6rKmuCYaM0MHqZ6Zc9EHU88KwAlVgCkzdc3CItOKe3dyrpeTGUkq+iZSOYWCGvypWoe8kO2XE5jxw3tk6qeookANEteeEbo2U47jqZYJdwA64Dxrrswgg5lJj5vSBafuMsrHzeQEqKf/TpM/TG0y3m8tCNyUPadfNztw3MMy42SPGdxMIcuXcCsULGZaiHzePNerCbvdTHQNmc/W7MCqremquo6byfGFUrFDV4ClNLvOjM+FJnVBsjUPq4Df1GwBpy1p10d+lhkhBK77zyf462tf+25bc6ELvcnOwGuhSXn+sUjI7qhyu5l0y4yswBlCp12Qmm3KuKGBhF5i2ss9FbVnoV6WGZnNRF0Y6e/Jpn2UiU0a0aujUuSvRLHMmONn7LWEQs4A1ZQQyoQDUDN86Pe9FsqkOBYEyyg4aQwdHUi7V8SVY9jHJPeACmLdMqPO3tWBe2NGzPSqLCXfd79PDYlUTqEQCZEKvNKFk8Pto5+T7L9k3yYfByUeK7Nvaf10smtyu5lSwhFrqmXGLRDcqdnufdN+O2w5+9FjhnwtZ6D1E7TCO2AOAI6R62eyKNlxUyH3+2Wqyk2/Y4spjxglY1wRsdZwnZl2iF78SEI7SlPRvCOH9UBVxxKs3laHCx/+EID6QMt3jY7ldulqn5aZXH5N3c3klSWgWxJqG73XV8r1Xa8KwK6YGfv8/YmZFVtq8dA7y3HLy1/i0zXbfbevOZhcZU7Hpg5A8r/xsBMInq+bybbMtEEAsNnN5C7sRgdXL9O0aa0gOph4L2fgvB+26yFTmVg+8x+t2AoA2Kt3pbKPUjKTpX0vHZzk4BsiK2A3pZyFJm3LjEcxPj8BwFIwRSMhdMwSMxPRBglvy0zIXh9KtcxIMZMpe68NVKGQIwBlte6uHZzYD5P1oKEpZbRMUJRidz7dTK5Kx5k26+E5aaHgCOWVW2px8LRXce/r35BieyFXLRgT0vpHqz4bY2ZypGbTz+r7afSIW8omduUr464ALGNmZPKHl5sp07/Q+x1SBY5aZ4fGzBhEEbGyUWu5UgHY8C7rC5sWEyxmWoh8wbzSpU11ZiKhdJGu/7tgDADgs7U7IIRQ3EymgDx7dWGf2Uxe9RjsNpLKrYD3IOs37sX8Xc3N1Jg7ZqY5azPRoNl/ZIINs7Fo1XY8M38VNuRZ0RjwEDOaq0IKkEZimXHK83vfl0QypVTPBRxLDR0ot7VAzAgh7DakrWDq39OZNG4B7pWirFhm9O/R2ZyHm4kOGroJX16z+RkxI2vuSFQ3kzqTlqjWHqfjp64fwMmS0vFax4ZCs4zMlhmz4OvZMe5qu2yTXaSNPC/UnUXbLo9N27spk0FEl2owCbKGRDK3ZSbsrvOSy1Kip6DH7eBb9zNChfK8ZVuwfkcDZn++XrlHuQKOAef8vNwljjXDfb2ztT9kOc+knXUZkgIjtxuSxpXJdtHtUlCUxqjlhboQzZY4+jn6fqnvpLdl5un5q3HgH2djR30ThBCe7lDZBpPrs1hgMdNCbDeTRxyBKWZGzoR7d0qbf4VIuxKom8nUuXhbZswPlt+ieXZgqodI0QVJrYcgMX833TY5yHtZZtSYmfzdTHVNjjXjhYVrc4qUc6e/j8ueWIAxN76Kz9bs8HUM+1iG85e3S2bayMwbo5jJIgZryL47Z2Iv5LXxcmHkyzXPfoK/zf0GQDo+ybQ2k2nmGtVcARK7ow6bv+ftnlJdJsr3MvuRAZHSyuC2zDgCRA0Adj5DxYw9i6XLGRiymfRzAHJUALbXEDKLGeou8WeZca8aDtDnSY2XoO2U+5Jups4k5dw0CDUmUsrAObBrGa44dhjOPWSgvY2uzWRfz1zp3BHNzZQ5H12UKHFVyZQt3uubktoClbmHK9nn0XgoY9E413NqmjyqVhLdUmHHzPiwzOhuJimEZHtlcT4qPOlzEtWEi3IuBssnjWui181+bsi57KxP4I2vNqq1eAx1bWjtI85maod4WmZIKib1/QPOw18aDdsdRHVDQknPM5k949H86szkijiX+ymLZ3d/uN1M+VtmZAfvFTOjW2bk6fu1zNQ1qrOST7K4mhoTKaWy55fr8xQzWdxM0lUhq9XK849H/LmZbCtM2FJiOOQ+ZC2UxkQK22ub8OQHK/O2Lj2SyS6h7aaPmz7gAqplxis1W58FAunZu5ebxrSyMT0eoM6O45EQ9uxernzOTwAwFTO08rYeM3PKvn3RrUMMx4/qpRzDTwfeSGJZstWZodldANBDxswYTPoxgyWvSbPM6G6m9Lb0f2UAME0xpn3F+YcOAgBcevRQZcDr16UMF40bjG/1qrC30ftPXYHZ0AWCs4iiIa6KWCpkscG6JmeCVxoN+7PMSDeTYtFx2imtb6bnW0e1zFhGwZk+Ru7nw7POTGb75sy96pZ5HvQ22dlMBoFjElPOBMNsmQpr518aDSvPRtir4nDIfaxigcVMC/EKAKYdpWWpQcDyIbUsy54N7qxP2At60WwmiWV517vwMn/ndDNJMZOpWOrbzdSMmJmOtpjxiJkhhwgRy4zfmBk9jmfJxhrPz+piJFd2kev72Swz0s1kssxE3YOTjhQz5fGIq8ONhkPoEI/Yz8b/vbccV/57If48e3Fe7Te1m8bNeGUlecXMeK3Qq+/LezkDg2Um05HSVOP+Xcpcpm81ZsYy/txEAlZpFqGezVRZFsU7Vx2Fu8/aVzuH3HECudxMyhpDZHDpkdUy47bk0YBywOx2kBYT+erQ2T4dhK49cQS++P1EjOjdURFFsoJxl3JnYI2EnQBgKg6zoT+/UhzoIigSCtltTqSE/e7UNybtd42uV5cNeY+81sKS6fe0zk66DYbJo9Jnu6058jy83KgUrwrAcrsUnt07ONe8I6mnNG5Y93Q7TS4zg8Wejj+KKJLuKu18S6Jh5dkwJQFQ19Obizfhk9XeE8ZCwGKmhYQ136dEDy6ks0/6Mku3xLbaRnsf6Wwmw2BCsgBo3EyzA4CFPF4Oy0we6yu5v5veZ2WmNoifOjPpmBn39mzo+12yyVvMuAv5+T8f07EAk2Um3SE3kBm77ByzHU/OSstjZjFjWRY6Zo7xzYZqAOrKvvmiF/FKH8eUlUREiSaSacdpyoAI5/qeZrlMH89tmelGOnqJV50Zy3Lvky5nkEi6s5kA5xrr52BqP4UuZyDFTE2j4ybxCpKWbibX++4RMN6oiRmaymsHpGr7om4mvU+Q7z69drK+SmetIrK+sneuCsAx3c0kLTOGGkY0LqlGscyk7DblSgUHnPvgVWdGipmuRKiFLHMwMxUz6RRnszXHT7qyl2VGLuGx2RCs/cujhuCXRw3BnMuPwKn79bXbYbdJi38xWmZCegC0+RlJpITybETDIcNyBhaGV6WtdZ+t3YHrnvs0E+uZX//ZVrCYaSHyvdTdTHpHabLMAI6ffhMpPW6yzOhmTno8z+UMfFtmZKfpFTMjLUbpc8jHzSQf9E6lsczvud1MasyMv+Po1pIlG6t9fzZXdpGO6eU9clgPAFAsbQAZfHxmM8mOvEM84oolkQOxHMBlEHBLOpOd9RnrAXncaDq1RKn+qQt3w9pMpn15WXQiHpYgQI1boCZ4SUnMLGYAtwVAWa0+6c5m8sJXnRlynzuSrCjpapLnGg1byppNciauD5S0cjitjOwEAKfPe2hPxxWkB3dKVDeT+dmj4lW6nXuSej6RkLsCcO4lEPQA4LB9bsqxtbgqeX1qGpL2+ZYa+kQTUlAo4jFMxUxaoFHR4FX9mL5/oZC73XaatY9CcroV0EmNTmF7XZP9bFGRNbhHB0w5Zij26N7B3qbEzOjCyJDN5F00UD2XxoTjdrUsj7i5cAgn79sHfz5jbwBpy+M1z36CA/4wGwtXbfM8910Fi5kWIn2P3kGRbnO5ImZKpJghS81HQkrhOMD9UJpqCujkLJqX+bMUM7lSs6UgMblZvHAsM9HMvvItmpefZaZfl3RQdTY3ky7G8hUz+vnvP6Azvj+mHwBn5pc1ADhLnRnHzRRW6kwAjiCWs+lttY15t1+/xwtXpU3F+rNmyq6hrgCKWpTONFCZZ640fVaf5dsWTcUy4178r4wEALsGmyzCii40mbuUf+44AboAZCTjDgQcVxMtLFhhWDbBVHGZ9hl6qr8Mrh3dt5OrnfR8ymNhZVCW76HrHBXLTLp9/bqU4cIj9sAvjxqCWMRZaFKeay7LjCtmJtMO072mx5cCUKmeHcsuZpzq2ul3y0somywzXqJMSc02xMw46dK5Y2ayLTQp+/7K0qgxGF45pmKZyRzfdjM518suNqgFLjuCVz1OfVMS7yzZnDmGul/n2OntA7qm49YaEkk88t4KVDck8J273/IdEtBW8KrZLcSuM5PTMmOeQTqWmYbM5xwzdyQUImXh1ReeChjPAOCclhln1gPkdjN1Koti3Y765gUAl+WImbHrniDzXyuz3d9xpJjZq1clVm6pw4adDdhZ36Ss46N/VtJcN9Mhe3bFGQf2w7F7Vdn3rEJ3M8k6M5GwEzOTJUanOpNiXh6PuERq1BbGqmUmW3aUjh5bJJ8/OsB0LI16WGa8LCzOM2oaqBzLjHPeqZSzsKh8tqmzzLHM5HAzxegkQXMDaG2hbqZGstBk7oJx5vOm0HooQHpgqm5IOGKGxMzsP6Azfj1xOIb2dGbcxgBgMpg0JJIojTlxDVLk0SDdddsbXOcjrTJ/P3t/3P+/pbj+pJHmcyT3m5bxv2rSt+yf9Zpaua6by83ksS4SzTIDgG1aUU7LSn83W8BxeTyiuKUc8agKABm7Ry0zXuehW9NzWWaaU2cmmRJ21plJrOvs0b3cFh1HfauncnxZHRrQLTM0aNh8D15ctBYvfbIu8zfzZ+TvdDX7eCRk93Fzv9poW6gLAYuZFuLdwasvPO2Y6Muli5kSPQYg6fxMX7qEYpkxv0S53Uzp/zpupuwBwDKQsCWWGa+YGXk2soOw68z49DPJNvXsGEe3DnFsqm7A0k01ysxVoouXvC0zme/36VSKk/bpo/ytQzx9nuY6M/m5mXTRKEWBdPftsN1M/ttPU7+PGdETVx47DIAqZrp1iMPKzERpcC991oUQ9j3yMmnr25o8BLiXewrQY2bcnX2pkj5r3of9u+ZmSvkVMz5iIvQso46lUazeVkcsM47gsywLPztyz+xtzQzw8h40aJYZOYOnkyTZh9Dr0Lk8/Tweu1cVjt2ryvscSZ9ExQzFZenyuWo2PR/TfvQqtXoNpdJo2H4evSiPh7Gp2nm3HdenGpTuWGacZ8nLwqQXzfN6RqMeFkuKa20m+S4JJ17GJNZ1fnfCCHx/TH8M7tHBHiuklWnDTier0TNmRgaJa+cihQzg9G9e61eVkESGbh3iWL0tvbzNvz5YVVAxw26mFhIiHTzFZZnxqFRqi5nMonBK50weuHBInZn4sczoKbQ6sjMvzZiVG3PEzMiOsbYp/2wmv6nZeqpwvjEzJbEw9sik73q5mnQxlq9lRhb+KzV0+nYAsO1mSn/Wr5upOks2kx30GZFupoyYyWMVcymWupTH8M9zDsCQTMwF7c9lR0+fP931pIpp75gZag2hrljqPtSXQQhZjpjNZZmh4l8flPSxLxRyrAUJZdXsXGLG+Y4XzjID6c/KgPfthpgZ4zFcIlCKFdWap4uZ9LGi2neJmCnLPdsHdMuMeY7rikHKadFy2qgG0+r7UQWHXkNJ9onZYnRkm+W7rJQLMMTMdCZiRq+4LZFxSYBcpNQ8uHtVuKZIy73chfzOzE/W4b8L1wLwJ2ZKomGM7FOpPPeyXMHCVdvxkxkf4Ml5K3PHzGj3QBZvpKhF+5x6VPK6NDSl7L7sR4cNwp1n7pOz/W0Ji5kW4pWa7c5mMpvD9ZgZmp0Q1h4mdYVUErnutTZTDouDfMFyW2akm6k5lhmtzkwiafStyssn+8t8i+bVknoUfTun42bWedRfqW2hZUZaTKjwlMiZX2My/aI3ErdAvpYZPWVZdkTyGXFKoecvZvTZNxUCXTOdqjKzJjEzgFlMewXyOpaZlOs7gFsEqYOg086uRjeTWfzL/aptoRVMBYmr8Odmyp6aLcWK42YCzDEzJvS6H3q/0ZhM3+MGO2vK+fwPDuoPAOhVKdO8swtAE2rMjNkyk8vypUOL5nklQMhjU6Gkrwgv77GXBcWynBR9OVnyEtjy/aTXz8t9qMc5urKwNCtLVjeTJpzldxau2o6Zn6atIn7cTCbk5O3fH67CK5+tx5VPLcTGTN8X0evMRMxB4jLGj6JU1CaftytTJ1O2yP7hwQNclpxdDbuZWohXanYiqT68asyM87lsbibVMiNfgpBSIwPIVjTPp2Um6ihtE3K7TNVsjptJFnsTIv0S6IXS7FWcNcuM36AyaS0pi4Xt2ajue9c/q7fRL9IMa7LMlJNZ7c76hH3t4tGQr5gZGgC8rVbtcBwxox43HzeTFGJ6tVvaEcsMG/eKzGY3J+2oTbNukyuWCnD9e7TjzMfN5OUGsH8ns+vGZMp+xvxaGLJlqzRqIqNv5zIAwF9e/RpjBnVRYmbMx/CIT4iog7TJMnPZMUPRvSKOI4Z2d+2rTydnkclsKNlMHpYZl+UrV8xM2CxKTfdJTtRSwl0fy7bMeBwvbFmkvIS0zKTs79DvdfRYfytX+40FIW03U+5sNzvYXQsApvgVnjo020myZntazPTrXGovZEzbqlt9qUv7r2ftB0AVc/Tz8rokU8KeGJpqRe1qCt+CgOO10KQ+6/OyzFTYlpm0mynu0TnrgWPUpOmdzZR9kJMvnxyUvYrs2YIkk82kWzayocfMAEB9o/s49nonmVPOd20mai1xVpZuNH62tQKATZaZcMhCeeZ6VtcnlLV0/LiZZLxAp9KYK+bA8VnrYqbllhnamXXMuEj0mZkiZpJuK0vEMHulg4CaOup8P6wtdkmPQ6+VOQDY2zLjHnwdN1ATmRDkTjF2t58ihHAFAF94xB74Vq+O2FTdgL+8ujinS8s1wMvlHDRrnh6bI4953qGD7EGN7qu3XzHjwzKTK/Vdhz6/ipXDkDIPuGM0JLZlRjt+lMR/yHeivimJVEooAbdq0TynH8qhxbTU7GwVgLNbZoQQrvWcjGLGUHrAD3t0Kzdul2ndajaT9/EB4JfjB+P40ekK2N/q1dHebrLMANkLX+5qCt+CgGNHpevLGdhpp2oGCqB2HPrChCUR80sf0V4CXzEzrVRnxq4Vk7Gu5JXN1ORk58i2m2I83DEz+bmZHGtJxG6nl2VGflbehubWmTFZZgC1cJ5XAPBX63fiobeXuQKct2ba3KksapuEAXXwoq5IuT//lZKd++GFFJLdlFoc2QLQZRkCS0mlBtTAz/8uWotXMib1JMleC2lxDVQQUfO3LuIAzTKTKwA4lLtonolcAcD0Wsj71KOiBBeNSwf5bq5utN3OXsLJs86OJoBNlpls++rVqcTzc17f8QwAzjdmhpxrXBMGpuVdvO6Dl2WG1tUpIW6mpBKPZQ4ABrzT1E1tTlv1PCwzOWJmaF+tu6YonXK0x4su5THF4nT6/n3xi/GDcfG4wZl25s5mktBxar/+neyfaRyTvoCo/r1CwWKmhdB6AZRslhnaKXTQzJ4lHp2zyzLjI5vJbwVgW8x4uZm0bKbGRMrTGqRDl2gosU3m3mLGiZmR230dxnZ9lUbDqMzhZpIDujyfvC0zpMS6CVo4j5afp1kAE/78Bq577lO88tk65btbyYJz6uybDgzu4/oVZNUelhkJ7eP2JObrcCikZJSYY2YsO/VVopvnf/Lwh0hQq0jI7cOnnzf58ilUULpSsQ2/K8sZJNUJhxeOmyx31iCNEykl1oKEdr46XrN+fQkMWpzPs73kWfHtZlIWI/RqY/Y263i5mQD1Xul9m468x/rfFTETca61Ih7ClnJ/qGWmU47g6FzZTH6L5lFxlc0y08vnvdKxLMu2ynXrEMctp4/G5ROG2ccy15nxEDPknE1lLQAoK6hLsj2Pu4rCtyDgyAdFn2G7Y2aI/5V0NvoMWS1P7g7Aso/npwJwDiVgr5odc4JWTchZocxmAtz1SkxQ83s8EiKmYPdxZFPt8vqZ//q1OEhrS1nMcTPpKZ4S3dKUSwjcPusrTLzjDTswMZubCXA6geoGR8zEI2G7Q/+UrGmydrsapLw1Uwivc3lM6UzpzyYR5Xd9KXnfyj3iIuh6PHuQRR31UvkmMR0OWa7Zrl5DBEi7KeX7IR9xr5iZHfXmeyjJK2YmpKZm+7bMZP7u9T41JdyWGdq2OnK+XoOIaVE/wBEBdtG8ZG7LTB15FmRQcC5on+QtZvTrmX2fiptJ26dJvOayzGQTM3HSt1AxEwlZilu5nIhfXXjruOvM6PdIdzMJc3IDeTX1fhwARvetxC2nj8Y+/TplbU825Lu6T79K13IcajaTPL755un3aW+PNtHP0XUDCwmLmRYSMnTugMkyY+50K1xiphVjZnIM0rZlJlcAcGY/HUuituXEz/pMVCRQMWP6ruwE5BnnWpvp6w3VWLnFCWyT+yyJhm2Rsr3WI2amUc3OylV07sl5K/HFup2Yt2xL5lhqsUEdpwpwk7ECMF2xW4i06JHnL61JncvUwnXZ3EyA//TsmobsbqYuRLBSMaN3wqaYmbBlKYvjAe4aIuk2JOz76lhmsot4L/INAJbXVAai+zmGsyqy+f2Q+5Fl4CVyqYU6Yi3wDGLV3UyZdsr77lVnxsRGksXnNbvOdnyvWbbJ0pUNr5gZ/Xh6irOOtMzo186umWIRN1Mi6cqUo25xOtDndjOpz5ZnnRnSblNXTC0zTgCw851xw3rguwf0y9qWXEzM1BA6ed8+rr9FDPfWyxipu4sO3bOr8XP0+aOFXgsJi5kWQi0zm6sb8OdZX2Hd9nqlSBbgHQDncjORF0iv80H/KzvHFxetxX8+XmNsW7bsC7qP0hwxM1LklETDzmzTR9yMKmbCxK9tcjOl/+uOmXHvt7Yxge/c/T+ceu/btgioa6SWmYybycMyI4WPtOBks8w0JVNYnylGJVO9s9WZAdQ4KCUA2CBCFq3ejv1umIXfPrMIjYmU7QbqXKYGAEdzWGb8usqkZaYsbm47rUuyRzd3hVopANfvcOr10kDaipKIEg8RsiyX0K9pSLgCYun4STvaP502Gv26lHrWsKD3QBf12RaaVNrt0zKTK9BeX6TSeVdSOVea9oyZsTPgMjEzhgBgnfU78194NB4JoU+nUnQqi6JflzKfbczhnsviGjWl4nvN7uV11MUTTTOmAcB6jEqNRx2ZfXNYQnQ3k9dyGfQ8TXGKtD1OnRnn712bmZJNmbBXFZZOOw4njO7t+pupzoxX/Jdumblo3GAcOrgrfnvccM/PmdzehYBTs1uIfMESqRROu/dtLNtci5Vba12ZErTzoQ+XniJb4rGGk93pE//slppGXPzoR55ty7U2k57NlBLp2Sd1CwghbJETj4RQFgujtjHpKwhYfk+aIWmHoyMgY2akmElv/2LdDjQmUkrHsrm60W5DdUMCFSVRJV26kgQqNySSrpctH8vMuu31dibC+oyYoVYgEzRmhhbNG9KjAh3iEaVI11tfb0JjMoWPlm+z11oKWXJJAXNqZIlhVp4rPXtrTSNun/UVZn22HoC3m4l2rNQyIzPYhvTsgNXb6vDV+p0YM6gLAOpSTa/f0yEesWNdIqGQ635XNyTtmB3TrJwOkiP7VOLNK8d7nhe9B7qb1BSXoaek0r95kSsA2JRhBKgxM7IpnplCHsHLTgCwapmJZrHM+I1no1iWhdd+dSQEhPEaAe5sohxaRouZ8WOZyS5m9ODpGHEzOTEzKXsSF7LSbfYS+j87ck/UNyVx9Iie5vZrAcAycNlehiPTHHq9TC7/VCq7ZcZvYcNceFlHTHVmvK6J3leWxyN45IKDs36uGDKZALbMtBj5oHyzsQbLNqfdHu9+s9lVJMtrocmKuGrq9Koz46yXkf5vMiXw/tItWQNkc2UzyZeMdrC6lSJB0hzjkbAtfHyJGVljJWOGzBozk9kkT3n88B4Ihyy89fVmXPfcJ8pnqRiQUfbS4lAaDaMiHrH3o1cTBRwxIuvm1GexzMhS3YAjZrIVzQPUbCYaAFxVWYJ3rhqPp342Ft8fky50tiEzi95Z32RnMlWWRhEOWa70W0k2y0wqJfD52h1YtqkGQgjM/WojLn18Pg6/5TU8/O5y27rkNajSjpW6KFZkXHpylebF63faf9MtHNR8Hw5ZOHHv3jhulFNKv7Yh4Yoh8YqZyQW9LjR2Rd+PI2bc+85lYYjmCPB0LDPqvp13JZEzi8y7ArAWM+MjALi50Iw7E14p1V5ki5nJZnXWcYSvtg/pMiFupgbFpZfe5tVXlUTDuOq4b+HAgV2Mf9czsPQ2hu0+2dlmckUqbibDfrqUt46Y8ULNZkofd7/+nbFX746uz5qsxyYUN5PP77Q1xdGKAGPyG5eRRQJNMTP0QS7XzP1edWbk8yg3vbl4E97+ZpPyXd2PniubSb5kdFDWxYziKoqG7FWK83EzyXOnM9X/Ld6kDIh6avb+A7rgjjP2AQDM/nyDsl9qNt5W24RUStgCqTQWRogEourVRGnb7QDgLJaZNUTMrMu4VnKlZtsBwPUJJQBa/m3/AV0wQutIdtYnnODfjKDIJwBYtum5BWsw6c43ceStr+MnD3+IKU98jGc/XuPKCtItghKvwl2bMqJrSI+06+mr9dX233SXEY2biYTT9+KeH+yPfTOpntVKzIx7Vp7L7eOFLt7puykHXsuUYpsjeNEumufxPsnnXLdoyPuUEu4yCDqe2Uy6ZSapPk8m5MD+7SHdPD/THLzierzI5mYKGaxmXiJWxh7p4knGzETCln096hPuYOt8inwq+zeUyVAFMlzbTNY7OWm0LMd6Et6FYsYUMxOLhPDfX34bN58+WvmsXysL/VwxZDIBLGZajOkFXLW11n6oTdlMSsdNiqkB3m4m3TJz56uLMeOd5cpx9RofuS0z6f9GwyFbscvBd1ttI97+epNijoyFQ/YA7i8AWK0OKc/ts7U7cPYD7+EnD39of1YvmgcA44anFy3buLPBdsEAqmVmR12TEvwqBVNllowmO2ZGupmyWGaomFm/vR5CiJzZTLLmw5baRpeY0T8j2dmQsBeck+vGqFU3nQtjDgBOH+fD5VvtbbM+W28HG//w4P7K58s0MXPBYYPQr0spzj90kLL92hNGoDQaxpUT0wtSDqvKWGY2uIWol2VGIgVUTaM7Zqa5lhmK/rybLDOAW3TkEk9e5Rec4wrjfk3Ph1d1Xa9MmZgtZtLPXJOPAOB///QQnHVQf/w5MxloLVxuphyWGcXNFPW+5pEcGTZe2UzyWoY1q6++yG8+RT692h82tFHu3yKB5SbrnV79Vz+XtrfMqOMNRe+X/Ma/qAHAxREzw2KmhZg6wvqmlLOCbQ4xAzgWAgDoRlJj8/ErA+4OI2fRPOG89E7WRPrF//4/38NZ971nL4IWi6TjIcqI6TwXtmUm0y6ZCjn78/UQAli6qcauq+LUmVEHv96Z1NKvNziWAJmRA6TFCp152WImS62ZOq3OTCIlPDNVVm9zMkPW7ahX1vTxsswMylTk/GZDtWcqrZ71A8AuOy7dX9RtoWaGeFtmpDuIVgX9w8kj8YeTR2FwDyegt1xr+zUnjMAbV4yz440k5x82CIumTsD+A9KmeLmPTdWN2Jx5xhNacKusIAxoVsiYdL8lXWuXtYZlRo+ZoYMv3aXuDsolnpwKwNndTO61tNQMmJKouz6HxJTSnt6nWgHYT2r2yD6VuPGUUc0uj+9FvgtNmkrgS0KGvo3GaFH0+CogfT8Hdiuz90WXM9CF8iGZjByvSrleKFZyucii16Q0i/VO9hf0nGtI/9mpzF/GWXMxVQCW6PelOZYZdjO1E3LNTpxsJu8U0l9NGIZj9+qJ604cgZP2daLRI0pQpJwFqPunZmt9ZpgtAFgIZ1AOWU6dhoZECvVNSXy+dgcA4KmPVqXbn3l4pVh4d8kWrNtuXshRIgdY2SEPy8Rb0NWsv1iXnuHbbibtiRyc+Y4qZpyO4KkPV2HsTXPsNsoOw641Y0jP1mNm5HmboDEz2+ualP15WWZkXMk3G6ttKxJdgRdQK5FKpBDplMvNZJgJyWst09WnfmcvHD60Ow4Y0Bmn798XADCwq9OZmywE3gGEzrHLYhH065Iu7vVl5t7pMQpelhmZQWWKmfGyouRDo3YPab+tpB7rwag53uGcAcAebibLspRnxCvoGgCG9uyAozKWyPQxZb8hY0FSSKWEpxVoV+BVNM4LpdBjNstM5rm54aSR6JEp6S8XiwUcdx09ft/OZfb2SEirAKy5+G//3j649Ogh+L8LDsp1igqmpA0qPJQK7VmWNJDGGmUxTWIxbmvLRra0e7dr1G/MDAcAtzt0v7EcsO2/24F83paZ7x7QD38/+wCcd+ggz3o0cpBeTAb167+zl/KCbtRSMrOlZlOLeTjk+JwbmlL4YNlW1+ftuJeMeHrs/RU4/JbXcM/rX3seQwoE+YLocSIA8Oma7Vi5pdaVmi2RMRr0vKmb6dUvNtiDGBV2dq2ZLG4mWjTLK7qfupmAdKB3up3eqaR9O5eiPBZGU1LYwk3vzE0Fu6SY6WJwM6kBwO7XtiFTlXnV1nR79+hejhnnj8G/f3aI3ekP6uak3OqxWvkwuk8nAMBVzyzCmm11zkw4bIiZoSvESzdTQ8K2CpqCP3MF5HoxvJf6fPlxM8mMl2zkSs22F5mMuPdD45u80uGBtPD5wykjAaSvkxSEdNVsannKZplpK/KPmfF2RdBrLs+xe0Ucr19xJB4870BccJjj7jQtZ7BH93JbHNDJWLrasmr1614Rx6VHD/W9TpWEXmM52epKXEIhwzNlckVSC7hkR11uy3ZrEQmZ+xHA/Rz5FVbFmJrNYqaF6IFs3+qlihlnwThvMeOFKeKfzj4nHzIQ+/XvbP+ul+f2qkgJuNcLiZFO883FG+2/Lc4Eesr2U8HQmEjh5plfetZxcLKZ0t/5Vi+3mPnDfz/Ht29+zS5Ip1+ZwQYx43U8Ogt2LDNuMVNPMkv0omQUIYQtZuTnnluQrumzZ/cOnpYMy7IwtEp9DvQZkckys3yztMxEXd9Ri+aZKgAnsX5HPRqTKURCFnpVujvugcTMnm1tplz8euJw9O1ciuWba3Hv69+4Kumqlhnne+Vx6mZSs/1aYpl56ZJv49oTRuD7B6qFx+i7GTLMotNtzt0FOhWAvdxM3taS0pizLZtlBgB6VZbinavG44VfHGZbzmJkkqGImSKwzOSyaGVLzaZ9G53klMUiOHJYD0Xsm5Yz2KNbB8WqZ1rOoLnuSlOb5TNOs/1M/XO2OjO0ObkqW7cmipCP5BAzPi0zqpgpDhlRHK0IMPSB7mwoOCVfeDWbyd9lp6lz+uxxdN9K++cnLxyLi8cNxmn7uas/egUt0sq6YZINcNq97+Dvbyyx/1bXpAbxfkmyWCS6RUiiBwB36xC3zcg6T8xbCcBdsdTOnlm307aeVHvE69AUSDtmRls5Wwhn2frSaNh+eU2WmR11Tkqt7HAfe38FAODYvapcn6cM18WMj5gZaZmRHSbteKJKALDJzZSyv9+3c6lREAwibqZcA2s2+nctwy/HDwGQtqzJzlqKmI6KmDFbZpy4hvTf8qn6q/OtXh1x/mGDXMGNviwzPl5FpwJw7qJ5OlRge2UyUXpVliqik8bM0IrehRAz7lXIc7iZiKVKH/DopGz/AZ2hY7puYc0yQzODqJtJtxQ2F3qN5TpetA6T0TJjymYyWGak5ek7e7uL3LU2Sp0ZPWamFQKAC2ElNFEcrQgw9AHtVBZTfL3076Y0v1ycc8hA++d5S9OWi98eNxw9KuK488x97b+NGdQFvzp2mPGh8ruSa66HWHawp+yTfvmOH9ULA7qmhdvGarOYkYG2tCOT1hm9H1y7PW0B0deSGdKjApaVDr499KY5mL9iq6dlhlallZaZrTXqDEgP4NUDLCnyvCpKIva5SiaOzC5mhvbMLmbKYmHPQdsWMx7r5RizmZqStpjxquBKt3sFL/ulW0W6jTLmqaIkYossapkxrcZc05iw01VbwzLjhRowao4b8DOxkPfB082UpfaLEjPTDGsYXTVbWmYiISunkGgLXAtN5rWcgfq8yZpcgHkxzBLyfJpiZvboXq6kRtMAYKffadkzTq+xyTJjipkxZjOl3GJm3/6dMf93x3hWtm5N6F3Sn9HmBwCHjT8XEhYzLSSsWWYG99BjZgxuJp8zhm4d4vbiY2MzEfk/OXxPvH/10XbGDMW0jpHX4pHqSq7qYFkWC+Pt34xXZpIHZaq9nnXQADx54Vj85fv72tkSmzwsM0s3peNF6CAqxcwBmewYiawTo4uZyrIopp64F3pXlmBzTSPOeeB9LFq13Xg8Ss+O6f1IkSShKeWlUWeJBSlmPlqxFfe9uQTJlMCWTKZV1/IYTiRlwvt0KjUWnKIM0ywz+qzcsiyjqwkAumVmf55F8wydxxfrd+LR99JWo/4eYqZv51IcP7oXjh/dy5Uani/y3kvLVXeSOeMVAFxusMzI92O/TA0aAFiyyW39aw6mWiaAlmXjo/OW3/XKDmz0KJoHaDEzzRCQtM6Mn3WZ2pK8s5mIUPQaJEui5nV9qAiUP1MxuWf3Dk7tFy01Wxa37Nmx9bK5ZPYdTaM2u5myZDNp59m5PLZL1jSiGly3HrpTs5vhZiqSbCZezqCFqGImhv36d8LBe3TBu0vSlhQpXNTVsP0/wI/9+GA88t5yHDeqV87PmiaOXumkyaRumXHaN2FET/TuVIq+nUvt4miydH0sErJ/lgOYl2Xmq0xRvCHESnHqfn3w0idr8YOD++OCbw/Cw+8ux5uLneJ/VYZYj8mHDMTp+/fFD+57Dx+v3IYFHmKGFoLrk7GQrd5Wh1RKoDGZwtJNNTjngffT5xxK14awi201JfHvD1fhV/9aYLe5NmMB6tohjqNH9MQTPzkYd8xejDPH9MvZCY0Z2AXfH9MfyzfXYJ9+nYwxLBUlEWNMjwxU9A4Adg+KMoUegMuKJLEsC389a7+s7faLnvbbjbgPvVKzHTcTiZnJXMezxw7EXXO+xuaaRozu26lV2uiV7k1FR67FBtOf9w7uBMhyBoaBgFrAmuPakxaNxiIQM7o1KKeYUdxMZiF3yJ7mwn6qCExfN+q+7FERt9ujupmStoW2Z4W/FcP9IO99S9xMubK/2go6yc0WABzLY8HIYoyZYTHTQnQxY1kWbjp1NI689XV0LY/Zswr6MufzUJfGwrjg23v4+ix9aOUaIl4dsF5imz6Q0qpBXS8j+zgxOpLumQHMK2ZGBu0OJfVNhvaswNwrxinHomKmdydzB1Qej+C4UVX4eOU219/26F6OCSOqcMJoR/BJ0/WGnQ044x/vYMWWWuzZvYPSVou417bXNeHqZxbZf6MrcssMhoP26IrHftLV2D6dSDiEaaeOyvqZdNyMajkKhyw7rkhfmdb0s4lDB7du5VcT+uJ41DJD44FMlpmdDQm8n3Gb0iDOOZcfiYfeWYZJOVx4flECgKmwIR26HzGTLbgT8E7NBrTYj2ZkkNEAdT+LTLYlLakzoz+zU08cgcfnrfR8R0yWmT6dSvHAuQegW4c4LMtSqvJKa2UiJbB6W/rd7VnZimIm6c/NZAoSN7mZdiWqmPGOmclHlBRjNhOLmRZCH2g5Ix3YrRz/+3V6wJYvdHOymfKG6JZYOKSYpnX0Etv0geyRETO0QzF11NnETE1Dwk4T1uNHKLpbqaqjdwfUw2OmNbBrOX4zSV3VtVuHGOKR9DWYl0k1N630LGd0X2+oVsTb+h31djxFa6xqa8LkZupZEbcHWy/LTCiUzj7T7223DnG8/Zvxu2TmHo+EUVkatVPfuxPLDI0NoYJCFur7fO0Ou44RtVxUlkXxy6OGtFob6SMb9oiZ8Sdm0p/3XjVbuPYr8Vtnxgtpwl+4ahumv7UMQGFqzABu93jOOjPUzaS5Is49dBDO1apNU+h1KyEZYeOHO4tCSoFKi+YBTlZgtr4kX+S970qKmioCOUvRPLvsRIEMGHt0S08my2Jhl+VFzTjzL0qaK4LakuJoRYChLzidkfbtXIa+nR1zf/eKOMYN645T9u3TZmKGKvBYDtO4Xucjplhm0i/szaePRv8uZbjvnAOM+8gmZmSRu24d4nZ5fhNdO8QVF4DJHSPxyoTqZhAblmXZriYTsqOTL7AsACdZv6MeW2rS50U7sNbElNFEa2F4xcwA5pWzB3Qt26UuCHrd6c909kozsvQA2Hgk1KriRcdUpwnI380kZ921jUn837vLlVWQARozk93N5LWUQTbkQNGUFPj3h2oBy12NbpnJ5S7342byolN5FNHMul5eligppiKaZVmKmVaNmcn0l53LzVZHU5Xom2d+gSNuec0uvOk38aO1KY2FsWjqBHz0u2Ncf4s22zLj3M9iyWZiy0wLUS0z3h2jZVmYft6YNm0L7WMjhpeLogel0QdZDvSj+3bCG1eOc385Q7aYGTtehriYTIRDFnp2LLFf+J6V3h1Qdw8x4zVI9OlUqlQbBoArJw5DVccSu36NnDHSOjZAelFJaTlpq7VT9DR0QBUzdDDQC7KVRMPYoS0e2S+LeGsLunWI20UE6b0pjYXxfz86CIlUSolj0he3fPTHB9kB7m2BGgDsbKeiw08peZraes2zn6AsFsap+/W1tzVlKZqnZjM1JwDY/Z2CBQDrFYBb4GbKRceSKB46bwzK4hHPOA65+5BlKdZKmdXXsxUtM7IfpRMbZd26zPnJ+jHzV2zFPa9/AwB4OlNFvRAZaBJTXwNkXz8rG2yZaYfQF9wrO2VXQS0z0lWhZzMlU+lCcHaJbaNlxl8nkM0yY8fL9MwuZgCgKuNqSruGvDt8LzeTKSAWUEuiTxjRE1ccOwznHzoIp+7X1w4ylb52uYL3iEy21YYd9diSWfixrdxM0i1JLU6KmMmyto08ZzpYemUxtRVUwOgBwYcN6YYjh/VQtumWGT3zr7VR0r0tKgzzczNFNf/AzE/WKb9nrTPTSpYZSsECgPOMmaGWm+ZkvBwyuFtWsRuyY2bSv+vWyqrWjJkxrMdGg/dl8dJZn21AKiUw9fnP7L+9/mW6CGmhLDPZaK6bSc1mKo6YGRYzLYS+0Ca3wa5EaDEzgNuHe+fsr3DITXPw30Xp7Bf5gtF4ES8LiI7MYNlU3eCqNGzKZPJCdjq5Op+OpRFjR+7VXlq/4rhRvXDRuMEu4SM7WdlZyUyt9TvqsbmN3UxHf6sn+nYuVWb5fUgAdLbCVDLWh1oW+u5iMUMFjJ9npkzJ7An7EhItIewRABwlP/tpgx4r8uZidTX5bHVmSlpsmXHvs2AxM3lWAKarSbdFkKi+rhe91pblFtgtwRQvtYMslXJipvjd3K82YPrby7Bg5TbXvStUAHA2QiHnHvldlwlgy0y7hL7Q+orDuxqhWGbcGRhCOH73f32QrrgrO2o6y/CydOjIOImmpHCtgSSXQcgW/CvplbEEVXXM7iaxLEupj/KrCUNx2OBuOGtMf+PnacyMXvdFotdskWJma20T1mZWzG4ry8yhg7vhf78er2Rh+Y6ZMRSo69e5cJYZP2KGdnq98lwnpzmEvCwz5Fpmcw3bn9csM3VNScz8ZB3eXbIZD7611H6nTO7IUkOKcT4Y3UwFEjP5pmYD6XowFfGIZ7xbSzh0cDfs2b0cE0em3x/ab3XrEG9V0WeKPaR93tCeFRjaswOakgK/fyFtlZlyzFDFWlqo1OxcyOepuTEzxSJmOGamhdAXfM/uuV0qbQl93eQDumZ7PRat2o7XvtyARDKFNZmVrpdkCtqNyqRcb69zry6dC5rRsnFng702VHVDwo6B8eNm2rd/ZwBLsd+ATjk/mx4Q0m29aNxgXDzeu4Po0yndkURClue90c3fo/pU2r73nXadmbYRMxLqnvTtZsp0JvS7cjXrXQUN+vVjvaKxD3oWW1vgaZkhnW+nPAKAAaAiHsHOhgQufeJj5TN7963EmQe6RXWpZo3KF5N7JihF8wDgqZ8dgvqmZIvWAvNiaM8KvHr5kfbv1LLQmplMgNkyo1v1TtuvL6a99AUAYI9u5Tjv0EFYva0OM95ZDgB2Mb9iIxYJoaYx2Xw3E6dmtx9e/9WRqE8k2yxQ1C80Zmb88B5YvKEav/73QqXqrY6sbLvVULzND1UdS7C9rgm/fWYR/nrWfujRscTOZOpeEXctfmni+NG9sP+Ao3xlH1BXRa4CT6P6VGJEr47Yu18nzwGAvpTRsIXenUrRs2McK7c49V86+ziHlkADY+kgHw5ZCFnpwG59gbg4WYvG+e6uFTPSGtOpLJr3ANt7F7SVruFF48DyT812nrPHLzwYf5u7BC8sXIOKeAQH7dEVQ3t2wM+PHGwcsNU6M8GOmdGzl/x4TcrjkTYRMiaoZaY1M5kAtd958LwD8X/vrnCVg/jRYYMwrKoCO+sTGDOoC2KREC4aN9gWM60ZkNyayOcpHwtLMbqZWMy0AgMNSwsUAhq2csnRQ/DSJ+uwYkstQhYwrKqjXdtDWh4iIQsT9krXbTjv0IG4+plPMDHHAoo6UyYMxaWPf4x5y7biuuc+xb0/3N+Ol/FjlZH4DdbLp2MsjYXx4iXfzvEZYtnoXIZwyEJVxxJbzHQqi7Z5jELXDnGcd+hAxMIhl/iLZuoFxbS4DdlxHzK4Kxat3o5uHWK73Cc/rKojwiHLDpr2g7TknbgLFtijhfyuPHaY/TMdlP24hi3LwhM/ORiNyRT26l2Ju76/L248ZSTikXBOYaHEzDTDMuPX5bsroNatkJV7MrGroUsJ+HFv++FvP9wPd8xejD+fsY+97chhPVzB7UA66ULf3rNjCd65ajz+9NIXmOSjinshsMVMHs8aVwBm2hRqCS2LRfD3s/fHA/9bijPH9MOoPp3wo4fm4fO1O3DeoYNwy8tf4oih3e3B86wx/bF3304YkocAAdKrR//7Z2Nxwl3/w0ufrMP3//EuPlqRLlI3pA2yVZqzvk02vrN3b3y0fCsEBM4ZOxCAUzQQaLu0bJ3rTtzLuD2WKfynC6qJe1Xhy3U7ceq+fTF57EBfsR+tTZ9OpXjzynF5Wa5euuTbWLqpZpdUKf7hwQPQlEzhvEMHKeuD0QrAnUr9tf2gPdTKz16prjpKNlMzLBQl0TAuO3ooUkLgzlcXAwC2N9OK2lLoI7irrYB+2F7ruMp/euSerbLPiSN72TE5zaVXZSnuIAsDFxvNiZlRLDO8NhPT2ugZRd/q1RG3fHdv+/cZ549BSqRXUR3QtQwHkw7asizjkgV+2Kt3JU7Ztw+e/mg13lmy2d6erzDyQ3OqqGZjcI8O+L8LDlK2DSRrG+1RYKub7Gh0C8Bp+/fFafv3NX1ll9I7z0De3p1K8/5OcxncowP+eIq7XD6tnNzWGVWlLbTMAGkrKwBbzGypzT++rTWg2Zq3kn6lWLjk6CH4xxtLcNv39il4ZmmQMFWpz0UxrprNYqYd0TdH0bT0eibpn08Y3bpm/l9PHI5N1Y0oiYTwymfrAQAjezdPHGXjionD8PpXG2wrSlvwo8P2QEVJFJ3Lopi4V2FNw7KjKVQ6bnukrsmJpcknHbU5tDSbycTWmsKImW8P6Y6pJ47AwXt2xfAq/67FXcUZB/bHGYYgbCY7cTtmJg83U5TdTM3innvuwS233IK1a9dir732wh133IFvfzt7LMTuyAXf3gMbdjbgmBE9c3+4lenZsQQzzk9XOJ6/YiuWbKzB6L6tL2b27N4BC66b0KazgS7lMfz0iNYxU7eUTmVRrNtR76tSLeOP2kYnIL6t4z5KY1KMWq0WuFsoy0wsEsq6nhITTJyYmTzcTM0stteWFL2YeeKJJ3DppZfinnvuwaGHHoq///3vmDRpEj777DP0788qnFISDeOGk0YWuhnYt3/nTLp121AsL8+u4ObTR+OLdTsxrJUCGhmgpsE7u6+16dOpDN06xDGoW8trAB02uBv+9/UmfGcXBE8zuw9SzOg1t7JBhQ+vzeST22+/HT/60Y9wwQUXAADuuOMOvPzyy7j33nsxbdq0AreOYdqW0X072UsvMK1DbWMi94daidJYGG9eOU5Z3LK5/PWs/fDyZ+swcWR+GYcMkw07ADgPy0w8XHxF84qjFR40Njbiww8/xIQJE5TtEyZMwNtvv238TkNDA3bs2KH8YxiGkezdhotbmiiNhZUMquZSWRbF9w7ox8GtTKsiLSv5VJYui4fRIR5BWSyMsmYs09EWFLVlZtOmTUgmk+jZU40B6dmzJ9atW2f8zrRp03D99dfviuYxDBNAfjF+MDrEIzg2z5pKDNMemTiyCl+s25lXuYRoOIRHLjgIAsXj9i9qMSPRg/SEEJ6Be1dddRWmTJli/75jxw7069evTdvHMExwKItFcNG4wYVuBsMUBafs2xen7Jt/mYddbeHMRVGLmW7duiEcDrusMBs2bHBZayTxeBzxeNuscswwDMMwTPFR1DEzsVgM+++/P2bNmqVsnzVrFg455JACtYphGIZhmGKiqC0zADBlyhScffbZOOCAAzB27Fj84x//wIoVK/DTn/600E1jGIZhGKYIKHoxc8YZZ2Dz5s244YYbsHbtWowcORIvvvgiBgwYUOimMQzDMAxTBFhCX9CnnbFjxw5UVlZi+/bt6Nix+EpwMwzDMAzjJp/xu6hjZhiGYRiGYXLBYoZhGIZhmEDDYoZhGIZhmEDDYoZhGIZhmEDDYoZhGIZhmEDDYoZhGIZhmEDDYoZhGIZhmEDDYoZhGIZhmEDDYoZhGIZhmEBT9MsZtBRZ4HjHjh0FbgnDMAzDMH6R47afhQravZjZuXMnAKBfv34FbgnDMAzDMPmyc+dOVFZWZv1Mu1+bKZVKYc2aNaioqIBlWa267x07dqBfv35YuXLlbrXu0+543rvjOQN83nzeuwd83sV53kII7Ny5E71790YolD0qpt1bZkKhEPr27dumx+jYsWNRPghtze543rvjOQN83rsbfN67F8V83rksMhIOAGYYhmEYJtCwmGEYhmEYJtCwmGkB8Xgc1113HeLxeKGbskvZHc97dzxngM+bz3v3gM87+Ofd7gOAGYZhGIZp37BlhmEYhmGYQMNihmEYhmGYQMNihmEYhmGYQMNihmEYhmGYQMNippncc889GDRoEEpKSrD//vvjzTffLHSTWpWpU6fCsizlX1VVlf13IQSmTp2K3r17o7S0FEceeSQ+/fTTAra4ebzxxhs48cQT0bt3b1iWhWeffVb5u5/zbGhowC9+8Qt069YN5eXl+M53voNVq1btwrPIn1znfe6557ru/8EHH6x8JmjnPW3aNBx44IGoqKhAjx49cPLJJ+PLL79UPtMe77ef826P9/vee+/F6NGj7YJwY8eOxUsvvWT/vT3eayD3ebfHew2wmGkWTzzxBC699FJcffXVmD9/Pr797W9j0qRJWLFiRaGb1qrstddeWLt2rf1v0aJF9t9uvvlm3H777bj77rsxb948VFVV4ZhjjrHXwgoKNTU12HvvvXH33Xcb/+7nPC+99FI888wzePzxx/G///0P1dXVOOGEE5BMJnfVaeRNrvMGgIkTJyr3/8UXX1T+HrTznjt3Li666CK8++67mDVrFhKJBCZMmICamhr7M+3xfvs5b6D93e++ffvipptuwgcffIAPPvgA48ePx0knnWQLlvZ4r4Hc5w20v3sNABBM3owZM0b89Kc/VbYNHz5c/OY3vylQi1qf6667Tuy9997Gv6VSKVFVVSVuuukme1t9fb2orKwUf/vb33ZRC1sfAOKZZ56xf/dzntu2bRPRaFQ8/vjj9mdWr14tQqGQmDlz5i5re0vQz1sIISZPnixOOukkz++0h/PesGGDACDmzp0rhNh97rd+3kLsHvdbCCE6d+4s7rvvvt3mXkvkeQvRfu81W2bypLGxER9++CEmTJigbJ8wYQLefvvtArWqbVi8eDF69+6NQYMG4cwzz8SSJUsAAEuXLsW6deuUaxCPx3HEEUe0q2vg5zw//PBDNDU1KZ/p3bs3Ro4cGfhr8frrr6NHjx4YOnQofvzjH2PDhg3239rDeW/fvh0A0KVLFwC7z/3Wz1vSnu93MpnE448/jpqaGowdO3a3udf6eUva471u9wtNtjabNm1CMplEz549le09e/bEunXrCtSq1ueggw7CjBkzMHToUKxfvx5/+MMfcMghh+DTTz+1z9N0DZYvX16I5rYJfs5z3bp1iMVi6Ny5s+szQX4eJk2ahO9+97sYMGAAli5dit/97ncYP348PvzwQ8Tj8cCftxACU6ZMwWGHHYaRI0cC2D3ut+m8gfZ7vxctWoSxY8eivr4eHTp0wDPPPIMRI0bYg3J7vdde5w2033vNYqaZWJal/C6EcG0LMpMmTbJ/HjVqFMaOHYs999wTDz30kB0s1t6vgaQ55xn0a3HGGWfYP48cORIHHHAABgwYgP/+97849dRTPb8XlPO++OKLsXDhQvzvf/9z/a0932+v826v93vYsGH4+OOPsW3bNjz11FOYPHky5s6da/+9vd5rr/MeMWJEu73X7GbKk27duiEcDrsU6oYNG1wqvz1RXl6OUaNGYfHixXZWU3u/Bn7Os6qqCo2Njdi6davnZ9oDvXr1woABA7B48WIAwT7vX/ziF3juuefw2muvoW/fvvb29n6/vc7bRHu537FYDIMHD8YBBxyAadOmYe+998add97Z7u+113mbaC/3msVMnsRiMey///6YNWuWsn3WrFk45JBDCtSqtqehoQGff/45evXqhUGDBqGqqkq5Bo2NjZg7d267ugZ+znP//fdHNBpVPrN27Vp88skn7epabN68GStXrkSvXr0ABPO8hRC4+OKL8fTTT2POnDkYNGiQ8vf2er9znbeJ9nC/TQgh0NDQ0G7vtRfyvE20m3u9y0OO2wGPP/64iEaj4v777xefffaZuPTSS0V5eblYtmxZoZvWalx++eXi9ddfF0uWLBHvvvuuOOGEE0RFRYV9jjfddJOorKwUTz/9tFi0aJH4/ve/L3r16iV27NhR4Jbnx86dO8X8+fPF/PnzBQBx++23i/nz54vly5cLIfyd509/+lPRt29fMXv2bPHRRx+J8ePHi7333lskEolCnVZOsp33zp07xeWXXy7efvttsXTpUvHaa6+JsWPHij59+gT6vH/2s5+JyspK8frrr4u1a9fa/2pra+3PtMf7neu82+v9vuqqq8Qbb7whli5dKhYuXCh++9vfilAoJF555RUhRPu810JkP+/2eq+FEILFTDP561//KgYMGCBisZjYb7/9lDTH9sAZZ5whevXqJaLRqOjdu7c49dRTxaeffmr/PZVKieuuu05UVVWJeDwuDj/8cLFo0aICtrh5vPbaawKA69/kyZOFEP7Os66uTlx88cWiS5cuorS0VJxwwglixYoVBTgb/2Q779raWjFhwgTRvXt3EY1GRf/+/cXkyZNd5xS08zadLwAxffp0+zPt8X7nOu/2er/PP/98u4/u3r27OOqoo2whI0T7vNdCZD/v9nqvhRDCEkKIXWcHYhiGYRiGaV04ZoZhGIZhmEDDYoZhGOb/27ufV/i+OI7jL3z9aK7F+BWRmvJjNhQlNiyEGhYWKElpZGPEhvIPKLJgQTG7MQ0lio1sWbCQXwtlxQxl48fGzxiD70Kf6TN9Pn0WXz7D7ft81F3c0znve85dvTq30wVgaoQZAABgaoQZAABgaoQZAABgaoQZAABgaoQZAABgaoQZAABgaoQZAFHldDoVExPzy3V0dPTVUwNgUv989QQA/P84HA55PJ6ItoyMjIj7YDCohISEaE4LgEmxMwMg6hITE5WVlRVx1dTUqLe3V/39/UpPT1ddXZ0kaXx8XMXFxTIMQ7m5uerp6dHd3V241szMjKxWq1ZWVmS322WxWNTS0qL7+3t5vV7ZbDalpKSor69PLy8v4XHBYFCDg4PKycmRYRiqqKjQ+vp6tF8FgE/AzgyAb8Pr9crlcmlzc1M/fhsXGxuriYkJ2Ww2BQIB9fT0aHBwUFNTU+FxDw8PmpiY0Pz8vG5vb9XU1KSmpiZZrVatrq7K7/erublZlZWVam1tlSR1dnbq5ORE8/Pzys7O1vLyshwOhw4ODlRQUPAl6wfw3/CjSQBR5XQ6NTs7q6SkpHBbfX29Li8vdX19rf39/T+OX1xclMvl0tXVlaT3nZnOzk4dHR0pLy9PktTd3S2fz6fz83MlJydLev+0ZbPZ5Ha7dXx8rIKCAp2dnSk7Oztcu7a2VuXl5RoeHv7sZQP4i9iZARB11dXVmp6eDt8bhqG2tjaVlZX90ndtbU3Dw8M6PDzUzc2NQqGQHh8fdX9/L8MwJEkWiyUcZCQpMzNTNpstHGR+tF1cXEiS9vb29Pb2psLCwohnPT09KS0t7VPXCuDvI8wAiDrDMJSfn//b9p+dnp6qoaFB3d3dGhoaUmpqqjY2NtTV1aXn5+dwv/j4+IhxMTExv217fX2VJL2+viouLk67u7uKi4uL6PdzAAJgDoQZAN/Wzs6OQqGQxsbGFBv7fl5hYWHhw3VLS0v18vKii4sLVVVVfbgegK/FaSYA31ZeXp5CoZAmJyfl9/vl8/nkdrs/XLewsFDt7e3q6OjQ0tKSAoGAtre3NTo6qtXV1U+YOYBoIswA+LZKSko0Pj6u0dFRFRUVaW5uTiMjI59S2+PxqKOjQwMDA7Lb7WpsbNTW1pZyc3M/pT6A6OE0EwAAMDV2ZgAAgKkRZgAAgKkRZgAAgKkRZgAAgKkRZgAAgKkRZgAAgKkRZgAAgKkRZgAAgKkRZgAAgKkRZgAAgKkRZgAAgKkRZgAAgKn9C8ksfcSQAIh6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract video features using ViViT\n",
    "video_features = extract_vivit_features(preprocessed_frames)\n",
    "\n",
    "# Detect scene transitions\n",
    "scene_transition_indices = detect_scene_transitions(video_features)\n",
    "\n",
    "# Cluster the scene transitions\n",
    "clustered_scenes = cluster_scene_transitions(video_features, scene_transition_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Object Detection with YOLOv5\n",
    "yolo_model = YOLO(\"yolov5xu.pt\").to(device)  # Replace \"yolov5s.pt\" with the correct path if necessary\n",
    "\n",
    "def detect_objects_in_frame(frame):\n",
    "    # YOLOv5 expects PIL images\n",
    "    pil_frame = Image.fromarray(frame)\n",
    "    results = yolo_model(pil_frame)\n",
    "    detected_objects = []\n",
    "    for result in results:\n",
    "        for cls in result.boxes.cls:\n",
    "            cls_name = yolo_model.names[int(cls)]\n",
    "            detected_objects.append(cls_name)\n",
    "    return detected_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad300de3843d4c4188cbc9473f30a391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farha\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\farha\\.cache\\huggingface\\hub\\models--Salesforce--blip-image-captioning-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BlipForConditionalGeneration were not initialized from the model checkpoint at Salesforce/blip-image-captioning-large and are newly initialized: ['text_decoder.cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 person, 27.0ms\n",
      "Speed: 3.9ms preprocess, 27.0ms inference, 153.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Description for Scene 1: Objects detected: person. Visual: there is a woman that is standing in front of a projector screen with her hands on her hips and a black shirt and red skirt with a red and black top and white striped skirt and grey skirt are standing next to a man who is holding a white board that has his hand on his right hand in the other hand with his left side of his chest and he is wearing a gray shirt with black and blue and gray and the same pants and his shirt, and is his pants, a blue, he has a brown and there are a grey shirt that are both are his arms are on the bottom, with red, his legs, in his neck, both with the back, on him and her, while he are in a maroon and him is on, the right, her and with him with two legs and she is in her neck and on a neck with both, she are wearing his and are with?? is he? are, that his, is her with, are? and that? with he and? a shirt is?,? his? her? on and in? she? that with she, there? of him are are her is the? in that she with an? he's?s are is,\n",
      "\n",
      "0: 640x640 (no detections), 29.0ms\n",
      "Speed: 3.0ms preprocess, 29.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Description for Scene 2: Objects detected: . Visual: stanford engineering logo on a white background with a red and black logo in the middle of the letters stanford, and an image of a man in a suit and a woman standing on the right side of an engineer's neck with his arms in front of his back to the back of him and behind him, with the text that says, that is a sign that reads, is the words, in his head and the man, on his face, the same, he is behind the other on that he are the front, a person, to him is on, are in that are his and on and he'i are on him?? that, at the bottom of that?,? are, i? and in this is, this? with? a? is? in? on? of? the? at? he? or that man? s? to me, for the sky? i, of me? - and thats? we are? about? it? from the sign, or the name, it, we? she? you? /? out? by? an? this man and with that and i am? what? there? for that on it is that s are at that with an i '?\n",
      "\n",
      "0: 640x640 1 tv, 1 laptop, 28.0ms\n",
      "Speed: 2.0ms preprocess, 28.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Description for Scene 3: Objects detected: laptop, tv. Visual: a screenshot of a computer screen with a message that reads, take a break from the war\n",
      "\n",
      "0: 640x640 1 laptop, 29.0ms\n",
      "Speed: 2.0ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Description for Scene 4: Objects detected: laptop. Visual: a screenshot of a computer screen with a message in the middle of the screen and an image of an object on the right side of it\n",
      "\n",
      "0: 640x640 1 laptop, 29.0ms\n",
      "Speed: 2.0ms preprocess, 29.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Description for Scene 5: Objects detected: laptop. Visual: a screenshot of a computer screen with a message that reads, don't know how to use it\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Generate descriptions with YOLOv5 and BLIP\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", load_in_8bit=True)\n",
    "\n",
    "# blip_model.to(device)\n",
    "\n",
    "blip_model.eval()\n",
    "\n",
    "descriptions = []\n",
    "for i, transition_frame in enumerate(clustered_scenes):\n",
    "    key_frame = preprocessed_frames[transition_frame]\n",
    "    # Convert back to HWC format and to uint8\n",
    "    key_frame_img_uint8 = (key_frame * 255).astype(np.uint8).transpose(1, 2, 0)\n",
    "\n",
    "    # Detect objects using YOLOv5\n",
    "    detected_objects = detect_objects_in_frame(key_frame_img_uint8)\n",
    "\n",
    "    # BLIP Description Generation\n",
    "    inputs = blip_processor(images=key_frame_img_uint8, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Modify the generation parameters to increase descriptiveness\n",
    "    outputs = blip_model.generate(**inputs, max_new_tokens=250, num_beams=10, length_penalty=2.0, no_repeat_ngram_size=2)\n",
    "\n",
    "    visual_description = blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Combine detected objects and BLIP description\n",
    "    combined_description = f\"Objects detected: {', '.join(detected_objects)}. Visual: {visual_description}\"\n",
    "\n",
    "    descriptions.append(f\"Scene {i+1}: {combined_description}\")\n",
    "    print(f\"Description for Scene {i+1}: {combined_description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Text Detection with Google Cloud Vision\n",
    "def detect_text_from_frames(frame_folder):\n",
    "    text_data = []\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    # Iterate over all frames in the folder\n",
    "    for frame_file in os.listdir(frame_folder):\n",
    "        frame_path = os.path.join(frame_folder, frame_file)\n",
    "\n",
    "        # Read the frame image\n",
    "        with open(frame_path, \"rb\") as image_file:\n",
    "            content = image_file.read()\n",
    "\n",
    "        # Perform text detection on the frame\n",
    "        image = vision.Image(content=content)\n",
    "        response = client.text_detection(image=image)\n",
    "        texts = response.text_annotations\n",
    "\n",
    "        # Collect all detected texts from this frame\n",
    "        frame_texts = [text.description for text in texts]\n",
    "\n",
    "        # Join all the detected text in this frame into a single string\n",
    "        detected_text_str = \" \".join(frame_texts)\n",
    "        if detected_text_str:  # If there was any text detected\n",
    "            text_data.append(f\"Detected in {frame_file}: {detected_text_str}\")\n",
    "\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1.42G/1.42G [04:58<00:00, 5.12MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Combine Descriptions and Audio Transcription\n",
    "video_transcript = transcribe_audio(video_path)  # Transcribe the video\n",
    "detected_texts_from_frames = detect_text_from_frames(\"./video_frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meaningfully combine the transcript, descriptions, and detected text\n",
    "combined_text = f\"{video_transcript} {' '.join(descriptions)} {' '.join(detected_texts_from_frames)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEBUGGING PURPOSES ONLY\n",
    "save_path = \"combined_text.txt\"\n",
    "\n",
    "# # Save the combined text.\n",
    "# with open(save_path, \"w\") as f:\n",
    "#     f.write(combined_text)\n",
    "\n",
    "# Load the combined text.\n",
    "with open(save_path, \"r\") as f:\n",
    "    combined_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Final Summary using summarization model\n",
    "model_name = \"facebook/bart-large-cnn\"  # Use mBART for summarization\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_text = summarize_text(combined_text, summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "A policy is a set of actions that, uh, the agent should take in the world. Does anybody remember what a model was? Yeah. A model is like a representation of the world and how that changes in response to agents' actions. The reward model specifies what is the expected reward, um, that the agent receives from taking a particular action. How do we do the planning problem? So, we're not going to talk about learning today. We're just going to talks about the problem of figuring out what is the right thing to do. When your actions may have delayed consequences, which means that you may have to sacrifice immediate reward in order to maximize long-term reward. The Markov decision process is where we think about an agent interacting with the world. So, the agent gets to take actions typically denoted by A. Those affect the state of the world in some way, um, and then the agent receives back a state and a reward. A Markov chain is a sequence of random states where the transition dynamic satisfies this Markov property. And so essentially it allows us to say that the future is independent of the past given some current aggregate statistic about the present. The transition dynamics. is that you have, um, a finite or potentially infinite set of states and you have a dynamics model, which. specifies the probability of the next state given the previous state. And in this case, uh, the transition dynamics, it doesn't, we don't actually have actions yet and we just think of it as sort of, maybe it already has some way it's moving in the world. Which dimension represents the start state? Um, so this is a great question. Well, which dimension, which, which state is the startstate? I'm not specifying that here. So in this particular case, you could have it as, um, the transition of saying if you start in state. In this case, we're just thinking of Markov chains. So it's as if your, let's say your agent, um, had some configuration of its motors. And then it just starts moving about. And what this would say is, this is the transition probabilities of if that agent starts in state S1. The environment you're in is described as a Markov process, and this describes the dynamics of that process. We're not talking about how you would estimate those. This is really as if this is how that world works. The model is just specifying that the transition model over how the world works over time. It would specify the distribution over next states that you would be in. So for example, if we were looking at state S1, um, it has a 0.6 chance of staying in S1. In this case, we have similar dynamics from S4. From S4, it has a probability of 0.4 going to state S3. So you're just sampling from this transition matrix to generate a particular trajectory. So that was just a Markov chain. Next, we're gonna add in rewards. The rewards for the Markov decision process can either be a function of the state, the state in action, or state action next state. Right now, we're still in Markov reward processes, so there's no action. So in this case, um, the ways you could define rewards would either be over the immediate state or state at nextState. A horizon is just the number of time steps in an episode. A return just says, if I start off in time step t, what is the immediate reward I get? In the general case, we're going to be interested in these stochastic decision processes, which means averages will be different than particular runs. So what I mean by deterministic is that if you always go to the same next state, um, no matter which, if there's only a single next state you can go to, then the expectation is equivalent to a single return. If your horizon is always guaranteed to be finite, it's fine to use Gamma equal to one in terms of, from the perspective of mathematical convenience. If your future rewards are exactly as beneficial to you as the immediate rewards, then that means that your discount factor is correct. Visual: stanford engineering logo on a white background with a red and black logo in the middle of the letters stan Ford. Image of a man in a suit and a woman standing on the right side of an engineer's neck with his arms in front of his back. Screen Mirroring Document Screenshot Focus NNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values) y = 0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward Discount Factor - Mathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor <1 0: Only care about immediate reward 1: Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1001.jpg: Discount Factor lecture- lecture2-1 lecture2 -2. Discount Factor is Mathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor < 1 0: Only care about immediate reward 7 = 1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Screen M Ос Form NININN Detected in frame_1004.jpg: Discount Factor [ lecture2-2 x) Screen Mirroring Document Screenshot Focus NNN .. IN Discount Factor ACTIVE-X is a mathematical formula that avoids infinite returns and values. Humans often act as if there's a discount factor <1 0: Only care about immediate reward = 1: Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y = 1 Sen Men NNN can also use the Discount Factor. If episode lengths are always finite, can use y = 1. Mathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor <1 0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y = 1 San M D - IN T1 Im Discount Factor. Mathematically convenient ( avoid infinite returns and values) Humans often act as if there's a discount factor < 1 0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward Human often act as if there's a discount factor <1 y=0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y = 1 Screen Mirroring Document Screenshot 18 Рос NNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values) 1 Serw X From NNN Discount Factor 11- Mathematically convenient ( avoid infinite returns and values) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Screen Mon Сос Screenshot Focus NNNN \" Discount Factor x. Focus NNNN Detected in frame_1016.jpg: Discount Factor 11- • Mathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor < 1 0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Sen M X From NNN Focus NNNN Detected in frame_1018.jpg: Discount Factor • Mathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor < 1 0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Screen Mirroring Document Screenshot NININN Discount Factor. If episode lengths are always finite, can use y = 1 Screen Mirroring Document Screenshot NININN Detected in frame_1020.jpg: Discount Factor ec lecture2-1 lecture2,2. Mathematically convenient (avoid infinite returns and values)  Detected in frame_1022.jpg: Discount Factor • Mathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor < 1 0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Screening Docum Screenshot Facia ' NININN Discount Factor isMathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor. Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Screen Mirroring Document Screenshots. 20  Рапи IS NNN Discount Factor • Mathematically convenient ( avoid infinite returns and values) Humans often act as if there's a discount factor < 1 0: Only care about immediate reward 7 = 1: Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y = 1. Human often act as if there's a discount factor <1 0: Only care about immediate reward = 1: Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y = 1. = 1 Screen M D Scho Fo ININ N 21 x Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Human often act as if there's a discount factor < 1 0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y = 1 Screening Document Screenshot Focus NININN Discount Factor 2-1: Mathematically convenient (avoid infinite returns and values) Discount Factor - NNNN • Mathematically convenient ( avoid infinite returns and values) Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y =1. Humans often act as if there's a discount factor < 1 0: Only care about immediate reward 7 = 1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Screening Document Screenshot A Focus NNNN \" Discount Factor • Mathematically convenient ( avoid infinite returns and values) Human often act as if there's a discount factor < 1 0: Only care about immediate reward 7 = 1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Screen Moring Сос Screenshot Focus NNNN \" Detected in frame_1039.jpg.  Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 7 = 1 : Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Screen Mirroring Document Screenshot focus \" NNNNN Discount Factor\" Human often act as if there's a discount factor <1 0: Only care about immediate reward y= 1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use = 1 Son M Do 50 Роси IN NNN Q Discount Factor. Screen Mirroring Document Screenshot Focus NNNNI Discount Factor. Mathematically convenient ( avoid infinite returns and values) Humans often act as if there's a discount factor. discount factor < 1 0: Only care about immediate reward 1: Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y = 1 Screening Document Fea NININN Detected in frame_1044.jpg. Human often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Screen Mirroring Document Screenshot Focus NNNN ' Human often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward. If episode lengths are always finite , can use y = 1 Screen M D Fre NNN .. Detected in frame_1048.jpg: Discount Factor lecture2-1 lectures2-2 • Mathematically convenient (avoid infinite returns and values) Discount Factor isMathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor < 1 0: Only care about immediate reward 1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Screen M D From NNN. Focus NNNN ' Detected in frame_1052.jpg: Discount Factor 2-2. Mathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor < 1 0: Only care about immediate reward 71: Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y = 1 Screening Document Screenshot Focus NNN.. \" Human often act as if there's a discount factor <1 0: Only care about immediate reward = 1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use = 1 Screen D ●ININN \" N Discount Factor • Mathematically convenient ( avoid infinite returns and values)  Humans often act as if there's a discount factor <1 y=0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1. Human often act as if there's a discount factor. factor <1 0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use = 1 Son Mwen D \" IN Discount Factor. Focus NNNN Discount Factor is Mathematically convenient ( avoid infinite returns and values) Humans often act as if there's a discount factor <1 y= 0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y = 1. Do From NNN Discount Factor isMathematically convenient ( avoid infinite returns and values) Humans often act as if there's a discount factor < 1 0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y = 1 Screen Ming Docum Screenshot NNNN Discount Factor . If episode lengths are always finite, can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values) Discount Factor lecture2-2 is Mathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y =1.  Discount Factor lecture2-2 isMathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor < 1 y=0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y = 1 Screen Mirroring Document Screenshot Focus NNNN  Humans often act as if there's a discount factor < 1 y=0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1. Mathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite, can use = 1 Sam M D IN NNNN Discount Factor 71- • Mathematically convenient ( avoid infinite returns and values) Focus \" Discount Factor is Mathematically convenient ( avoid infinite returns and values) Humans often act as if there's a discount factor < 1 y=0: Only care about immediate reward y=1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Screen Mirroring Document Screenshot A Focus \" Detected in. frame_1076.jpg: Discount Factor . 2-2 K NNNN. Human often act as if there's a discount factor < 1 y = 0: Only care about immediate reward 1: Future reward is as beneficial as immediate reward If episode lengths are always finite, can use y = 1 Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1079.jpg: Discount Factor 11- 1. If episode lengths are always finite, can use y = 1 Son Mag F IN NNNN Discount Factor 11- 1. Mathematically convenient ( avoid infinite returns and values) If episode lengths are always finite , can use y = 1 Screen Merong Doc Screenshot 18 Ро  ' NINN Discount Factor. Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor. S 10 ро N Detected in frame_121.jpg: Models, Policies, Values истин2-1. Model: Mathematical models of dynamics and reward. Policy: Function mapping agent's states to actions. Value function: future rewards from being in a state and/or action when following a particular policy. Focus INNINN Detected in frame_123.jpg: lecture2-1 Models, Policies, Values NNNN. Policy: Function mapping agent's states to actions. Value function: future rewards from being in a state and/or action when following a particular policy. Model: Mathematical models of dynamics and reward Policy: Function mapping agent's states to actions. Value function: future rewards from being in a state and / or action when following a particular policy. Model: Mathematical models of dynamics and reward Policy: Function mapping agent's states to actions. Value function: future rewards from being in a state and/or action when following a particular policy. Model of how the world works Sex NINN Detected in frame_130.jpg: D Models, Policies, Values 2-1 NNNN. Policy: Function mapping agent's states to actions. Value: future rewards from being in a state and/or action when following a particular policy. Model: Mathematical models of dynamics and reward. Policy: Function mapping agent's states to actions. Value function: future rewards from being in a state and / or action when following a particular policy. Sein M Doc Рощи - IN NNN Detected in frame_133.jpg. Screen Mon D 58 Рапени Detected in frame_136.jpg: -1 2-2 Models, Policies, Values NNNN ⚫ Model: Mathematical models of dynamics and reward • Policy: Function mapping agent's states to actions. Model: Mathematical models of dynamics and reward Policy: Function mapping agent's states to actions. Value function: future rewards from being in a state and/or action when following a particular policy. Given a model of the world Markov Processes. Markov Rewind Punesse (MRP) Markov Decision Processes (MDPS) Evaluation and Control in MDPs. Next Time: Policy evaluation when don't have model of how the world works. Focus NNNN Detected in frame_143.jpg: lecture. Today: Given a model of the world • Markov Processes Markov Reward Processes ( MRPS) •. Markov Decisionprocesses (MDPs) • Evaluation and Control in MDPS lecture2-2 Screen Mirroring Document. Screenshot A Focus A Focus. Today: Given a model of the world • Markov Processes. Processes (MDPs) • Evaluation and Control in MDPS NNNN Stop Screen Mirroring Document Screenshot Focus. Detected in frame_147.jpg: Today: given a model. of. the world. This Time: • Making good decisions given a Markov decision process. Next Time: ⚫ Policy evaluation when don't have a model of how the world works. Given a model of the world. Markov Processes • Markov Reward Processes ( MRPs) Markov Decision Processes. (MDPs) Evaluation and Control in MDPS lecture2-2 Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_152.jpg: Today: Given a models of the. world. Focus NNNN Detected in frame_155.jpg: Today: Given a model of the world • Markov Processes •. Markov Reward Processes (MRPs) • MarkOV Decision Processes. (MDPs) • Evaluation and Control in MDPs Screen Mirroring Document Screenshot A focus 2-2 K Detected in. frame_156.jpg. Markov Decision Processes (MDPs) Evaluation and Control in MDPS lecture2-2 Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_159.jpg: Documents lecture. Today: Given a model of the world lecture2-2-1-lecture-2 and-2. Today: Given model of world: Markov Processes • Markov Reward Processes (MRPs), Markov Decision processes (MDPs), Evaluation and Control in MDPS 2-2 Screen Mirroring. Jecture Today: Given a model of the world. Markov Processes • Markov Reward Processes ( MRPs) Markov Decision processes (MDPs) Evaluation and Control in MDPS lecture. Screen Mirroring Document Screenshot focus NNNN Detected in frame_164.jpg: Jecture. Today: Given a model of the world Markov Processes • Markov Reward Processes ( MRPs) Markov Decision processes (MDPs) Evaluation and Control in MDPS NNNN Stop Screen Mirroring Document Screenshot Focus 1 Detected in frame_168.jpg: Documents Today: Given model of world lecture2-1. Today's Plan 2-1 x: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works. Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS 2-2 Screen Mirroring Document Screenshot Focus NNNN . Detected in frame_172.jpg: Today: Given a model of the world. Markov Processes • Markov Reward Processes. ( MRPS) Markov Decisionprocesses (MDPs) • Evaluation in MD PS lecture2-2. Chiservabilly Markus Deusion Press (MDP) State Rewardi World Adions Agent MDF can hop number of me bug problem admings BE MOR    BE MOR. MDPs can model a huge number of interesting problems and settings. Full Observability: Markov Decision Process (MDP) State s Reward r World Agent Action a NNUN. MDPs can model a huge number of interesting problems and settings Bandits: single state MDP • Optimal Shap Screen Mirroring Screenshot focia. MDPs can model a huge number of interesting problems and settings Bandits: single state MDP Optimal Screen M Screenshot + Focus S Full Observability : Markov Decision. Process ( MDP ) State s Reward r World Agent Action a NNNN. MDPs. model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen M Screenshot + Focus S Detected in frame_184.jpg: Full Observability: Markov Decision Process (MDP) State s Reward r World Agent Action a N.  Detected in frame_187.jpg: Full Observability, Markov Decision Process (MDP) State's, Reward r World Agent Action a. MDPs can model a huge number of interesting problems and settings Bandits single state MDP Optimal. This Time: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works. Today's Plan lecture cture-cture2-2 NNNN. MDPs can model a huge number of interesting problems and settings Bandits: single state MDP Optimal Screen Mirroring Screenshot Focus. Screenshot i Focus ' S NININN Detected in frame_192. Full Observability: Markov Decision Process (MDP) State's Reward r World Agent Action a NNNN. MDPs can model a huge number of interesting problems and settings. Bandits: single state MDP • Optimal ■ Shop Screen Mirroring Document Screenshots. Full Observability: Markov Decision Process (MDP) obscve States Reward r World Agent Action a NNNN. MDPs can model a huge number of interesting problems and settings Bandits: single state MDP • Optimal Seven Min. Screen Mining 10  NININN Detected in frame_200.jpg: Full Observability: Markov Decision Process (MDP) obscruzti World States Reward r Agent Action a NNNN. MDPs can model a huge number of interesting problems and settings. MDPs can model a huge number of interesting problems and settings. Bandits: single state MDP IS can be used to test different types of MDP. MDPs can model a huge number of interesting problems and settings Bandits: single state MDP • Optimal Screen Mo IN ININ N Full Observability: Markov Decision Process (MDP) observation World States Reward r Agent Action a. This Time: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works. Stop Screen Mirroring Document Screenshot Focus Today's Plan lecture2-2 NNNN. Full Observability: Markov Decision Process (MDP) obscrvation World States Reward r Agent Action a. MDPs can model a huge number of interesting problems and settings Bandits: single state MDP • Optimal ■ Seren Ming NININN MDPs. can model a huge number of interesting problems and settings Bandits: single state MDP • Optimal Screen M Full Observability: Markov Decision Process (MDP) obscruation World States Reward r Agent Action a. Markov Decision Process ( MDP) obscruation World States Reward r Agent Action a NNNN. MDPs can model a huge number of interesting problems and settings. Bandits: single state MDP. This Time : Making good decisions given a Markov decision process. Next Time : Policy evaluation when don't have a model of how the world works. MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen M NINN Detected in frame_223.jpg: Full Observability: Markov Decision Process (MDP) obscrvation World States Reward r Agent Action a MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen M Do NNN Detected in frame_226. MDPs can model a huge number of interesting problems and settings Bandits: single state MDP • Optimal ■ Screening D IS Full Observability: Markov Decision Process (MDP) observation World State's Reward r Agent Action a. MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal ■ Screen M Screenshot 11 G Focus 'S NININN Detected in frame_231. M Sc screenshot 11 G focus 'S. NINinN Full Observability: Markov Decision Process (MDP) observability World States. Markov Decision Process (MDP) can model a huge number of interesting problems and settings. Bandits : single state MDP • Optimal Se Screen Сс Screenshot 11 Focus S NININN Detected in frame_234. State St is Markov if and only if : P ( St + 1 | St , at ) = P ( ( St+1 | ht, at ) Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus NNNN ' This Time: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works. Stop Screen Mirroring Document Screenshot Focus Today's Plan State St is Markov if and only if: P ( St + 1 | St , at ) = P (St + 1|ht, at ) Future is independent of past given present Stop Screen Mirroring Document Screenshot 50 Роси NNNNN \" Detected in frame_242.jpg: Recall: Markov Property x lecture2-2. State St is Markov if and only if: P ( St + 1 | Stat ) = P (St +1 | ht , at ) Future is independent of past given present NNNN Stop Screen Mirroring Document Screenshot Focus . State St is Markov if and only if: P ( St + 1 | Stat ) = P (St +1 | ht , at ) Future is independent of past given present NNNN Screen Mirroring Document Screenshot Focus \" State St is Markov if and only if: P ( St + 1 | St , at ) = P (St +1 | ht , at) Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus NNNNNN \" State St is Markov if and only if : P ( St + 1 | St , at ) = P (St+1|ht, at) Future is independent of past given present NNNN Stop Screen Mirroring Document Screenshot Focus \" State St is Markov if and only if: P(St+1|St, at) = P ( St + 1 | ht , at) times Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus NNNN \" Detected in frame_255.jpg: Recall: Markov Property x lecture2-2. State St is Markov if and only if: P ( St + 1 | Stat ) = P ( ( St | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot Focus NININN . State St is Markov. Markov if and only if: P(St+1 Star) = P( St+1|ht, at) timestep Future is independent of past given present Song Doc F IN NNNN Detected in frame_259.jpg: Today's Plan N NNNNI. Next Time: Policy evaluation when don't have a model of how the world works Screening D Screenshot Detected in frame_260.jpg: Documents Recall: Markov Property x lecture2-2. State St is Markov if and only if: P(St+1 St, at) = P ( St + 1 | ht , at) timestep Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus NNNN. State St is Markov if and only if: acl P ( St + 1 Stat ) = P (St + 1 | ht , at ) timestep Future is independent of past given present NNNN Screen Mirroring Document Screenshot A Focus \" Detected in frame_263.jpg: Recall: Markov Property • Information state: sufficient statistic of history. State S, is Markov if and only if: P(St+1|St. a) timestep action P (St + 1 | ht, at) Future is independent of past given present Screen Mirroring Document Screenshot Focus NNNN \" Detected in frame_265.jpg: Recall: Markov Property 2-2 • Information state: sufficient statistic of history. State St is Markov if and only if: action P ( St + 1 Stat) = P (St + 1 | ht , at ) timestep Future is independent of past given present K Screen Mirroring Document Screenshot focus NNNN Detected in frame_269.jpg: 2-1 Recall: Markov Property • Information state: sufficient statistic of history. Focus - NNNN ' Detected in frame_27.jpg: Today's Plan x 8 NNNn. This Time: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works. State St is Markov if and only if: achon کا P ( St + 1 | Stat ) = P (St +1 | ht , at ) timestep Future is independent of past given present. Markov Property 2-1 So A • Information state: sufficient statistic of history. State St is Markov if and only if: achon کا P ( St + 1 Stat ) = P (St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot FOCU \" Recall : Markov Property - NNNN So do ro S , .... • Information state: sufficient statistic of history State st is Markov if and only if: achon کا P ( St + 1 | Stat ) = P (St +1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot FOCU \" Detected in frame_276.jpg: Recall: Markov Property - NNNN So do ro S,..... Sy. State St is Markov if and only if: achon کا P(St+1|St, at) = P ( St + 1 | ht, at ) timestep Future is independent of past given present ecture2-2 So do ro S , ..... ST Screen Mirroring Document Screenshot CA) Focus NNNN \" Documents Recall : Markov Property • Information state : sufficient statistic of history. Today's Plan: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works. State St is Markov if and only if: achon کا P ( St + 1 Stat) = P(St+1|ht, at) timestep Future is independent of past given present Screen Mirroring Document Screenshot Focu \" 2-2 K Recall: Markov Property 2-1 - K NNNN So do ro S,.... ST. State St is Markov if and only if: · action . کا P ( St + 1 St , at ) = P (St + 1 | ht, at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot Focu \" Detected in frame_284.jpg: Documents Recall: Markov Property lecture2-2 NNNN So do r. s,.... Sy Information state: sufficient statistic of history. State St is Markov if and only if: achon کا P ( St + 1 Stat ) = P (St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot focus ST NNNN Detected in frame_286.jpg: Recall: Markov Property - NNN So do r. s,.... ST • Information state: sufficient statistic of history. State St is Markov if and only if: achon کا So do r. s,.... Sy P(St+1 St. a) = P (St + 1 | heat) timestep Future is independent of past given present Screen Mirroring Document Screenshot fo \" detected in frame_289.jpg: ecall: Markov Property. This Time: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works. Markov Process or Markov Chain Memoryless random process. Sequence of random states with Markov property. P is dynamics/transition model that specifices plsss = 5. If finite number (N) of states, can express P as a matrix. P = P ( 51 SN ) P ( SSN) P ( SNISN ) Markov Process or Markov Chain Memoryless random process. Markov Process is a (finite) set of states (s = S) P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix P(S|$1) P(52|51) P (SN 51)\\ P(51 $2) P($2 $2), P(SN $2 ) P = P (S1 SN ) P (52 SN) P ($2 Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S | $ 1) P ( 52 | 51 ) P ( SN 51 ) \\ P ( 51 $ 2) P  P ( $ 2 $ 2 ) P ($251) P(SN 51) S D Sorshot For IS NIN N Detected in frame_294.jpg. Markov Process or Markov Chain - Memoryless random process Sequence of random states with Markov property. Markov Process is a (finite) set of states (s = S) P is dynamics/transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) If finite number (N) of states, can express P as a matrix P (515) P (525) P(SNS), P (552) P('SN $2), P= P (SSN), P('S2SN), P(SN SN), ININ N Markov Process or Markov Chain - Memoryless random process Sequence of random states with Markov property. Markov Process is a (finite) set of states (SES) P is dynamics/transition model that specifices p(5.5' = 5) Note no rewards, no actions. Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix P(S|S1) P(51 $2), P(251) P (SN 51), P (52 $2) and P ( SN $ 2) Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = 5 ' 5 = 5) If finite number (N) of states, can express P as a matrix P (55) P (5252) P(SNS) P ($2) P = P(SSN), P (S2SN), P(SN SN) S NNNNN 3 Markov Process is a (finite) set of states (s = S) This Time: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works Se Decant Fo IN Detected in frame_300.jpg: Markov Process or Markov Chain Memoryless random process. Markov Process or Markov Chain Memoryless random process. Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) If finite number (N) of states, can express P as a matrix P (151) P ( $ 251) P ($152) P(5252) P = P (S15N ) P ( 52 5N ) Sc TIT P(SNS)\\ P(SN $2) P('SN SN) NAJN Markov Process or Markov Chain 9 www is a memoryless random process with Markov property. P is a dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5) If finite number (N) of states, can express P as a matrix P ( $ 1 $ 1 ) P ($251) P(SNS) P (51 $2) P('SN $2') P ( SN SN) Screening Schot From NINN Detected in frame_302.jpg: Markov Process. Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. Markov Process is a dynamics/transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix P ($151) P($152) P(52 $2) P ($251) P (SNS) P = P ( S1 SN ) P ( 52 SN ), P ( SN SN ) S F NINN Markov Process or Markov Chain - NANO N Memoryless random process Sequence of random states with Markov property. Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) P is dynamics/transition model that specifices p ( 5 + 1 = 5 | 5 = 5) Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. Markov Process ⚫S is a (finite) set of states (s = 5) P is dynamics/transition model that specifices p ( 5 + 1 = 5 | 5 = 5 ) If finite number ( N ) of states, can express P as a matrix P ( $ 1 $ 1) T Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5 ) If finite number (N) of states, can express P as a matrix P ( $ 1 $ 1 ) P ( ($ 251) P ( SNS ) \\ P ( 51 $ 2 ) P(5252) P(SN|$2) P = P (S1 SN) P.(52 SN)P(SN SN) S From NINN Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. Definition of Markov Process ⚫S is a (finite) set of states (s = 5) P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note: no rewards, no actions. If finite number (N) of states, can express P as a matrix P ( 5151) P ( $ 251 ) P ( SNS ) \\ P ( 51 $ 2 ) P( 5252) P(SN|$2) P = P Markov Process or Markov Chain 2-1 - Memoryless random process. Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = 5 ' st = 5 ) Note: no rewards, no actions. Markov Process or Markov Chain is a memoryless random process with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = 5 | 5 = 5 ) Note: no rewards, no actions. If finite number (N) of states, can express P as a matrix P (515) P (5251) P(SNS), P(5152) P ($2) P = P (SSN), P (52 SN), P ($SN SN), Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. Markov Process is a (finite) set of states (s € S) P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix. Markov Process or Markov Chain Memoryless random process. Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note: no rewards, no actions. Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. Markov Process ⚫S is a (finite) set of states (s = S) P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5) If finite number (N) of states, can express P as a matrix P ( 151) P ( 52 $ 1) P(SNS) P('SN') P('SSN') P (S2SN) P ('SN SN') P = P(SSN) P'SN' P T Markov Process or Markov Chain Memoryless random process ⚫ Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix. Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. Markov Process: S is a (finite) set of states (s = S) P is dynamics/transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix. Markov Process or Markov Chain Memoryless random process. Sequence of random states with Markov property. P is dynamics / transition model that specifices p ( 5.15 | 5 = 5 ) Note no rewards, no actions. If finite number (N) of states, can express P as a matrix P(55) P(525) P (SNS), P(5152), P (5252 ) P (SN $2) P= P(SSN) P('S2SN') P('SN SN') Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. Markov Process ⚫S is a (finite) set of states (s = S) P is dynamics/transition model that specifices p ( St + 1 = 5 ' 5 = 5 ) Note : no rewards , no actions If finite number (N) of states, can express P as a matrix P (ss) P (525) P(SNS)\\ P(5152) P('SN $2) P= P ( SSN ) P ( S2SN ) P( SN This Time: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works. T Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = 5 | 5 = 5 ) Note : no rewards , no actions If finite number (N) of states, can express P as a matrix P (S151) P ( $ 251 ) P ( 51 $ 2 ) P(5252) P(SN 51) P = P(S1 SN) P($251) P('51 $2') P('5252' P('SN|52') P( SN SN) T Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. If finite number (N) of states, can express P as a matrix P (S|$1) P($251) P(SN Si)\\ P(5152) P (5252) P('SN $2) P = P(515N), P(52 SN), P (SN SN) Book S Fo RNS Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note: no rewards, no actions. If finite number (N) of states, can express P as a matrix P(S151) P($251) P(51 $2) P ($2 $2), P ( SN | 52) P = P ( 515N) P ( 52 SN ) P ( ( SN SN ) S Foc RNS Detected in frame_324.jpg: 10 Р San M ' IN NANO Detected in frame_325.jpg: Markov Process or Markov Chain. Memoryless random process with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = 5 ' 5 = 5) Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. Markov Process ⚫S is a (finite) set of states (s = S) P is dynamics/transition model that specifices p(5+1 = s' st = 5) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix P($1 $1) P($251) P(SNS)\\ P(51 $2) P (5252) P('SN $2') P = P (S1 5N Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. If finite number (N) of states, can express P as a matrix P (515) P (525) P(SNISL) P('5152' P(5252) P(\"SN $2) P= P(SSN) P ('S2SN' P('S1 5N') P('SN SN)' P(' SN $2') P = P ( SSN) Son Mwing D 10 Р NNN Detected in frame_327.jpg. Markov Process or Markov Chain is a memoryless random process with Markov property. Markov Process is a (finite) set of states (s = 5) P is a dynamics/transition model that specifices p ( 5+ ) = s's , = 5 ) If finite number (N) of states, can express P as a matrix P ( ss ) P ( 5251) P ( SNS ) P( 5152) P(5252), P ( SN $2) P = P ( SSN) P  P ( 52 SN ) P  NIN N 3 Detected in Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note: no rewards, no actions. If finite number (N) of states, can express P as a matrix P ($1 $1) P($251) P(SN 51), P(5152), P (5252) P (SN $2), P = P (S1 SN) P ($52 SN), P ($2 SN) Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. Markov Process is a (finite) set of states (s = S) P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note: no rewards, no actions. Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. Markov Process is a (finite) set of states (s = S) P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix P(S|5i) P(5252) Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. Markov Process is a (finite) set of states (s = 5) ⚫P is dynamics/transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note: no rewards, no actions •If finite number (N) of states, can express P as a matrix (P(S|$1) P(5251) P (SNS), P (5152) P.(5252) P('SN|52) P= P(SSN Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix P($1 $1) P($251) P(SN 51)\\ P(5152) P ($2 $2) P ( SN SN ) ) NNNN 3 Detected in frame_334.jpg. Markov Process or Markov Chain Memoryless random process ⚫Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note: no rewards, no actions. If finite number (N) of states, can express P as a matrix P ( $1 $1) P (52 51) P(SN $1)\\ P(51 $2) P($2 $2), P ( SN SN) NNNN Markov Chain Memoryless random process ⚫Sequence of random states with Markov property. Markov Process is a dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note: no rewards, no actions. If finite number (N) of states, can express P as a matrix P($1 $1) P($251) P(SN $1)\\ P($152) P (5252) P('SN $2) P = P(SSN), P(52SN), P (SN SN) NNNN Markov Process or Markov Chain x Memoryless random process Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note: no rewards, no actions = If finite number (N) of states, can express P as a matrix. Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. Markov Process ⚫S is a (finite) set of states (s = S) P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix P (S | S ) P ( 52 | 51) P ( SNS ) \\ P ( $ 1 | $ 2 ) P(5252) P(SN SN) N This Time: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works. Today's Plan 3: T Markov Process. Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note: no rewards, no actions. If finite number (N) of states, can express P as a matrix. Markov Process or Markov Chain Memoryless random process. Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5.55 = 5) Note: no rewards, no actions. Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix P(551) P(5251) P (SNS), P (5152) P ($2) P= P(SSN), P(S2 SN), P (\"SN SN\") Sc ' Detected in frame_344. Markov Process or Markov Chain A A Memoryless random process with Markov property. P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) If finite number ( N ) of states, can express P as a matrix P ( 551) P (5251) P  P ( SNS ) P (SNS) P(5152), P ( SN $ 2 ) P = P ( SSN ) P(S2SN) P (\"SN SN\") F Detected in frame_345.jpg: Markov Process  MarkovChain A A memoryless Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property. P is dynamics/transition model that specifices p ( 5 + 1 = s ' st = 5) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix P ( $ 151 ) P ( ($ 251) P ( SN $ 1 ) \\ P ( 51 $ 2 ) P(52 $ 2) P(SN $2) P = P (S1 SN) P('S2SN') P ('SNS') P (SN SN) SeeMix Markov Process or Markov Chain 2-2 Memoryless random process. Sequence of random states with Markov property. P is dynamics/transition model that specifices p(5+1 = 5' st = 5) If finite number (N) of states, can express P as a matrix P (S | 51 ) P ( 52 | 51) P ( SNS ) P ($1|$2) P(5252), P (SN $2), P= P(SSN), P(SN SN) S NNN Markov Process or Markov Chain A AM Memoryless random process with Markov property. P is dynamics/transition model that specifices p(5+1 =s' st = 5) Note: no rewards, no actions If finite number (N) of states, can express P as a matrix P($1 $1) P($251) P(SN 51)\\ P($12) P ($2 $2) P (SN 52) P= P(515N) P('52 SN) P(\"SN SN) S Schot From NNNN N This Time: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works. Today's Plan x lecture2-2 NNNN. Markov Process or Markov Chain - Memoryless random process. Sequence of random states with Markov property. P is dynamics/transition model that specifices p(5+1 = s' st = 5) If finite number (N) of states, can express P as a matrix P (S | 51 ) P ( 52 | 51) P ( SNS ) \\ P ( $ 152 ) P(5252) P(SN $2) P= P(S|SN) P('S2SN') P('SN SN') S NANO ' IN Markov Process or Markov Chain is a memoryless random process with Markov property. P is a dynamics / transition model that specifices p ( 5+ ) = s's , = 5 ) If finite number (N) of states, can express P as a matrix P(ss) P(512) P($2 $2) P ($251) P (SNS) S IS NNNN If finite number ( N ) of states , can express P as a matrix P ( ss) P ( 512 ) P ( $ 2 $ 2 ) P ($ 251) P P ( 515N) P( 525N) S IS NNNN Detected in frame_353.jpg: Example. Mars Rover Markov Chain Transition Matrix. P 51 53 :54 56 $7 LL 102 92 02 BZ 02 16 10.6.  frame_355.jpg: Example: Mars Rover Markov Chain Transition Matrix, P Si. P Si 0.4 $2 04 53 0. 4 04 SA 0.3 0.2 0.6 70.6 0.5 0.1 0.0 0 0 0. 2 0.7 0.8 0.9 0. Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 04 $3 0.2 0.6 70.6 0. 4 0 0 0. 2 0.5 0.8 0.9 0.7 0.10 0.3 0 0 1 0.0 0.1 1.0 1.3 1.4 1.2 1.5 1.6 1.1 2.2 2.0 2.3 2.5 2.4 2.1 3.2 3.0 3.3 3.5 3.4 3.1 4.2 4 Sereng Ор Screenshot Fo ' NON INN Detected in frame_358.jpg: Example: Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 $3 $3 0.2 0.6 70.6 0. 4 0 0 0.0 0. 2 0.1 0.5 0.7 0.8 0.9 0.10 0.3 1.0 1.2 1.1 1.4 1.6 1.3 2.2 2.0 2.1 2.3 3.2 3.0 3.3 4.2 Today's Plan 2-1 x NNNN: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works. T Example: Mars Rover Markov Chain Transition Matrix.  frame_361.jpg: Mars Rover Markov Chain Transition Matrix, P S1 0.4 $2 04 $3 04 S4 56 57. 0.2 0.3 0.5 0.6 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 0.35 0.36 0. Mars Rover Markov Chain Transition Matrix, P $1 53 54 56 ST 0.2 0.4 0.6 70.6 /0.4 04 04 0.3 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0.39 0.40 0.41 0. Screening    Роси Screenshot Focus NOIN IN N Detected in frame_364.jpg: Mars Rover Markov Chain Transition Matrix, P $1 0,4 $2 0.4 $3 $3 0.2 0 0 0.6 70.6 0. 4 0.0 0. 2 0.1 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.3 0 Mars Rover Markov Chain Transition Matrix, P $1 0.4 Sz 04 $3 S4 $5 56 57 0. 4 0.2 0.6 70.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0.39 0.40 0 Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 04 $3 S4 0.2 0.3 0.5 0.6 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0.39 0.40 0 Mars Rover Markov Chain Transition Matrix, P $1.4 $2 0.4 53 & SA $6 57 0. 4 55 56 0.3 0.6 70.6 0.2 1.4 0 0 0 1.2 00 0. 2 0.1 1.3 1.0 1.6 1.5 1.1 2.0 2.3 2.2 2.4 2.1 3.0 3.2 3.1 4.0 4.2 5.0 5.2 6.2 7.2 8.2 9.2 10.2 11 Today's Plan: Making good decisions given a Markov decision process. Next Time: Policy evaluation when don't have a model of how the world works. Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 04 $3 & 55 S4 56.0.4 0.2 0.6 70.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0.39 Screening D Sir F NANN ' Detected in frame_372.jpg: Example: Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 $3 $4 S4 0.3 0.2 0.6 70.6 0. 4 0 0 0.0 0 0 1.4 1.2 1.3 1.0 1.1 1.5 1.6 1.7 1.8 1.9 1. Screen Mirroring Document Screenshot A Focus N N N. N Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.3 $ 3 54 $ 5 $ 6 57 0.2 0.6 (0.4 0 0 0. 4 0 0 1.4 1.2) 0.5 56 0.1 0.0 0. 2 0 0 2 0 2 1.0 1.1 1.3 1.5 1.6 1.7 1.8 1.9 1.10 1. Screen Mirroring Document Screenshot A Focus N N N. N Detected in frame_375.jpg: Mars Rover Markov Chain Transition Matrix, P S1 0.4 Sz & 9, az $3 S4 55 56.  frame_377.jpg: Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 $3 56 9, az S4 55 57 0.2 0.6 70.4. 0 0 0. 4 0.3 0.5 0.8 0.9 0.10 0.0 0. 2 0.1 1.0 1.3 1.2 1.4 1.5 1.6 1.7 1.8 1.1 2.2 2.0 2.3 2.5 2.4 2.1 3.2 3.3 3.4 NNNNN Detected in frame_378.jpg: Example: Mars Rover Markov Chain Transition Matrix, P $1 0.4 S₂ 04    ‘’ ‘”’  “’”  ’’ 0.6 70.6 0.2 0.3 0.5 0.8 0.7 0.9 0.0 0.1 1.0 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 1.1 2.0 2.2 NNNN Example : Mars Rover Markov Chain Transition Matrix. $2 0.4 $3 54 $5 $4 0. 4 $6 $4 a, az 57 0.6 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 0.36 0. Screen Mirroring Document Screenshot Focus Detected in frame_380.jpg: Example: Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 04 53 & S4 0.3 0.2 0.6 70.6 0 0 0.0 0 0 1 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.20 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 Mars Rover Markov Chain Transition Matrix, P Si 0.4 Sz 0. 4 $3 $3, S4 $5 0.3 $5, 9, az 57 0.6 $6 0.2 $6, ST 0.1 $1 $2 $2, S3 $4 0.5 $4, 90 az ST $2 0.7 $3 0.8 $4.4 0 0 0.0 0 0, 0 0 1 0, 1 0 0 2, 1 1, 1 2, 2 3, 2 2 2 1 2 1, 2 1 1 2 2, Focus NININ + Detected in frame_383.jpg: Mars Rover Markov Chain Transition Matrix, P Si. = 0 0 0.4 0.2 0.6 70.6 0. 4 0.3 0.8 0.5 0.7 0.9 0.0 0. 2 0.1 0.10 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.12 0.14 0.15 0.16 0.17 0.18 0.20 0.21 0.24 0. Screen Mirroring Document Screenshot Focus IN INN Detected in frame_385.jpg: NNNN Example: Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 $3 $3 S4 $5 0. 4 $6 0.2 0.3 a, az 57 0.6 70.6 0 0 0.0 0 0 1 0. 2 0.5 0 00.4 0.8 0.9 0.7 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0. Screen Mirroring Screenshot Focus Detected in frame_386.jpg: Example: Mars Rover Markov Chain Transition Matrix, P Si 0.4 Sz 53 S4 04 0. 4 ai az $5 $4 56 $7 0.6 0:2 0.2 02 0.3 06 70.6 70.4 0 0. 2 0.5 0.1 0.0 0.8 0.9 0.10 0.12 0.11 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.24 0. Screen Moving Document Screenshot Focus \" IN IN INN Detected in frame_388.jpg: Example: Mars Rover Markov Chain Transition Matrix, P ai az Si 0.4 $2 $2.4 53 04 54 0. 4 55 56 57 0.2 0.6 70.6 0. 2 0.3 0.5 0.1 0.0 0. S Socia Detected in frame_389.jpg: NNN N Example: Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 $3 0.2 0.6 70.6 0. 4 . 0 0 0.0 0 0 1 0.1 0.5 0.8 0.7 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0. Make good decisions given a Markov decision process. Policy evaluation when don't have a model of how the world works. Today's Plan c2-2 K NNNN \"  detected in frame_391.jpg: Mars Rover Markov Chain Transition Matrix, P Si D.4 $2 04    الله 0.4 04 54 04 $5 04 04 56 0. 4 di az 0.6 0.2 02 0,6 10.4 0 0 0.0 0 0 .0 0.1 0.3 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.24 Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 $3 S4 $5 0.2 0.6 70.6 0. 4 0 0 0.0 0 00.4 0. 2 0.3 0.8 0.9 0.10 0.1 0 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 1.10 1. Mars Rover Markov Chain Transition Matrix, P Si 0.4 Sz 04 $3 & S4 $5 0. 4 $60.4 9, az 57 0.2 0.6 70.6 0.3 0.8 0.9 0.7 0.1 0.0 0. 2 0. 1 0. 0. 3 0. 6 0. 5 0. 7 0. 8 0. 9 0. 10 0. 11 0. 12 0. 13 0. 14 0. 15 0. 16 0. 17 0. 18 0. 19 0. 20 0. 21 0 Stop Screen Mirroring Document Screenshot Focus ' NININ Detected in frame_396.jpg: Mars Rover Markov Chain Transition Matrix, P Si $3 S4 $5 56 57 0.4 0,4 0.2 0.6 70.6 0. 4 0 0 0.0 0. 2 0.1 0.3 0.5 0.8 0.9 0.7 0.10 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0 Foczis NININN + Detected in frame_398.jpg: T Example: Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 $3 $3 0.2 0.6 70.6 0. 4 0 0 0.0 0 00.4 0. 2 0.1 0.5 0.7 0.8 0.9 0.3 1.0 1.1 1.2 1.4 1.5 1.6 1.3 2.2 2.0 2.1 2.3 3.2 3.0 3.1 3 Mars Rover Markov Chain Transition Matrix, P Si $2 53 & 9, az 57.4 0.4 54 0. 4 04 55 0A 04 56 0.2 0.6 70.6 0.3 0.5 0.1 0.0 0. 0 P = 0 0 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 Stanford ENGINEERING detected in frame_400.jpg: Stanford ENGINEerING Stanford ENGineERING Detected in frame.400: Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 $3 S4 $6 a, az $5 57. Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 $3 & 56 9, az 54 55 57 0. 4 0.2 0.6 70.4 10.4 04 0.5 0.3 0.8 0.1 0.0 0. 2 0. 1 0. 3 0. 0. 6 0.7 0. 8 0. 9 0. 10 0. 11 0. 12 0. 13 0. 14 0. 15 0. 16 0. 17 0. 18 0. 19 0. 20 0. 22 0. 23 0. 24 0. 25 0 Screening D Serhat Fon NININN Detected in frame_403.jpg: Example: Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 $3 S4 0.2 0.6 70.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 Screen Mon Document Screenshot Focus NNNN Example: Mars Rover Markov Chain Transition Matrix, P S₁ 0.4 $2 $3 $3. $3 ✓ S4 $5 0.3 $4 $6 $6.4 04 $6 0.6. 0.2 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 Mars Rover Markov Chain Transition Matrix, P S₁ 0.4 $ 2 0. 4 $ 3 0.3 0.2 0.6 70.6 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0.39 0.40 The Mars Rover Markov Chain Transition Matrix is an example of a Mars Rover Matrix. It is a three-dimensional version of the Markov chain-transition Matrix. The Mars Rover is a form of the Mars Rover, which is a type of Matrix. ' INNINN Detected in frame_409.jpg: Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 $3 α, az S4 $5 56 57 0.2 0.6 70.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.34 0 Screen Mirroring Document Screenshot focus ' NNNNN Detected in frame_410.jpg: Mars Rover Markov Chain Transition Matrix, P $1 $2 $3 $3 24 S4 0.4 11.4 $5 56 DA 0.6 02 0.2 0.3 0.5 70.6 0. 4 0 0 0.0 0 0 1 0.1 0.7 0.8 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.24 0 N N N N Example : Mars Rover Markov Chain Transition Matrix, P $ 1 0.4 $ 2 0.3 $ 3 24 S4 0.2 0.6 70.6 0. 4 0 0 0.0 0 0 1 0 0 3 0.1 0.5 0.8 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.30 0.31 0.32 0.34 Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 $3 & 9, az S4 55 56 ST. 0.2 0.6 70.6 0. 4 0 0 0.0 00 0 00.4 0.3 0.5 0.8 0.7 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0 Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 0. 4 S3 & S4 0.3 0.5 0.6 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0.39 0.40 0.41 0. Screen Mirroring D Screenshot ' IN IN INN Detected in frame_416.jpg: Example: Mars Rover Markov Chain Transition Matrix, P ગ 04 $2 $3 S4 55 0.4 55.4 56 0. 4 ai az 06 0. 2 0.2 0.6 70.6 0.3 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 Mars Rover Markov Chain Transition Matrix, P Si 0.4 Sz ✓ 55 9, az 53 54 56 ST. = 0 0 0.2 0.6 70.6 0. 4 0 00.4 0.0 0.3 0.1 0.8 0.9 0.5 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.34 Mars Rover Markov Chain Transition Matrix, P S1 $2 34 S4 $5 56 9, az 0.4 $3 ST. 0 0 0. 4 0.2 0.6 70.6 0.3 0.5 0.8 0.9 0.7 0.1 0.0 0. 2 0. 1 0. 3 0. 0. Screening Docu So F ' IN NINN Detected in frame_421.jpg: Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 $4 53 34 S4 $ 6 9, az ST 0.2 0.6 70.6 0. 4 0 0 0.0 0. 2 0.3 0.5 0.8 0.9 0.7 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.24 0.25 0.26 0.28 0 NNNN Detected in frame_422.jpg: Example: Mars Rover Markov Chain Transition Matrix, P 51 0.4 $2 0:4. NNNN also detected in frame-423.jpg. Screen Mirroring Document Screenshot Focus INNINN \" Detected in frame_424.jpg: Example: Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 $3 0.3 ai az 55 56 57 0:4 0.2 0.6 70.4 0 0 0. 4 0. 2 0.5 70.6 /0.6 0.0 0.1 70.2 /0:0.2 70.0 /0 :0.4 70.1 /0.:0.3 70.3 /0;. 0 0 1.4 1 NNNN Detected in frame_425.jpg: NNNN Example: Mars Rover Markov Chain Transition Matrix, P $1 $2 $3 34 S4 $6 a, az 0.4 55 57 0.2 0.6 70.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 Screening D Shot Fo NININN Detected in frame_427.jpg: NNNN Example: Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 $3 $3.4 24 54 55 0. 4 04 0.3 0.2 0.6 70.6 0. 2 0.5 0.1 0 1 0.0 0 0 0 1.0 1.5 1.3 1.4 1.2 1.1 1.7 1.6 1.8 1.9 1.10 1.12 1.14 1.15 1.16 Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 04 $3 S4 0. 4 $5 0.3 di az $7 06 0.2 0.6 70.6 0 0 0.0 0 0 1 0 0 3 0.5 0 0 2 0.1 0.7 0.8 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.20 0.21 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34  frame_430.jpg: T Example: Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2.4 53 53 54 54 54 55 04 04 56 56 04 ai az 0.6 0.2 0.5 0.3 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 Sering Sco IN NINA Detected in frame_431.jpg: Example: Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 $3 34 9, az S4 55 56 57 0. 4 55 04 04 56 0.2 0.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0 Mars Rover Markov Chain Transition Matrix, P S₁ 0.4 $2 $3 $3 E 56 9, az S4 $5 57 0.2 0.6 70.6 0.3 0.5 0.8 0.9 0.7 0.1 0.0 0. 1 0. 0. 2 0. 3 0. 4 0. 5 0. 6 0. 7 0. 8 0. 9 0. 10 0. 11 0. 12 0. 13 0. 14 0.15 0.16 0.17 0.18 0.19 0.20 0 Mars Rover Markov Chain Transition Matrix, P Si 04 $2 0.4 $3 S4 0. 4 04 14 $5 0.04 04 di az 56 $7 0.6 70.6 0.3 0.2 1.4 1.0 1.1 1.5 1.3 1.8 1.6 1.2 2.0 2.5 2.4 2.2 3.0 3.2 4.2 5.2 6.2 7.2 8.2 9.2 10.2 11.2 12.2 13.2 14.2 15. Mars Rover Markov Chain Transition Matrix, P $1 $2 $3 E $6 a, az 04 S4 $5 57 0.4 0.2 0.6 70.6 0.3 0.5 0.8 0.7 0.9 0.1 0.0 0. 1 0. 2 0. 0. 3 0. 4 0.10 0.01 0.02 0.04 0.05 0.06 0.07 0.08 0.09 0.12 0.14 0.15 0.16 0.17 0.18 0.20 0. Mirroring Document Screenshot Focus NNN Detected in frame_437.jpg: Mars Rover Markov Chain Transition Matrix, P Si 04 $2 di az $3 S4 $5 56 $7 04 0.4 ሲታ 0.2 0.6 70.6 0. 4 0 0 0.0 0 0 1 0.3 0.1 0.5 0.8 0.7 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.20 0.21 0.24 0.25 0.26 Focus ' NNNNNN Detected in frame_439.jpg: Example: Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 04 53 9, az S4 55 56 57 0.2 0.6 70.6 0 0 0. 4 0. 2 0.3 0.5 0.1 0.8 0.9 0.0 0.10 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.12 0.14 0.15 0.16 0.17 0.18 0 Screen Miring D Sish Focus ' NNNNN Detected in frame_440.jpg: Mars Rover Markov Chain Transition Matrix, P S₁ 0.4 $2 $3.4 04 34 54 0. 4 55 04 04 56 04 9, az 0.2 0.6 /0.4. 0 0 0 P = 0 0..4 Sc IN NINN Example : Mars Rover. Markov. Chain. Transition Matrix , P Si 0.3 $ 2 04 53 9 , az S4 55 56 57 0.5 0.7 0.8 0.9 Screen Miring D Sish Focus ' NNNNN Detected in frame_441.jpg: T Example: Mars Rover Markov Chain Transition Matrix, P Si 04 $2 0.4 53 & 54 9, az 56 57. Screen Ming Doct Schet fo NININN Detected in frame_443.jpg: Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 $3 54 $5 56 57 04 0.2 0.6 0.3 0.5 0.8 0.7 0.9 0.1 0.0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0. Mars Rover Markov Chain Transition Matrix, P $1.4 $2 0.4 53 56 a, az 54 S5 57 0.6 70.6 0. 4 0 0 0.0 0. 2 0.3 0.1 0.5 0.8 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0.39 Stop Screen Mirroring Document Screenshot ' IN NINN Detected in frame_446.jpg: Example: Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 04    04 21.4. $5 04 04 56 0. 4 ai az 57 0.6 0.2 0.3 0.5 0.1 0.8 0.0 0. 2 0. 1 0. 3 0. 0 0 0. The Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 $3 S4 55.4 9, az 57 0.2 0.6 70.6 0. 4 0 0 0.0 0 00.4 0. 2 0.5 0.3 0.8 0.9 0.1 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0.36 0  frame_449.jpg: Mars Rover Markov Chain Transition Matrix, P Si 04 $2 0.4 53 & 56 9, az 54 57. P = 0 0 0. 4 0.6 70.6 0.0 0. 2 0.3 0.1 0.5 0.8 0.9 0.10 0.7 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.24 0.26 0.28 0.30 0.31 0.32 0.34 0.36 0. P = 0 0 0.4 0.2 0.6 70.6 0.3 0.5 0.8 0.9 0.7 0.1 0.0 0.02 0.01 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 0.36 0 Mars Rover Markov Chain Transition Matrix, P $1 $2 $3 S4 $5 0.4 $6 $6 9, az 57 0.6 70.6 0.2 0. 2 0. 4 0 0 0.0 0. 0.3 0.8 0.1 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 0 The Mars Rover Markov Chain Transition Matrix, P S₁ 0.4 $2 $3 54 $5 0.2 $6 $6 0.3 $6.4 0.6 70.6 0 0 00 0 0. 4 0.0 0. 2 0.1 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.24 Screening D Screenshot Focus \" IN IN IN\" Detected in frame_455.jpg: T Example: Mars Rover Markov Chain Transition Matrix, P S1 0.4 $2 $3 0.3 $4 $5 $6 $7 $8 $9 $10 $11 $12 $13 $14 $15 $16 $17 $18 $19 $20 $21 $22 $23 $24 $25 $26 $28 $29 $30 $32 $33 $34 $35 $36 $37 $38 $39 $40 $41 $42 $43 $44 $45 $46 Son M D Socia NNN Detected in frame_456.jpg: Example: Mars Rover Markov Chain Transition Matrix, P $1 $2 $3 $4 $5 $6 0.4 a, az 57 0.6 /0.4 9 , az 57. 0.2 0.5 0.3 0.8 0.9 0.10 0.12 0.1 0.0 0. 1 0. 2 0. 3 0. 4 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1. Mars Rover Markov Chain Transition Matrix, P S₁ 0.4 Sz $3 S4 04 0. 4 56 04 ai az 0.2 0.6 S 70.6 0.3 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.34 Foctia NININ + Detected in frame_459.jpg: Example: Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 04 $3 S4 55.4 0.2 0.6 S 70.6 0.3 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29 Screen Mining De Screenshot Focus NANN \" Detected in frame_461.jpg: Example: Mars Rover Markov Chain Transition Matrix, P Si 53 S4 $5 0.4 0. 4 di az 57 0.6 0.2 02 0,6 0,3 0.3 0,4 0,5 0,2 0,1 0,0 0.1 0.0 0 0 0. 2 0. 1 0. 3 0. 0. 6 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0. Screen Mining Do Screenshot Focus NANN \" Detected in frame_463.jpg: Example: Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 $3 S4 55. az 57 0. 4 04 0.2 02 0,6 0.6 70.6 0 S₁ 0 0.0 0.1 0,2 0,3 0,4 0,5 0,7 0,8 0,9 0,10 0,11 0,12 0,13 0,14 0,15 0,16 0,17 0,18 0,19 0, NANN Example: Mars Rover Markov Chain Transition Matrix. P di az Si 53 S4 $ 5 56 57 0.4 0.6 70.6 0.2 0.5 0.3 0.8 0.1 0.0 0. 2 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 Screen Mirroring Document ScroFocus Detected in frame_465.jpg: Example: Mars Rover Markov Chain Transition Matrix, P Si 04 $2 $3 S4 04 0.4 0.2 0.6 70.6 0 S₁ 0 0 0. 4 0.1 0.3 0. 2 0. 3 0.8 0.9 0.10 0.0 0. 1 0. Mars Rover Markov Chain Transition Matrix, P Si 04 Sz $3 S4 04 0.4 14 $5 04 04 56 04 di az $7 0.2 0.6 S₁ 70.6 0. 4 0 0 0.0 0 0 1 0.1 0.3 0.5 0.8 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.29 0.30 0. The Mars Rover Markov Chain Transition Matrix, P S₁ 0.4 $2 $2 0.3 $3 $4 $5 $5 04 56 0. 4 di az $ 7 $ 7 0.6 0.2 $7 0.5 $7. $ 3 S4 04.4 14 $ 5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0. The Mars Rover Markov Chain Transition Matrix is an example of a screen mirroring image. It shows the screen being Mirrored by a Mars Rover. NINN . Detected in frame_469.jpg. The Mars Rover Markov Chain Transition Matrix, P S₁, is an example of a Matrix. Matrix , P S ⁁ 0.4 $ 2 0. 4 $ 3 S4 0.2 0.6 70.4 0 S≁ 0 0 0.0 0.1 0.3 0.5 0.8 0.9 0.10 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.24 0.25 0.26 0.28 0.29 0.30 0. Screen Mirroring Document Screenshot Focus IN NINN Detected in frame_473.jpg: Example: Mars Rover Markov Chain Transition Matrix, P SI 0.4 $2 04 الله 04 S4 04 3.4 56 04 di az $7 06 0.2 02 0.6 (0.4 0.5 0.3 0.1 0.0.2 0.8 0.7 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.24 0 Mars Rover Markov Chain Transition Matrix, P S₁ 0.4 Sz 04 $3 S4 0. 4 $6 9, az 57 0.2 0.6 70.6 0.3 0.8 0.9 0.5 0.7 0.1 0.0 0. 2 0. 1 0. 0. 3 INN Detected in frame_474. Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 04 $3 04 04 54 0. 4 14 $5 LA $6 ai az ST0.4 04 0.6 0.2 0.3 0.5 0.8 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0.39 0. Mars Rover Markov Chain Transition Matrix, P a, az Si 0.4 $2 04 $3 0.3 04 54 0. 4 $5 56 ST 0.2 14 04 0.6 70.6 0.5 70.4 0 0 0.0 0 0 1 0.1 0.7 0.8 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0 The Mars Rover Markov Chain Transition Matrix, P E S1 0.4 Sz 04 53 & 54 55 0:4 04 04 56 0. 4 9, az 57 0.2 0.6 70.6 0 S₁ 0 0 0.0 0.3 0. 2 0.5 0.8 0.9 0.7 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0 Screen M Dic Screens IN NNN Detected in frame_480.jpg: Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 $3 & 56 9, az 54 55 57 0. 4 0.2 0.6 70.4 0 S₁ 70.6 0 0 0.0 0 0, 0.1 0.5 0.3 0.8 0.7 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0 The Mars Rover Markov Chain Transition Matrix, P ST Sz $3 SA 55 00 L 56 9, az S Compos 1.2 0.2 11.2 02 06 10.6 0.4 0 0 0.6 70.4 b S NININ Detected in frame_481.jpg: Example: Mars Rover. Markov. Chain. Transition Matrix , P ST. Sz $ 3 SA 55. 00 L. 56 9 , az S compos 1. 2 0. 2 11.6 10.2. Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2 0. 4 53 Ex a, az S4 55 56 57 0.3 0.6 70.6 0 S₁ 0 0 0.0 0. 2 0.2 0 0 1 0.8 0.5 0.7 0.9 0.1 1.0 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 1.1 2.2 2.0 2.1 3.0 3.2 3.3 3.4 3. Screening D Shot Focus ' IN IN INN . Detected in frame_485.jpg: T Example: Mars Rover Markov Chain Transition Matrix, P S1 0.4 $2 P 0.6 - Cooopool 04 $3 & 9, az S4 $5 56 57 0.3 b Screen M D Street Fo NININN '  frame_486.jpg: NNIN N Example: Mars Rover Markov Chain Transition Matrix, P S₁ 0.4 $2 $3 34 S4 9, az 55 56 57 0. 4 $ 5 56 57. S ' 70.6 0 S ⁁ 0 0 0 1.4 0.2 0.1 1.2 1.0 1.3 1.5 1.6 1.7 1.8 1.9 2.4 2.3 2.2 2.1 2.0 2.5 2.6 2.7 2.8 3.2  frame_487.jpg: Mars Rover Markov Chain Transition Matrix, P Coopool a Si 0.4 $2 ai az 53 S4 $5 Si $7.4 04 14 04 0.6 0.2 02 0,6 S' 10.60.4 0.3 0.5 0.1 0,3 0,2 0,1 0.0 0,0 0 0 0 1 0 0.000 0.00 0.0000 0.01 0.001 0.008 0.003 0.006 0.007 0.018 0.002 0.009 0 The Mars Rover Markov Chain Transition Matrix is a type of Matrix. It is a series of boxes that are connected by a single line of code. In frame_488.jpg: T Example: Mars Rover. MarkovChain Transition Matrix, P S₁ 0.4 52 9, az $3 S4 $5 56 57 0.2 0.6 P S ' 70.6 0. 4 0 0 00 S ⁁ 00.4 0.3 0.1 0.5 0.8 0.7 0.9 0.10 0.12 0.14 0. P - Cooopool NNN NI 0.2 0.4 0.6 S ' 0.5 па 1.2 112 02 3' 10.4 a Compost 9 , az Example : Mars Rover Markov Chain Transition Matrix. P Sz    $ 5 4 S 0.3 0.8 0.7 0.9 0.1 0.0 0 0 0. 2 0. 1 0. 3 0. 4 0. 6 0.10 0.11 0.12 0.13 0.14 0.15 0. 14 0.16 0.17 0. Compost Detected in frame_491.jpg: NNN Example: Mars Rover Markov Chain Transition Matrix, P 51 0.4 $2 0. 4 53 0.3 04 S4 04 55 04 04 $6 0.6 a - Cooopool 0. 2 0.20.4 0 0 00 0 0.0 0.1 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0. Screening Doct Screenshot Fo NININ Detected in frame_493.jpg: Mars Rover Markov Chain Transition Matrix, P 51 0.4 $2 0.6 P - Cooopool 0.3 $3 ai az S4 55 56 ST 0. 4 0:4 0. 2 0.2 0 0 0. The Mars Rover Markov Chain Transition Matrix, P 9, 92 ST Sz a Coorpool LA 53, is an example of a Mars Rover system. P $ 1 0.4 $ 2 0. 4 $ 3 24 S4 $ 5 56 9 , az 57 0.3 0.2 0.6 - Cooopool 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0 The Mars Rover Markov Chain Transition Matrix is shown in frame_496.jpg. It is a three-dimensional version of the Mars Rover's Markov Markov Transition Matrix. The matrix is made up of a Coorpool LA, a Cooopon - NNN and a Coopon. NNA Example: Mars Rover Markov Chain Transition Matrix, P Si 04 $2 a Coopool 04 الله di di. 0,6 S ' 70.6 0.4 0 S₁ 0 0 0. 4 0.2 0. 2 0.6 S' 70. 6 0.3 0.1 0.5 0.8 0.7 0.9 0.10 0.0 0.  frame_499.jpg: Mars Rover Markov Chain Transition Matrix, P $1 0.4 $2, P Si 04 $ 2 a Coopool. az 0. 4 54 $5 56, P - 0.2 0.6 S' S₁ /0.4 0 0 0.0 0 0, Sc NNNN 0:4 b NINA 3  frame_500.jpg: Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 $3 $3. NINN Detected in frame_501.jpg. Mars Rover Markov Chain Transition Matrix, P a, az Sz    0 $4 $5 ST 06 1.2 9:2 0.2 112 0:2 a P. '3' 10.6 0.4 0 0 0.0 0 00 0. 4 0.6 S' 0.3 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.20 0.21 0.22 0.24 0.25 0.26 0 The Mars Rover Markov Chain Transition Matrix, P 51 0.4 $2 $2. $ 3 54 0. 4 04 04 56 0 : 4 a az 57 0.6 a - Coorpool 15 0.2 0.3 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.30 In frame_504.jpg: Example: Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 $2 0. 4 $3 $3 0.3 $4 0.5 $4 $5 $6 $7 $8 $9 $10 $11 $12 $13 $14 $15 $16 $17 $18 $19 $20 $21 $22 $23 $24 $25 $26 $28 $29 $30 $32 $33 $34 $36 $37 $38 $39 $40 $41 $42 $43 $44 $45 $46 $47 NNNN Detected in frame_506.jpg: Mars Rover Markov Chain Transition Matrix, P 0.4 $2 $3 54 $5 04 0. 4 04 04 56 04 di az $7 0.6 a P - 0.2 0. 2 02 0.3 06 S ' 70.6 0.1 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0. Mars Rover Markov Chain Transition Matrix, P Si 0.4 52 0.6 a Cooopool - 0. 4 $3 S4 0.2 0.3 0.5 S' 70.6 0.41 0. 0 0.0 0 0 0, S' ' 0.1 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.24  frame_509.jpg: Mars Rover Markov Chain Transition Matrix, P 온 Sz 53 SA 9, 42 55 56 ST a 06 P. Compost 1.Z 0.2 LA 0:2 S' /0.6 0.4 0 0 0 51 0 00.2 0.6 S ' 70.6.41 0.0 0 0.3 0.1 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.20 0. The Mars Rover Markov Chain Transition Matrix, P S1, has been shown to be more complex than the one in the previous example. The Mars Rover Chain TransitionMatrix, P Si, is a more complex version of the Mars Rover Matrix. Mars Rover Markov Chain Transition Matrix, P Si 0.4 $2 04 a  ‘له di az’ S4 55 56 ST 0.2 0.6 a P - 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0.35 0.36 Mars Rover Markov Chain Transition Matrix, P S1 0.4 $2 $3 $3 0. 4 Ex S40.4 55 04 56 0.3 ai az ST Coorpoo a 0.6 - 0.5 0.8 0.7 0.9 0.10 0.2 0.0 0.1 1.0 1.3 1.2 1.4 1.5 1.6 1.7 1.8 1.9 1.1 2.2 2.0 2.1 3.0 3.2 3.3 3.4 3. Mars Rover Markov Chain Transition Matrix. P a, az $1 Sz 53 SA 55 ST A Ша 24 06 1.2 1.Z 0.2 8.2 02 a P. S ' /0.6 0.4 0 0 0.0 0 00 0. 4 0.6 a P - S' / 0.5 0.3 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.20 0.24 0.25 0.26 0 Coorpool Detected in frame_525.jpg: Example. Mars Rover Markov Chain Transition Matrix. P PC 5T 53 1012 A 55 $7 8.2 .02 f 3' /0.6 0.4 0.2 0.6 S' 0.3 0.5 0.8 0.9 0.10 0.1 0.0 0. Screening SF NNN IS Detected in frame_527.jpg: Mars Rover Markov Chain Transition Matrix, P Pls, Si 0.4 Sz 34 S4 $3 0.2 0.6 - S' 70.6 0. 4 0 0 0.0 0 0 - 0. 2 0.1 0.5 0.8 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0. Coorpos detected in frame_528.jpg: Mars Rover Markov Chain Transition Matrix, P P(s, 15, & Sz 53 SA m 55 04 56 α, az S 0.2 9.2 02 IZ 0. 2 3' S₁ 10.6 0.4 0 0 0. 4 0.3 0.5 0.1 0.6 02 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.24  frame_530.jpg: Mars Rover Markov Chain Transition Matrix, P P(s. 15), S1 0.4 $2, Ex 53 S4, S' 70.6, S₁ 0 0 0. 4 0.2 0.6 0,6 S ' 0.3 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0. The Mars Rover Markov Chain Transition Matrix, P P(s. 15) is a matrix that shows how the Mars Rover moves from one state to the next. P P ( S . 15 ) = 6 Si 04 S₂ 53 54 $5 56 04 0.4 04 04. 0.2 0,6 S' 70.6 0.60.4 0 0 0. 4 0 S ₁ 0.0 0.1 0.3 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.14 0.15 0. P P ( s . 15 ) = 6 $ 1 0.4 S ₁ 0.2 0.6 S ' S₁ /0.6 0.3 S' 70.6   อ 0 0.1 0.8 0.7 0.9 0.5 0.0 0. 2 0. 3 0. 4 & 54 04 $ 5 0..4 ai az 57 0. 6 a P - Cooopool 0.16 0.17 0.18 0.19 0.20 0.21 0.24 0.25 0.26 Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6 S1 0.4 $ 2 0.3 $ 3 54 0. 4 $ 6 0.2 $ 6 9 , az 57 a Coooooo ! 0.6 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0 The Mars Rover Markov Chain Transition Matrix, P P P(s. 15)=6 S1 04 S₁ 0.4 $3 9, az 54 56 0. 4 04 0.2 0.6 - S ' S ₁ 70.6 0. 2 0.3 0.5 - S' S ⁉ 70.4 0 0 0.0 0.1 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0. P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0. 4 $ 3 0.2 0.6 S' S ₁ /0.6 0.3 0.5 0.8 0.9 0.10 0.1 0.0 0. 1 0. 0. 2 0 0. 3 0 0 0 1 0 0 2 0 1. 2 1.2 1.3 1.4 1.0 1.1 1.5 1.6 1.7 1.8 1.9 1. Cocopool detected in frame_538.jpg: Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 Si 0.4 $ 2 $ 3 54 55 04 0. 4 04 56 0.2 0.6 S' S₁ 70.6 0.0 0 0 0. 2 0,6 S ' 0.1 0.3 0.5 0,2 0,3 0,4 0.7 0.8 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0 NIN Detected in frame_539.jpg: Example. Mars Rover Markov Chain Transition Matrix, P P(s. 15)=6 S₁ 0.4 $2 $2 0.3 $3 0. 4 84 S4 0.5 114 5 1.2 11.2 06 a P. '3' 10.6 0 0 0.0 0 0 1.4 0:4 0 00.4 ' 15 NIN detected in frame-540.jpg. Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 $ 1 0.4 $ 2 ✓ 53 54 0. 4 04 0 4 S5 04 04 56 04 a, az 57 .. 0.6 a - 0.2 0.1 0.3 0.5 0.0 0 0 0. 2 11.2 06 a P. ' 3 ' 10.6 0. 0 0 1.4 0 0 .0 1.2 1.3 1.1 1.6 1.5 1.7 1.8 1.9 1.0 2.2 Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 Si 0.4 $2 54 ai az $3 $5 56 57 04 0.2 0.6 S ' S₁ 70.6 0. 4 . 0 0 0. 0.3 0. 2 0.0 0 0 .0 00.4 0.1 0.8 0.9 0.5 0.7 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 Mars Rover Markov Chain Transition Matrix. P P ( s . 15 ) = 6 $ 1 0.4 $ 2. Cooopool Detected in frame_544.jpg: Mars Rover Markov Chain Transition Matrix, P P ( s . 15) = 6 Si 0.4 $ 2 04 $ 3 0. 4 S4 $ 5 0.2 0.6 S ' 0.3 0.8 0.9 0.5 0.1 0.0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 S₁ 0.4 S ₂ 0. 4 $3 $3 0.3 $ 3 0.2 0.6 IN IN IN In IN. NNN IS Detected in frame_547.jpg: Example: Mars Rover. Markov. Chain. Transition Matrix,. P P ( s . 15 ) = 6. S ' 70.60.4 0 S ⁁ 0 0.0 0.1 0.5 0.8 0.7 0.9 0.10 0.12 Screening Document Foca Detected in frame_548.jpg: Mars Rover Markov Chain Transition Matrix, P P(s. 15)=6 Si 0.4 $2 04 $3 $4 S4 $5 0. 4 0.3 0:4 a, az ST 0.21 0.2 02 0,6 a P . . S' 70.6 0.6 IN IN IN In - S ' 70.4 0 S₁ 0 0.0 0 0 0, 0.1 0.8 0.7 0.9 0.10 0.11 0.12 0. P P ( s . 15 ) = 6 Si 0.4 $ 2 04 $ 3 0. 4 S4 $ 5 0.2 0.6 S ' S₁ /0.6 0.3 $ 3 54 0.1 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6 S ₁ 0.4 S ⁁ $3 E $6 9, az S4 $5 57 0.2 0.6 S ' S₁ /0.6 0.3 S ≁ $4 $ 3 E $ 6 9. Froze NNN is Detected in frame_550.jpg: Screening Doct Screen Focu IN IN IN N NNNN  frame_552.jpg: Mars Rover Markov Chain Transition Matrix, P P(s. 15) ; az P ( S . 15 ) = . 6 Si $ 2 $ 3 S4 55 $ 6 0. 4 0.4 0.2 0.6 S' S₁ 70.6 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0. Coorpool Example. : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 $ 2 0.6 a P - 0.4 $ 3 0. 4 9 , az 54 04 55 56 $ 7 0.2 0,6 'S' 0.3 0,4 0.5 0,5 0.7 0.8 0.9 0.10 0.0 0.1 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.12 0.14 0. The Mars Rover Markov Chain Transition Matrix. P P ( s . 15 ) = . 6 $ 1 Sz 53 & 9 , az SA 55 56 $ 7 i a Coaspoo 00 04 1.2 0.2 21.2 NNNN Detected in frame_554.jpg: NNN.. Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6 Si $2 0.1 a Р Coorpool $3 di az 54 55 56 57 0.4 04 4 0. 4 04 4 06 0. 2 0.2 N 0.6 S ' 70.6 0.0 0 0.3 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0 P P ( 515 ) = 6 Coopon a P Si $ 2 04 53 S4 0.4 04 0. 4 55 04 04 a, az 56 0.6. 06 0.2 0.3 0,6 S' 70.6 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0. 4 53 0.3 55 04 56 04 9, az $7 0.6 P- 0.5 0.1 0.2 0.0 0. 2 0 0 0 1 0 0 2 0 2 1 1 1 2 2 2 1 2 1. 2 2. 2. 1 2.2 2.1 2.0 2.3 2.4 2.5 2.6 2.7 2.8 2.9 2.10 2.15 2.16 2.20 2.25 P P ( S . 15 ) = 6 Si 0.4 S2 ai az $ 3 S4 $ 5 56 ST 0.2 0.6 70.6 0. 4 . 0 S₁ 0 0 0.0 0. 0 S ⁁ 0.3 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.24 0.25 0.26 0.28 0.30 0.31 0.34 Mars Rover Markov Chain Transition Matrix. P P(s. 15) = 6 $ 1 0.4 $ 2 0. 4 $ 3 & S4 55. 0, 4 04 0,4 56 0.5 9, az S 06 1.Z 0.2 1.2 0:2 /0.4 0 0 0.6 S₁ / 0.3 0.1 0.0 0. P P ( s . 15 ) = 6 3 Sz 53 SA $ 5 9 , az S 06 1.Z 0.2 1.2 0 : 2 /0.6 0.4 0 0 0. 4 0. 2 0.6 70. 6 0 S₁ 0 0 1.4 1.0 1.1 1.3 1.5 1.6 1.7 1.8 1.9 1. 6 1. 2 1. 3 1. 5 1. 4 1. 1. 0 P= 0. 0.0.0 0.3 0. 3 0.1 Semiffos INNNN Detected in frame_589.jpg: Example: Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 Si 0.4 $2 0. 4 53 S5 S40.4 10.4 S5 04 56 04 a, az $7 0.6 0.20.6 Cloc S₁ 70.6    0 0 0.0 0.3 0.1 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 Semiffos • Detected in frame_590.jpg: N NJ NE Example: Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 S₁ 0.4 $ 2 55 53 54 0.2 0.6 Cloc S⁁ 70.6 0. 4 0 0 0. 0 00.4 0.0 0. 2 0.3 0.1 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.20 0 P P ( s . 15 ) = 6 S₁ 04 $ 2 0,4 $ 3 0.4 S4 $ 5 56 0. 4 0.2 0.6 70.6 0.3 0.5 0.8 0.7 0.9 0.1 0.10 0.0 0.01 0.02 0.03 0.06 0.07 0.08 0.09 0.12 0.14 0.15 0.16 0.17 0.18 0.20 0.21 0.22 0.23 0.24 0.25 0.26 P P(s. 15) = 6 $ 1 0.4 S₂ 53 S4 0.2 0.6 0.3 S3 & 54 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0 Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6 S₁ 04 $ 2 0.4 $ 3   อ 0 0.2 0.3   NNN 15 detected in frame_594.jpg: Mars Rover Markov Chain Transition Matrix, P P(s. 15)=6 S1 0.4 S₂ ai az $3 S4 $5 56 ST. 0 S⁁ 0 0 0. 4 0. 2 0.2 0.6 70.6 0.0 0.3 0.5 0.8 0.9 0.7 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.23 0.24 0.25 0.26 0.28 Mars Rover Markov Chain Transition Matrix, P P(s. 15)=6 $1 0.4 $2 $3 $4 $5 $6 $6 a, az 57 [1000000] 0.6. 0.2 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0. NNNN Detected in frame_598.jpg: Mars Rover Markov Chain Transition Matrix. P P ( S . 15 ) = 6 9 , az $ 1 Sz 53 SA $ 5 00 0.4 0.6 [ 10000 ] 04 0.2 112 0:2 /0.6 0.3 0 0 0. Mars Rover Markov Chain Transition Matrix, P P(S. 15) = 6 Si $ 2 04 $ 3 $ 5 S4 0.4 04 56 0. 4 ai az 56 57 0:4 04 0.2 0.6 0,6 [ 1000000 ] 70.60.4 0 0.0 0 0 0 1 0 1 1 1.0 1.2 1.4 2.2 2.0 2.1 2.3 2.5 2.6 2.4 3.2 3.3 3.0 3.5 3.1 3.4 4.2 Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6 S1 04 S₂ 0.4 $ 3 0. 4 & 54 0.3 ai az 56 57 0 : 4 04. 4 55 0.2 0.6 [ 1000000 ] 0.0 0 0.5 0.1 0.7 0.8 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0. P P ( s . 15 ) = 6 S1 04 S ⁁ 0.4 53 0. 4 & 54 0. four 55 0:4 56 0.3 9, az 57 0.6 0.2 0.5 70.4 0 S₁ 0 0.0 0 0 0, 0.1 0.02 0.01 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.20  frame_604.jpg: ST Sz Example. Mars Rover Markov Chain Transition Matrix, P P(S. 15) = 6 S₁ 0.4 $ 2 $ 3 $ 4 $ 5 $ 6 $ 7 d 06 12 Z 0.2 Dz 0:2 10.6 0.3 10.4 0 0 0. 4 0 0:4 0.1 10.2 0.5 10.3 0.6 10.1 0.7 10.5 0.8 10.7 0.9 10.8 0.10 10.9 0.11 10.0 0 P P ( s . 15 ) = 6 $ 1 0.4 $ 2 0. 4 $ 3 54 55 0.2 0.6 [1] initial 70.6 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0.39 0.40 0.41 0. P P ( s . 15 ) = 6 S1 04 S ⁁ 0.4 55 9, az 56 ST 0. 4 0.6 0.2 0.3 $3 $4 54 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0.35 0.36 0. Mars Rover Markov Chain Transition Matrix. P P ( S . 15 ) = 6 ST 9 , az N Sz 53 $ 4 55 56 S LLA 06 1.2 02 ΩΣ 0:2 10.6 0.4 0.6 [ 1000000 ] initial 0.3 0.2 0.5 0.1 0.0 0.8 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.12 0.13 0.14 0.15 0.16 0.17 0. Mars Rover Markov Chain Transition Matrix, P P(515) = 6 Si 0.4 $2 04 S3 0. 4 04 S4 04 04 $5 56 0:4 a, az 57 0.6 0.2 0. 2 0,6 [ 1000000 ] initial 0.1 0.3 0 0 0 S₁ 0 0 shh 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.24  frame_611.jpg: Mars Rover Markov Chain Transition Matrix, P P(s. 15)=6 S₁ 0.4 S2 04 $3 $4 & S4 a, az $5 $7 0. 4 0.6 70.6 0 S⁁ 0 0 0. 0 sht 0.2 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.20 0.21 0.22 0.23 0.24 P P ( s . 15 ) = 6 S₁ 0.4 $3 ai az 54 55 $6 57 0.2 0.6 /0.4 & S4 a , az $ 5 $ 7 0.3 0.5 0.8 0.9 0.10 0.0 0.1 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.24 0 P P ( s . 15 ) = 6 S₁ 0.4 $2 & $3 S4 55 0. 4 0.3 0.2 0.6 0 0 S ⁁ 0 0 0 initial. 0 0 1 initial 0.1 0.0 0 0. 2 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0  detected in frame_615.jpg: Mars Rover Markov Chain Transition Matrix, P P(S. 15) = 6 S1 0.4 S ⁁ 0 0 0 Stat 0.2 0.6 C1000000р initial 70.6 0 0 S₁ 00 0 Stat. 0. 2 0,6 0.3 0,4 0,5 0,8 0,9 0,10 0,11 0,12 0,13 0,14 0,15 0,16 0,17 0,18 0,19 0,20 0,23 0,24 0,  frame_616.jpg: Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.3 0.5 0.2 0.6 70.6 0. 4 55 04 560.4 ai az 57 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29 0. Soren Meising D Sorshot fea NNNN Detected in frame_617.jpg: Example: Mars Rover Markov Chain Transition Matrix, P P(s. 15) = . 6 Si 0.4 $2 04 53 S4 56 a, az $5 $7 0.3 0.2 0.6 [1000000]P initial 70.6 0. 4. 0 S₁ 0 0 0. stat 0. 2 0.0 0. Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6 S₁ 0.4 $ 2. P initsal 0.2 0.3 0.6 70.6 0. 4 9, az 57 06. [1000000] P initial 70.4 . 0 S⁁ 0 0 0.0 0 0 stat 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0 P P ( s . 15 ) = 6 Si 0.4 $2 Ex ai az $3 54 55 56 57 04 0.6 70.6 . [ 1000000 ] P initsal 0.2 0. 2 0. 4 0 0 0.0 0.3 0.1 0.5 0.8 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29 0.30 0 P P ( s . 15 ) = 6 S₁ 0.4 S2 0. 4 53 0.3 & 54 0.2 9 az 55 56 0:4 0.6 70.6 0 S ⁁ 0 0 0.0 0.1 0.5 0.8 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 Screen Ming D Scro Focus ' NINN Detected in frame_623.jpg: Example: Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 S₁ 0.4 $ 2 0. 4 $ 3 & S4 55 0.3 $ 3, az 57 0.6 . 0.2 $ 2.6 [ 1000000 ] P initial 70.6 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 P P ( s. 15 ) = . 6 Si 0.4 $ 2 0. 4 ai az $ 3 S4 56 $ 7 0.2 0.6 70.6 0 S₁ 0 0 0. Sht 0.3 0.5 0.8 0.9 0.7 0.1 0.10 0.0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0 Son M D Sort Foca NININN Detected in frame_625.jpg: Example: Mars Rover Markov Chain Transition Matrix, P P(s. 15)=6 0.4 S2 0. 4 $3 04 & S4 $5 0.3 0:4 $3 ai az S 4 $5. P P ( s . 15 ) = 6 Si 04 S2 0 : 4 $ 3 ai az S4 $ 5 $ 6 57 0.4 0. 4 04 0.6 [ 1000000 ] P initial 0.2 0.3 0.5 0.1 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S ⁁ 0.4 S⁁ 70.6 0. 4 0 0 0.0 0.2 0.6 [ 1000000 ] P initial S₁.70.60.4 0.5 0.7 0.8 0.9 0.10 0.3 0.1 0.02 0.01 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.12 0.14 0.15 0.16 0.17 Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 $ 1 04 $ 2 ai az $ 3 S4 $ 5 $ 6 ST 04 0.2 0.4 014 04 initial sht 06 0.6 70.6 0. 4. 0 S₁ 0 0 0. Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6. P initial 0.2 0.4 0.6 70.6 0. 4 . 0 S₁ 0.0 0 0 0 sht 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0 P P ( S . 15 ) = 6 Si 04 $ 2 0.4 $ 3 $ 6 & 9 , az S4 57 0.6 0.2 06 .. 3 0. 4 0 0 0 S₁ 0 0 initial shot 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.30 0.31 0.34 P P ( s . 15 ) = 6 S₁ 0.4 $3 & 9, az S4 $6 57 0.2 0.6 70.6 0 S ⁁ 0 0 0 sht 0.3 0.5 0.8 0.7 0.9 0.1 0.0 0. 2 0. 4 b S IS NNN. P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0. 4 $ 3 & 9 , az S4 $ 6 57 0.3 0.2 0.6 70.6 0 S⁁ 0 0 0. sht 0.1 $1 $2 $3 $4 $5 $6 $7 3 - 0.8 $7.6 . [ 1.000000 ] P = initial 0.5 56 0.7 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0  detected in frame_636.jpg: Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 S₁ 0.4 & 9 az $3 54 56 0. 4 04 0.2 0.6 0.3 0.5 0.8 0.9 0.7 0.1 0.0 0.10 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.12 0.14 0.15 0.16 0.17 0.18 0.20 0.21 P P ( s . 15 ) = 6 0.4 $ 2 0.3 $ 3 & S4 0. 4 0.2 0.6. 0 S₁ 0 0.0 0 0 0 initial sht. P P(s. 15) = 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.29 0.30 P P ( s . 15 ) = 6 $ 1 0.4 $ 2 04   อ 0 0.2 0.3 $ 5 56 0. 4 ai az $ 7 04 14 04 0.6 70.4 0 S₁ 0 0 00 initial 5ht 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0 Foo Detected in frame_640.jpg: Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6 Si 0.4 $3 S4 $5 0.2 0.6 70.6 0 S₁ 0 0 0.0 initial 5ht 0. 4 0.3 0.1 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 P P ( s . 15 ) = 6 Sz 53 SA 00 m a , az $ 5 56 4 04 0.6 [ 10000 ] P- 0.2 0. 2 11.2 4/0.6 0.4 0 0 00 0 0 stat. P P(s. 15)=6 S₁ 0. 4 $2 $2 w' 04 04 & S4 0..4 55 04 56 a, az 57 0.3 04.6 CP= initial. P = 0.0 0.1 0.02 0.01 0.03 0.04 0.  frame_643.jpg: Mars Rover Markov Chain Transition Matrix, P P ( S . 15 ) = 6 $ 1 0.4 $ 2 53 S4 04 0. 4 55 04 56 a , az 57 04 040.6 CP = initial 0.2 0.6 /0.4 0 0 0. 6 0.3 0.8 0.9 0.5 0.7 0.1 0.0 0. 2 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.12 Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 S1 04 S ⁂ 34 55 $ 3 S4 0.4 0 0 0.2 0.6 70.6 0. 4 0 S₁ 0 00 0 initial sht. 0. 2 0.3 0.5 0.1 0.8 0.9 0.0 0. P a, az P ( s . 15 ) = 6 Sz 53 00 04 SA 56 0.6 0.2 1.2 11.2 0:2 /0.4 0.4 1.6 70.6. Initial sht 0. 4 0 0 0.0 0. 0 0 P= 0 0 04 0.1 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.20 0.23 0.24 0.25 0.26 0  detected in frame_647.jpg: Example: Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 $ 1 0.4 $ 2 0. 4 $ 3 & SA. 0.2 0.6 initial sht ༧,༠.༦༠ མ ། 0.3 0.8 0.9 0.5 0.1 0.0 0 0 0 1.0 1.2 1.3 1.1 1.4 1.5 1.6 1.7 1.8 1.9 1.10 1. P initial sht 0.2 0.4 0 0 0. 4 0.3 0.5 0.6 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 0.35 0.36 0.37 0.38 0.39 0.40 0.41 0. The Mars Rover Markov. Chain Transition Matrix, P P ( s . 15 ) = 6 Si S2 0.4 53 0. 4 04 S4 $5 $6 0.3 0.2 02 0,.2 0,6 0,4 0.6 0 0 00 0 0.0 0,0 0.1 0,3 0,5 0,1 0.5 0.8 0.7 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0. Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 S₂ 56 a, az 0.4 $3 S4 55 57 0. 4 0.2 0.6 0 0 0.0 0 0 sht 0.3 0.5 0.8 0.7 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29 0 Mars Rover Markov Chain Transition Matrix, P P(s. 15)=6 $1 0.4 S2 $3 S4 55.4 56 0. 4 a, az 57 0.6 0.2 0. 2 0.3 0.5 0 0 0.0 0 0 .0 0 .4 0 00.4 NNNNN 3 Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 SI 0.4 S2 04  له di az $5 56 ST 0. 4 04 14 04 04 S4 0.6 [10]P initial 0.2 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0 Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 04 $ 2 53.6 . [ 1000000 ] P initial 0.2 0.4 0.6 0. 4 0 S₁ 0 0 0. 565 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29 0.30 0.  frame_657.jpg: 21 Example: Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6 S1.4 S2 0.4 $3 E S4 55 0. 4 $4 a, az 57 0.6. [1000000]P initial 0.2 0,4 0,6 0,5 0,7 0,8 0,9 0,10 0,11 0,12 0,13 0,14 0,15 0,16 0,17 0,18 0,19 0,20 0,24 0,25 0,26 0, P P ( s . 15 ) = 6 Si 04 $ 2 $ 5 53 S4 04.4 04 104 04 56 0.4 ai az 57 GO 0.6 0.2 02 02 0,6 initial stat 70.6 [1000000]P initial 532 0.5 0.3 0.8 0.9 0.7 0.1 0.0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.14 0.15 0.16 0. P P ( s . 15 ) = 6 S1 0.4 S2 0. 4 $ 3 E S4 55 0.2 S Socia NININ $ 21 Example : Mars Rover Markov Chain Transition Matrix, P P(s. 15)=. 6 $ 1 0.6 [ 1000000 ] P initial 532 0. 6 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.20 0.21 0.24 0.25 0.26 0.28 0.  frame_661.jpg: A Example: Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 0.4 S₂ ☑ $3 S4 $5 0.2 0.6 0,6 0.3 0,4 0,5 0,8 0,9 0,10 0,11 0,12 0,13 0,14 0,15 0,16 0,17 0,18 0,19 0,20 0,24 0,26 0,27 0,28 0,29 0,30 0,31 0,32 0,34 Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 Si 0.4 S2 0. 4 53 S4 a, az 56 $7 04 0.6. S $ Detected in frame_662.jpg: Example: Mars Rover Markova Chain TransitionMatrix. P P ( s . 15 ) = 6Si 0.3 S4 S4 $3 ☑ S4 0.2 S. Mars Rover Markov Chain Transition Matrix, P P ( s . 15) = 6 Si 0.4 S2 04 53 S4 04 0. 4 a, az $ 7 0.6 0.2 0.5 0.3 0.1 0.8 0.9 0.7 0. 2 0.0 0.10 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0 The Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6 0.4 S2 0. 4 $3 & S4 $5 0.3 0.2 NNNN 0.6 0.5 S₁ 0.0 0 0 0.1 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0. P P ( s . 15 ) = 6 Si 0.2 0.4 $ 2 & $ 3 54 04 0. 4 14 $ 5 0.3 04 ai az $ 6 0 4 0.6 [ 1000000 ] P initial 0.1 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.30 P ( s . 15 ) = 6 0.4 S2 & $ 3 S4 $ 5 0.2 0.6 0.3 0.5 0.8 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0.39 0.40 0.41 0.42 0.44 0.45 0.46 P P ( s . 15 ) = 6 0.4 S2 & $ 3 S4 $ 5 0. 4 0.2 0.6 0 S₁ 0. 0 0 0.0 0. 2 0.1 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0. P P ( S . 15 ) = 6 Si $ 2 04 $ 3 S4 0 : 4 0.4 11.4 55 0. 4 04 56 04 a , az 57 0.6 0.2 02 0.5 0.3 0.1 0.0 0. 0 initial sht 5.2 0.8 70.6 70.4 0 0 0 1 0 0. 6.6. 0. P P ( S . 15 ) = 6 S₁ 0.4 $ 2 S4 ai az $ 3 $ 5 56 57 04 0. 4 0.2 0.6 70.6 0.3 $5 56 57. $ 6 57 0.5 $5 $6 $6. $6 . [ 1000000 ] P initial sht 5 . 0.1 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.24 0 Rover Markov Chain Transition Matrix, P P(s. 15)=6 0.4 S2 04 $3 S4 0. 4 $6 04 9, az 57 0.6. [1.000000] P initial stat 0.2 0.3 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29 P P ( S . 15 ) = 6 S₁ 0.4 $ 2 ai az $ 3 S4 $ 5 56 57 04 0.6 0.2 0,6 [ 10 ] P initial 70.60.4 0 0 0.0 .0 0 0 .4 0.3 0.1 0.5 0.8 0.7 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0 Mars Rover Markov Chain Transition Matrix, P P(s. 15) = 6 Si 0.4 $2 04    0.2 0.6 70.6 0. 4 0 0 00 0 0.0 0.1 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0.36 Screening Di Detected in frame_678.jpg: Mars Rover Markov Chain Transition Matrix, P P(s, 1s) = . 6 Si $ 2 04    0.4 S4 56 di az 55 57 0.2 0.6 [ 1000000 ] P initial. 0 0 0 stat 0. 4 0. 2 0.3 0.5 0.8 0.9 0.7 0.1 0.0 0.01 0.02 0.03 0.04 0.06 0.07 0.08 0.09 0.10 0.11 0.12 Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6 S₁ 0.4 S2 0. 4 $3 & S4 0.3 $4 $6 0.6. di az 55 57 0.2 $.4 $ 6 04 9 , az 57 06 . 0.5 0.7 0.8 0.9 0.10 0.1 0.0 0. 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.12 0.14 0.15 Rover Markov Chain Transition Matrix, P P(s, 1s) = 6 Si 04 S2 ai az $3 54 $5 56 57 04 0.4 04 04 14 040.6 [10]P initial 0.2 0. 2 02 0,6 70.6 0. 4 0 0 0.0 0 0 sht 0.1 0.3 0.5 0,2 0,3 0,4 0,1 0,0 0,5 0.8 0.9 0.7 0.6 1.0 1.1 1.2 1.3 1 P initial NNNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $3 S4 $5 56 04 0. 4 04 14 04 D.A di az $7 0.6 initial shte 0.2 0.5 0.3 0.8 0.1 0.0 0 0 0 .0 0. The Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = 6 S ⁁ 0.4 S⁁ $3 $5 $5, az 57 0.6. [1000000] Ps initsal 562 0.2 0.3 0.5 0. 4 0.8 0.9 0.7 0.1 0.0 0 0 0 .0 00.4 0. 2 0. Mars Rover Markov Chain Transition Matrix, P P(s. 15)=6 S1 0.4 $2 $3 $4 S4 $5. 0 0 S₁ 0 0.2 0.3 $2 S4 di az $ 5 56 $ 7 0. 4 D.A 04 14 04 initial sht 0.6 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 P P ( s , 1s ) = . 6 Si S2 04 0.6 [ 1000000 ] P initial 562 M Detected in frame_686.jpg: Mars Rover Markov Chain Transition Matrix, P P(s, 1s. ) = 6 S1 0.4 $ 2 $ 3 $ 4 $ 5 $ 6 $ 7 $ 8 $ 9 $ 10 $ 11 $ 12 Sver Markov Chain Transition Matrix, P 0 Exaniple Man 5 105 0.4 0 ST 0 а 0 04 02 04 0 0 Example Sam 0. 0. 4 0.2 0.3 NINA Detected in frame_687.jpg: Mars Rover Marko Glam Episuds 151 \" Example Samolic sprandes stuming from 54 \" Mars Rover Markov Chain Episodes $1 0.4 0.2 0.6 0.3 0.5 0.8 0.9 0.10 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.34 0.36 0.38 0.39 0.40 0.41 0.44 0.45 0.46 0.48 0. Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_692.jpg: Example: Mars Rover Markov Chain Episodes S₁ Sz 53 1.4 0,4 SA $5 $6 0.40.4 A DA 0. 4 0:4 04 ST 06 0.2 0. 2 02 0.3 0.6 Example: Sample episodes starting from $4 S4, S5, S6, 57, 57,. 57... 54, 54, 55, 54,. 55, 56.... 54, 53, 52, 51.... SM D 100 Рос The Mars Rover Markov Chain Episodes is an example of a Mars Rover episode. It is a series of episodes that take place over the course of a few weeks. The episodes are written by the same team as the Mars Rover. Stop Screen Mirroring Document Screenshot focus \" I. IN IN IN N Example : Mars Rover Markov Chain Episodes $ 1 0.4 $ 2 04 0. 4 $ 3 $ 3 0.3 $ 4 $ 5 56 $ 7 0.2 0.5 0.6 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 Mars Rover Markov. Chain Episodes $1 $2 $3 S4 $5 $6 $7 0.4 04 0. 4 0.2 0.6 0. 2 0.3 0.1 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 0.36 0.37 0.38 Screen M D Screenshot Focus NNNN Detected in frame_700.jpg: Example: Mars Rover Markov Chain Episodes $1 $2 $3 S4 $5 $6 57 0.4 04 0. 4 0.2 0.6 02 0. 2 0.3 0.1 0.0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 Screen Ming Document Screenshot Focus NNNN \" Detected in frame_702.jpg: Example: Mars Rover Markov Chain Episodes $1 0.4 Sz $3 S4 $5 $6 57. 6 57 0. 4 04 0.3 0.2 0.6 02 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 Stop Screen Mirroring Document Screenshot Focus NNNN \" Detected in frame_703.jpg: Example: Mars Rover Markov Chain Episodes $1 $2 $3 S4 $5 $6 $7 0.4 04 0.2 0.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 Screening D is NNNN Detected in frame_705. frame_704.jpg: Example: Mars Rover Markov Chain Episodes $1 $2 $3 S4 $5 $6 57 0.4 04 0. 4 0.2 0.6 0.5 $7 0. 2 0.3 0.1 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 Stop Screen Mirroring Docum Screenshot A Focus ANNON \" Detected in frame_706.jpg: Example: Mars Rover Markov Chain Episodes $1 0.4 $2 $3 S4 $5 $6 57 04 0.2 0.6 0.3 0.8 0.5 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 S5 0. 4 0.4 0.2 0.6 0.5 $ 6 57 04 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0.39 0.40 Stop Screen Mirroring Docum Screenshot Focus \" IN INN Detected in frame_710.jpg: Example: Mars Rover Markov Chain Episodes $1 Sz $3 S4 $5 $6 57 0.4 04 0.2 0.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0 Screen Ming Document Screenshot WAT Focus NNNN \" Detected in frame_712.jpg: Example: Mars Rover Markov Chain Episodes $1 0.4 0.3 S2 04 $3 S4 $5 $6 $7 0.2 0.6 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.26 0.28 0.30 0.31 0.32  Detected in frame_713.jpg: Sample episodes starting from S4 54, 55, 56, 57, 57.57 S4 , S4, S5, 54,55,56, ... 54. 53; 52: 51.... 3 Detected in frames_714.jpg and frame_715.jpg. Mars Rover Markov Chain Transition Matrix, P P/s | 5) = 6 ST 53 S ய a NNNN 112 VE 0.2 BZ 02 /0.6 0.4 0 0 0.  frame_716.jpg: Example Mars Rover Markov Chain Transition Matrix, P P(s, 1s)=.6 Sz N M 53 55 56 $7 06 [100000]P initial sht 0.4 $4 04 11.4 0.2 02 0:2 06 /0.6 0. 4 0 0 0.0 0 0 1.2 0.3 1.4 1.3 2.2 2.3 3.2 4.2 5.2 6.2 7.2 8.2 9.2 10.2 11.2 12.2 13.2 The Mars Rover Markov Chain Transition Matrix, P P ( s . 15 ) = . 6 Si 0.4 $ 2 0. 4 $ 3 & 54 0.2 0.6 initial sht S₁ 70.6 0.3 0.5 0.8 0.9 0.7 0.1 0.0 0. 2 0 0 0 1 0 0. 1 0 1 1 1 2 2 2 1 2 1 1. 2 1.2 1.3 1.4 1.1 1.0 1.6 1.5 1.7 1.8 1.9 1. Mars Rover Markov Chain Transition Matrix, P P(s, 1s) = . 6 Si 0.4 S2 04   0.2 02 0,6 initial State 70.6 0. 4 0.6 00 0.5 57 0.0 0.1 0.3 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0. P P ( s , 15 ) = 6 $ 1 0.4 $ 6 a , az 57 0. 4 $ 3 $ 3 S4 $5 $5 0.6 [ 1000000 ] P initial 0.2 0.3 0 0 0 sht 0.1 0.5 0 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.30 0.  frame_721.jpg: Mars Rover Markov Chain Transition Matrix, P P(s, 1s)=.6 Si 04 S2 04 $3 0:4 0.4 di az $5 56 $7 0. 4 14 04 3 0.6 0.2 02 0. 2 DUG /0.4 0 0 00 0 0 S₁ 0 initial sht. 0.3 b Son M D Stoc NININN Detected in frame. P P ( s , 1s ) = 6 9 , az 0.4 $ 3 S4 $ 5 $ 6 57 0.2 0.6 [ 1000000 ] P initial sht 0. 6 Si $2 04 $3 S4 56 di az $5 $7 0. 4 S₁ 0 0 0. 0 0 P = 0 00. 4 0. 2 0.0 0.1 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.20  frame_724.jpg: Example: Mars Rover Markov Chain Transition Matrix, P P(s, 1s)=6 0.4 S ⁁ 0.3 $3 S4 $5 0.2 0.6 [ 1000000 ] P initial sht 0. 6 0. 4 0 0 0 S₁ 0 00.4 0. 2 0.0 0.1 0.5 0 0.8 0.7 0.9 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.20 0. Ars Rover Markov Chain Transition Matrix, P -.6 02 ма Z 5- L 04 70.6 0.4 0 D 0 0 00.4 b 0 0 M Example INTERN IN Example 54 $ 5.5 54 , 53 , 52 Detected in frame_726.jpg: Sample episodes starting from $4 54.55, 56, 57, 57.57. S4, 54, 55, 54,. 55, 56.... 54. 53, 52, 5.... NI Example : Mars Rover. Markov. Mars Rover Markov. Chain Episodes S₁ 0 52 0.4 06 12 $ 3 S4 A 0. 4 A $ 5 D4 $ 6 ம 57 2 1X2 02 0.6 Example : Sample episodes starting from $ 4 54.55 , 56 , 57 , 57.57 . S4 , 54 , 55 , 54, 55, 55 , 56 .... 54. 53 , 52 , 5 .... NI Detected in frame_728.jpg: Example: Mars Rover. Markov Chain. Episodes $1 $2 $3 S4 S5 56 $7 04 0 Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 S5 56 $ 7 0.4 04 0,4 0. 4 0.2 0.6 02 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.27 0.28 0.30 0.31 0.32 0.34 0.36 0.38 0.39 0.40 0 Screen Mog D Screenshot Foc NNNN \" Sample episodes starting from S4 S4, S5, S6, S7, 57, 57,... ⚫S4, 54, 55, 54,. 55, 56..... S4,. 53, 52, 51..... Screen Mog D. 0.2 0.3 0.4 0.5 0.6 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 Mars Rover Markov Chain Episodes $1 Sz $3 S4 $5 $6 $7 0.4 0.2 0.6 0.3 0.5 0.8 0.9 0.1 0.0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 Screen Ming Screenshot Focus NNNN \" Detected in frame_736.jpg: Example: Mars Rover Markov Chain Episodes $1 0.4 $2 $3 S4 $5 $6 S7. 0.2 0.6 Example: Sample episodes starting from S4 S4, S5, S6, 57, 57,. 57... S4 , 54, 55, 54,. 55, 56, S4,. 53, 52, 51. Stop Screen Mirroring Document Screenshot focus NNNN ' Detected in frame_737.jpg: Example: Mars Rover Markov Chain Episodes. Sample episodes starting from $4 54. $5, 56, 57, 57.57.... S4, S4,. S5. 54, 55,. 56, 54. 53, 52, 51.... A A 15. Shop Son M 20 Рос Detected in frame_739.jpg: Sample episodes starting from S4 S4, S5, S6, 57, 57,. 57... S4,. 53, 52, 51..... Screening Ос 18 Регрей. Screen Mirroring Document Screenshot Focus \" IN IN INN Detected in frame_741.jpg: Example: Mars Rover Markov Chain Episodes $1 $2 $3 S4 $5 $6 57.4 04 0.4 0.2 0.6 0.3 0.5 0.8 0.9 0.7 0.1 0.0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0 Screening D Scho Foc ' NININN Detected in frame_742.jpg: lecture Example: Mars Rover Markov Chain Episodes $1 $2 $3 $4 S5 56 $7 04 04 0.4 DA 0.2 0.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0. F ' IN NNN Detected in frame_744.jpg: Example: Mars Rover Markov Chain Episodes. Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57, ... S4, 54, 55, 56, 57, 57,. 57... S4,. 53, 52, 51.... Screen M Свещени Screenshot Гос NANN ' Shop Screening D Screenshot NNNN Detected in frame_746.jpg: Example: Mars Rover Markov Chain Episodes $1 Sz $3 S4 S5 $6 $7 0.4 04 0. 4 0.2 0.6 0.3 0.0 0.1 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.30 0.31 Screening D Screenshot NNNN \" Detected in frame_747.jpg: Example: Mars Rover Markov Chain Episodes $1 $2 $3 S4 $5 $6 57 0.4 04 0.2 0.6 0.3 0.5 0.8 0.9 0.7 0.0 0.1 1.0 1.3 1.2 1.1 2.0 2.1 3.0 3.1 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14 Screen Moving Document Screenshot A Focus NNNN \" Detected in frame_750.jpg: Example: Mars Rover Markov Chain Episodes $1 S2 $3 S4 $5 $6 $7 0.4 04 0,2 0.2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 0,10 0,11 0,12 0,13 0,14 0,15 0,16 0,17 0,18 0,19 0,20 0,21 0,22 0,23 0,24 0,26 0,28 0 Stop Screen Mirroring Document Screenshot Focus IN IN INN Detected in frame_751.jpg: Sample episodes starting from S4 54, 55, 56, 57, 57,. 57... S4, 53, 52, 51.... Screen Ming D Form Example : Mars Rover Markov Chain Episodes $ 1 0.4 0. 4 Sz $3 S4 $5 $6 57 04 0.3 0.2 0.6 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.20 0 D Form Detected in frame_752.jpg: D Example: Mars Rover Markov Chain Episodes $1 0.4 S2 $3 S4 $5 $6 $7 04 0.3 0.2 0.6 0.5 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 Screen M D Form ' NOININ Detected in frame_755.jpg. Sample episodes starting from S4 54, 55, 56, 57, 57,. 57... S4, 53, 52, 51.... Screen Mon Саб Screenshot Focus NNNN Detected in. frame_754.jpg: Example: Mars Rover Markov Chain. Episodes $1 Sz $3 S4 $5 $6 57.4 04 0.4 0.2 0.6 0.3 0.5 0.8 0.9 0.7 0.1 NNNN \" Detected in frame_756.jpg: Example: Mars Rover Markov Chain Episodes $1 0.4 $2 04 $3 S4 $5 $6 $7 0.2 0.6 0.3 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.34 0. $6 ST 0,4 0.4 04 04 0.2 0:2 0.6 Example: Sample episodes starting from $4 S4, S5, 56, 57, 57,. 57- 54, S4,. 55, 54, 55,. 56, 54.53, 52, 51... .. 15 Detected in frame_758.jpg: T Example: Mars Rover Markov Chain Episodes. Screen Mirroring Document Screenshot IN IN INN > Example : Mars Rover Markov Chain Episodes $ 1 0.4 S2 $ 3 S4 $ 5 $ 6 $ 7 04 0.2 0.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.34 Screen Mirroring Document Screenshot IN IN INN > Detected in frame_761.jpg: Example: Mars Rover Markov Chain Episodes $1 $2 $3 S4 S5 $6 $7 0.4 0.6 0.2 0.5 10.4 10.2 10.3 10.8 10.9 10.6 10.1 10.0 9.2 9.3 9.4 9.1 8.2 7.2 6.2 5.2 4.2 3.2 2.2 1.2. 2. NNNN \" Detected in frame_763.jpg: Example: Mars Rover Markov Chain Episodes $1 $2 $3 S4 $5 $6 $7 0.4 0.2 0.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.33 0.34 0.36 Screen Mon Document Focu IN + Detected in frame_765.jpg: 2-1 Example: Mars Rover Markov Chain Episodes $1 0.4 $2 $3 S4 $5 $6 $7 04 0. 4 0.2 0.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0 NNNN \" Detected in frame_766.jpg: Example: Mars Rover Markov Chain Episodes $1 $2 $3 S4 $5 $6 57 0.4 0.6 0.2 0.5 0.3 0.1 0.0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 Screen M Document Screenshot NNNN Detected in frame_769.jpg: Example: Mars Rover Markov Chain Episodes $1 $2 $3 S4 $5 $6 57 0.4 0.6 0.2 0.3 10.4 $ 7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 Stop Screen Mirroring Document Screenshot Focus NNNN \" Detected in frame_770.jpg: Example: Mars Rover Markov Chain Episodes $1 $2 $3 S4 $5 $6 $7 0.4 04 0.3 0.2 0.6 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.28 0.30 0.31 0.32 0.34 Mars Rover Markov Chain Episodes $1 0.4 Sz $3 S4 S5 $6 $7 0.3 0.2 0.6 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.34 0.36 0.37 0.38 0.39 0.40 Stop Screen Mirroring Document Screenshot focus NNNN ' Detected in frame_773.jpg: Sample episodes starting from $4 54. $5, 56, 57, 57.57 .... 54. 54, 54, 55, 54,. 55, 56 ... 54. 53, 52, 51.... TY IN N 3 detected in frame-774.jpg. S EX Focu NININN + Detected in frame_776.jpg: Example: Mars Rover Markov Chain Episodes $1 0.4 $2 04 04 $3 S4 $5 56 $7 0. 4 0.2 0.6 02 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 S Screening D Foc ' NININ Detected in frame_777.jpg: Mars Rover Markov Chain Episodes $1 Sz $3 S4 $5 $6 $7 0.4 0.2 0.6 0.3 0.5 0.8 0.9 0.10 0.12 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.34 0.36  frame_778.jpg: Example: Mars Rover Markov Chain Episodes $1 Sz $3 S4 $5 S6 57 0.4 04 0. 4 0.2 0.6 0.5 S4, 53, 52, 51.... S Screening D Fem ' IN IN Detected in frame_779.jpg. Screen Mirroring Document Screenshot Focus \" Markov Reward Process (MRP) is a Markov Chain + rewards. P is dynamics/transition model that specifices P ( 51-51s = 5) R is a reward function R(ss) =E[rts, = 0,1] Note no actions If finite number (N) of states, can express R as a vector. Markov Reward Process (MRP) is a Markov Chain + rewards. Markov Reward process is a (finite) set of states (s = 5) R is a reward function R ( ss ) = E [ rts , = 5 ] Discount factor • Note: no actions [0,1] If finite number (N) of states, can express R as a vector S. Markov Reward Process is a Markov Chain + rewards. R is a reward function R (sts) = E [ rt | st = s ] Discount factory [0,1] Note: no actions If finite number (N) of states, can express R as a vector. Markov Reward Process (MRP) is a Markov Chain + rewards. Markov is a dynamics/transition model that specifices P ( St + 1 = 5 ' 5 = 5 ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor [ 0,1 ] Note : no actions If finite number (N) of states, can express R as a vector IN. NNNN Markov Reward Process ( MRP) is a Markov Chain + rewards. Markov reward process is a set of states. If finite number (N) of states, can express R as a vector. Markov Reward Process (MRP) is a Markov Chain + rewards. Markov Reward process is a (finite) set of states (s € S) P is dynamics/transition model that specifices P ( St + 1 s ' | 5 , = 5 ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1] If finite number (N) of states, can express R as a vector. Markov Reward Process (MRP) is a Markov Chain + rewards. Markov Reward process is a (finite) set of states (s = 5) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor. If finite number (N) of states, can express R as a vector. Markov Reward Process (MRP) is a Markov Chain + rewards. Markov Reward process is a (finite) set of states (s € S) P is dynamics/transition model that specifices P ( St + 1 s ' | 5 , = 5) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor [0,1] Markov Reward Process (MRP) is a Markov Chain + rewards. Markov Reward process is a (finite) set of states (s = 5) P is dynamics/transition model that specifices P ( 5 + 1 5 ' 5 , = 5 ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor • Note : no actions [0,1] If finite number (N) of states, can express R as a vector. Markov Reward Process is a Markov Chain + rewards. P is dynamics/transition model that specifices P ( 5 + 1 5 ' 5 , = 5 ) R is a reward function R ( s ; s ) = E [ rt | st = 5 ] Discount factor. If finite number (N) of states, can express R as a vector. Markov Reward Process (MRP) is a Markov Chain + rewards. Markov Reward process is a (finite) set of states. R is a reward function R ( s ; s ) = E [ rt | st = 5] Markov Reward Process (MRP) is a Markov Chain + rewards. ⚫S is a (finite) set of states (s = 5) P is a dynamics/transition model that specifices P ( 5 + 1 = 5 ' 5 = 5 ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor [ 0,1] If finite number (N) of states, can express R as a vector S D Sor Foca Screen Meming D Fec ININ N Markov Reward Process ( MRP) is a Markov Chain + rewards. ⚫S is a (finite) set of states (s € S) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factory [ 0,1 ] Screen Mirroring D Screenshot Focus Markov Reward Process (MRP) NNNN. Markov reward process is a Markov Chain + rewards. R is a reward function R ( ss ) = E [ rt | st = s ] Discount factory [0,1] Markov Reward Process (MRP) is a Markov Chain + rewards. If finite number (N) of states, can express R as a vector. Screen Mirroring Document Screenshot Focus NONNINN Markov Reward Process (MRP) is a Markov Chain + rewards. Markov Reward process is a (finite) set of states (s € S) P is dynamics/transition model that specifices P (St + 1 s ' | 5 , = s ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor [ 0,1 ] • Note : no actions If finite number (N) of states, can express R as a vector Screening D Detected in frame_807.jpg: Markov reward process ( MRP) NNUN Markov Reward Process is a Markov Chain + rewards. R is a reward function R (ss) = E [ rt | st = 5 ] Discount factor [0,1] If finite number (N) of states, can express R as a vector S NNN Markov Reward Process (MRP) is a Markov Chain + rewards. R is a reward function R ( ss ) = E [ rt | st = s ] Discount factory [ 0,1 ] • Note : no actions If finite number (N) of states, can express R as a vector Screening Markov Reward Process (MRP) is a Markov Chain + rewards. It is a function that can be expressed as a vector. It can be used to test the theory of Markov chains. Screening D Screenshot A Focus Detected in frame_812.jpg: Markov Reward Process (MRP) ⚫S is a (finite) set of states (s = 5)  P is dynamics/transition model that specifices P ( 5 + 1 5 ' 5 , = 5 )  R is a reward function R ( ss ) = E [ rist = s ] Discount factor € [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector S Foca NNNN Markov Reward Process (MRP) is a Markov Chain + rewards. Markov is a dynamics/transition model that specifices P ( St + 1 5'5 , = 5 ) R is a reward function R ( s ; s ) = E [ rt | st = s ] Discount factor • Note: no actions [0,1] If finite number (N) of states, can express R as a vector. Markov Reward Process (MRP) is a Markov Chain + rewards. R is a reward function R ( s ; s ) = E [ rt | st = 5 ] Discount factor [ 0.1 ] If finite number ( N ) of states, can express R as a vector. Markov Reward Process (MRP) is a Markov Chain + rewards. ⚫S is a (finite) set of states (s = 5) P is a dynamics/transition model that specifices P ( St + 1 s ' | 5 , = 5 ) R is a reward function R ( sts ) = E [ rt | st = s ] Discount factor. express R as a vector Some wit Fec NNNN Detected in frame_816.jpg: Markov Rewardprocess. Markov Reward Process (MRP) is a Markov Chain + rewards. Markov is a dynamics/transition model that specifices P(5+1 5' 5, = 5) R is a reward function R(ss) =E[rt|st = s] Discount factor. If finite number (N) of states, can express R as a vector. Markov Reward Process is a Markov Chain + rewards. ⚫S is a (finite) set of states (s = 5) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor. If finite number (N) of states, can express R as a vector. Markov Reward Process (MRP) is a Markov Chain + rewards. Markov Reward process is a (finite) set of states (s € S) P is dynamics/transition model that specifices P ( S ++ 1 = s ' st = s ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor € [ 0,1 ] • Note: no actions If finite number (N) of states, can express R as a vector. Markov Reward Process (MRP) is a Markov Chain + rewards. Markov Reward process is a (finite) set of states (s = 5) P is dynamics/transition model that specifices P ( 5 + 1 5 ' 5 , = 5 ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor. If finite number (N) of states, can express R as a vector. Markov Reward Process is a Markov Chain + rewards. R is a reward function R ( ss ) = E [ rist = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states can express R as a vector. Markov Reward Process (MRP) is a Markov Chain + rewards. ⚫S is a (finite) set of states (s = 5) P is a dynamics/transition model that specifices P ( St + 1 s ' | 5 , = 5 ) R is a reward function R ( sts ) = E [ rt | st = s ] Discount factor. If finite number (N) of states, can express R as a vector. Focus NIS N Markov Reward Process ( MRP) is a Markov Chain + rewards. Markov reward process is a set of states (s € S ) with a reward function R ( s ; Discount factor [ 0,1 ]) Markov Reward Process is a Markov Chain + rewards. R is a reward function R (ss) = E [ rt | st = s] Discount factory [0,1] If finite number (N) of states, can express R as a vector S 160. Markov Reward Process (MRP) is a Markov Chain + rewards. R is a reward function R ( ss ) = E [ rt | st = s ] Discount factory [ 0,1 ] Note: no actions If finite number (N) of states, can express R as a vector. Markov Reward Process ( MRP) is a ( finite ) set of states. P is dynamics / transition model that specifices P ( St + 1 s ' | 5 , = s ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor [ 0.1 ] Mars Rover MRP 2-1 $1.4 0.4 $2 $3 S4 $5 56 57 0. 4 0,4 04 $ 2 04 $ 3 S4. $ 5 . 56 ST 0.2 0.6 0.3 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 Focus NNNN Detected in frame_835.jpg: Example: Mars Rover MRP $1.4 0.4 $2.4 11 0.41 $3 ecture2-2 K S4 $5 56 57 0. 4 0.2 0.6 0.3 0.5 0.8 0.7 0.9 0.10 0.1 0.0 0. 1 0. 2 0. 0. 3 0. 6 0. 7 0. 8 0. 9 0. 10 0. 12 0. 13 0. 14 0.14 0.15 0.16 Focus NNUN .. IN \" Detected in frame_837.jpg: Documents Example: Mars Rover MRP Rls, $1 0.4 $2 $3 $4 2-1 lecture2-2 . S4 $5 S6 57 0.2 0.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0 Mars Rover MRP R ( S ) = $ 1 0.4 04 . $ 2 $ 3 SA $ 5 56 ST 0. 4 0.2 0.6 Reward: +1 in s₁, +10 in s7, 0 in all other states San M \" IN NISS Detected in frame_840.jpg: Mars Rover MRP R ( s , ) = | $ 1 0.4 0. 4 $2 $3 $4 cture - 1 S3 SA $ 5 56 0.3 0.5 0.6 0.7 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29 0.30 0.31 0.32 0.34 0. Screen Mirroring Document Screenshot A Focus NNNN Detected in frame_844.jpg: x Example: Mars Rover MRP R ( s ) = | $ 1 0.4 0. 4 $ 2 0.2 0.3 $3 x K R(57) = 10 SA $ 5 56 $ 7 0.5 56 57 0.6 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 0.26 0 Screen Mirroring Document Screenshot A focus N IN IN \" Detected in frame_846.jpg: Mars Rover MRP Rls,)= | $1 0.4 04 $2 04 53 - R(57)=10 S4. Reward: +1 in s₁, +10 in $7, 0 in all other states. Mars Rover MRP Rls ( s , ) = | $ 1 0.4 04 $ 2 04 53 - R ( 57 ) = 10 S4 $ 5 $ 6 57 0. 4 0.6 0.3 0.2 0.5 $6 57. $5. $6. $7. $8. $9. $10. $11. Son M D IS NINA Detected in frame_850.jpg: 2-1 x 2-2 Example: Mars Rover MRP R(s,)= | $1 0.4 $2 $3 CTC- R ( 57) = 10 S4 $5 56 ST. $3 R(57) = 0.2 0.6 0.3 0.5 0.8 0.9 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.24 Screen Mirroring Document Screenshot Focus ' N IN IN 2-1 × 2-2. Jecture - 1 R ( 577 ) = 10 57 Example: Mars Rover MRP R ( s , ) = | $ 1 0.4 $2 S3 S4 $5 56 57 0. 4 0.6 0.2 0.5 0.3 0.8 0.9 0.10 0.1 0.0 0. 1 1.0 1.1. Resun, G (or a MRP) Discounted sum of rewards from Ume stea † to orizon G = 1171 Definition of State Value Function, V ( s ) ( for a MRPs) Expected return from starting in siste s Detected in frame_878.jpg: Return & Value Function. Return & Value Function T is used to define the number of time steps in each episode. Return, G is the discounted sum of rewards from time step t to horizon 1 ++ 3 N. State Value Function, V ( s ) ( for a MRP) is the expected return from starting in state s. Return & Value Function T NNN • Definition of Horizon Number of time steps in each episode. Can be infinite. Otherwise called finite Markov reward process Definition of Return , G ( for a MRP) • Discounted sum of rewards from time step t to horizon. Return & Value Function Definition of Horizon Number of time steps in each episode. Discounted sum of rewards from time step t to horizon G₁ = ++++++³+3+... Definition of State Value Function, V(s) (for a MRP) Expected return from starting in state s V ( s ) = E [ + 1 + 1+ 1 + 2 + 3 + 3+5 = 5 ] Return & Value Function • Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return, G (for a MRP) • Discounted sum of rewards from time step t to horizon Definition of State Value Function, V ( s ) ( for a MRp) • Expected return from starting in state s 3 Return & Value Function: Number of time steps in each episode Can be infinite Otherwise called finite Markov reward process Definition of Return: Discounted sum of rewards from time step t to horizon Definition of State Value Function. V(s) (for a MRP) Expected return from starting in state s V(5) EGs s. Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite. Otherwise called finite Markov reward process Definition of Return, G, (for a MRP) • Discounted sum of rewards from time step t to horizon Definition of State Value Function, V ( s ) ( for a MRp) • Expected return from starting in state s V ( 5 ) = E | G | s = s. Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return, Ge (for a MRP) ⚫ Discounted sum of rewards from time step t to horizon San Mig D 58 Рос IN Return & Value Function Definition of Horizon Number of time steps in each episode. Discounted sum of rewards from time step t to horizon. Expected return from starting in state s V ( s ) ( for a MRP) Number of time steps in each episode. Can be infinite. Otherwise called finite Markov reward process. Discounted sum of rewards from time step t to horizon. Expected return from starting in state s V ( s ) = E [ re + re + 1 + ²re + 2 + 3 + 3+3 + ·· | S₁ = 5 ] Return & Value Function x K Definition of Horizon • Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for an MRP) • Expected return from starting in state s 3 Number of time steps in each episode. Can be infinite Otherwise called finite Markov reward process. Discounted sum of rewards from time step t to horizon. Expected return from starting in state s V ( s ) = E [ G , 5 = s ] S D Return & Value Function 1 NAN N Return & Value Function • Definition of Horizon Number of time steps in each episode ■ Can be infinite •Otherwise called finite Markov reward process Definition of Return. Ge (for a MRP) • Discounted sum of rewards from time step t to horizon Definition of State Value Function. V ( s ) ( for a MRp) • Expected return from starting in state s V ( 5 ) EGs s ] [ +++++++ 5 , = sj Detected in frame_896.jpg: Return & Value function. Return & Value Function: Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return, G: Discounted sum of rewards from time step t to horizon 1++3 NNN: Expected return from starting in state s V ( 5 ) = E | G | s = s ] = E [ + 1+1+1 + 1 + 2 + 3 + 3+3 + 15] Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite. Discounted sum of rewards from time step t to horizon 1 ++ 3 NNN Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s.  Detected in frame_900.jpg: Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return. G (for a MRP) Return & Value Function Definition of Horizon Number of time steps in each episode. Discounted sum of rewards from time step t to horizon. State Value Function, V ( s ) ( for a MRP) Expected return from starting in state s. Return & Value Function 1: Number of time steps in each episode. Return and Value Function 2: Discounted sum of rewards from time step t to horizon. State Value Function 3: Expected return from starting in state s V ( 5) Return & Value Function • Definition of Horizon Number of time steps in each episode ■ Can be infinite. Discounted sum of rewards from time step t to horizon 1 ++ 3 Definition of State Value Function, V ( s ) ( for a MRP) Expected return from starting in state s V ( 5 ) EGs s ] | +++++ ³ 3+ NNN .. N Definition of Horizon Number of time steps in each episode ■ Can be infinite. Discounted sum of rewards from time step t to horizon. Expected return from starting in state s 11-3 = Detected in frame_907.jpg: Return & Value Function FF Return & Value Function C Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return, G (for a MRP) Expected return from starting in state s V ( 5 ) E | G | s = s ] = E [+ 1 + 1 + 2 + 2+ 3 + 3 + .. = Detected in frame_909. Return & Value Function 2-1 Definition of Horizon Number of time steps in each episode. Can be infinite or finite Markov reward process. Discounted sum of rewards from time step t to horizon. Expected return from starting in state s V ( 5 ) EGs, s. Return & Value Function • Definition of Horizon Number of time steps in each episode. Can be infinite or finite Markov reward process. Discounted sum of rewards from time step t to horizon. State Value Function, V ( s ) ( for a MRP) • Expected return from starting in state s A Return & Value Function: Number of time steps in each episode. Horizon: Can be infinite, otherwise called finite Markov reward process. Expected return: Discounted sum of rewards from time step t to horizon. State Value Function, V ( s ) ( for a MRP): Ex expected return from starting in state s V. Expected return from starting in state s NININ N Return & Value Function Definition of Horizon Number of time steps in each episode. Can be infinite. Otherwise called finite Markov reward process Definition of Return. Ge (for a MRP) Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite. Discounted sum of rewards from time step t to horizon +3 Definition of State Value Function, V(s) (for a MRP) Expected return from starting in state s V ( s ) = E [ + V + 1 + √² + 2 + 7³ +3 + · · · | S₁ = 5] Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return, G (for a MRP) • Discounted sum of rewards from time step t to horizon A. Expected return from starting in state s V ( s ) = E [ G | 5 = s ] = + 1 + 1+ 7 + ¼ + 2 + 3 + 3 - | St = 5 ] S Fea NINN N Detected in frame_918.jpg: Return & Value Function x Definition of Horizon Number of time steps in each episode. Expected return from starting in state s V ( 5 ) E | G | 5 = s. Detected in frame_920.jpg. Return & Value Function Definition of Horizon Number of time steps in each episode. Discounted sum of rewards from time step t to horizon. Expected return from starting in state s V ( s ) = E [ G , 5 = s ] S D Detected in frame_921.jpg: Return & Value Function: Number of time steps in each episode Definition of Horizon: Can be infinite Otherwise called finite Markov reward process Definition of Return. Ge: Discounted sum of rewards from time step t to horizon Definition of State Value Function. V: Expected return from starting in state s Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP) Expected return from starting in state s V ( 5 ) = E [ GS = s ] = E[+1 + 1 + 1+2+3+3 + |S;=5] 150 Semen M From NNNNN Detected in frame_926.jpg: Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return, G (for a MRP) Expected return from starting in state s V ( s ) = E [ G , ❘s , = s ] = E + V + 1 + 7² + 2 + 3 +3 + · · · | S₁ = S ] Screen M NININ N Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite. Discounted sum of rewards from time step t to horizon Definition of State Value Function, V ( s ) ( for a MRP) Expected return from starting in state s V(s) = E [ re + re + 1 + 7²re + 2 + 3³ +3 + ·· | S₁ = 5 ] Son Mo Detected in frame_928.jpg: Return & Value Function • Definition of Horizon Number of time steps in each episode • Can be infinite. Definition of Return, Ge (for a MRP) ⚫ Discounted sum of rewards from time step t to horizon 3 Return & Value Function • Definition of Horizon Number of time steps in each episode • Can be infinite. finite Markov reward process Definition of Return, G (for a MRP) • Discounted sum of rewards from time step t to horizon NNNN Definition of State Value Function, V ( s ) ( for a MRp) • Expected return from starting in state s V (s ) = E [ re + re + 1 + ²re + 2 + 3 + 3+3 + ·· | S₁ = 5 ] Screening Screenshot Focus Detected in frame_930.jpg: Return Return & Value Function • Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process. Return, G (for a MRP) • Discounted sum of rewards from time step t to horizon K. State Value Function, V ( s ) ( for a MRp) • Expected return from starting in state s. Som From = NOIS INN $  frame_932.jpg: Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return, G (for a MRP) • Discounted sum of rewards from time step t to horizon. Return & Value Function - Number of time steps in each episode. Can be infinite Otherwise called finite Markov reward process Definition of Return, G (for a MRP) ⚫ Discounted sum of rewards from time step t to horizon 1 ++ 3 Definition of State Value Function, V ( s ) ( for aMRP) Expected return from starting in state s. Return & Value Function - Definition of Horizon Number of time steps in each episode. Can be infinite Otherwise called finite Markov reward process Definition of Return. Ge (for a MRP) • Discounted sum of rewards from time step t to horizon Definition of State Value Function. V ( s ) ( for aMRP ) • Expected return from starting in state s V ( 5)  Detected in frame_937.jpg: Return & Value Function 2-1 Definition of Horizon: Number of time steps in each episode. Return: Discounted sum of rewards from time step t to horizon. State Value Function: Expected return from starting in state s V ( s ) = E [ Gs = s ] E [ ++++++ ³ ++ 3 + | St = 5 ] Definition of Horizon Number of time steps in each episode ■ Can be infinite. Expected return from starting in state s NNNN S Detected in frame_939.jpg: Return & Value Function 711. Return & Value Function 711- A Definition of Horizon Number of time steps in each episode. Discounted sum of rewards from time step t to horizon. Expected return from starting in state s V ( s ) = E [ G₁sts ] = E[+Y+1 +²+2+7³ +3 + - · | St=5] NINN Detected in frame_942.jpg: Return & Value Function NNN Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return, G, (for a MRP) • Discounted sum of rewards from time step t to horizon. Return & Value Function • Definition of Horizon • Number of time steps in each episode. Return, Ge (for a MRP) • Discounted sum of rewards from time step t to horizon NNNN State Value Function, V ( s ) ( for a MRp) • Expected return from starting in state s V(s) = E [ G | st = s ] = E[ ++++++ √³ + 3 + ·· | St = 5 ] Screen Ming D Detected in frame_945.jpg: Return & Value Function. Number of time steps in each episode. Can be infinite. Otherwise called finite Markov reward process. From Detected in frame_946.jpg: Return & Value Function A AM --- • Definition of Horizon • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process. Screening Detected in frame_948.jpg: Return & Value Function NNNN. Definition of Horizon: Number of time steps in each episode. Return: Discounted sum of rewards from time step t to horizon. State Value Function: Expected return from starting in state s V ( s) Return & Value Function NNNN • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return, G (for a MRP) • Discounted sum of rewards from time step t to horizon. Expected return from starting in state s V ( s ) = E [ G❘st = s ] = E[r₁ + 7+1+2+2 +3+3 + - - · | S�á = 5] Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return. Ge (for a MRP) • Discounted sum of rewards from time step t to horizon. Screening D Foc Detected in frame_952.jpg: Return & Value Function Th. Definition of Horizon: Number of time steps in each episode • Can be infinite. 5 Detected in frame_953.jpg: Return & Value Function cture-1 NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return, G (for a MRP) • Discounted sum of rewards from time step t to horizon Return & Value Function Th AAM- • Definition of Horizon • Number of time steps in each episode • Can be infinite. Otherwise called finite Markov reward process. Definition of Return. Ge (for a MRP) • Discounted sum of rewards from time step t to horizon. Expected return from starting in state s V ( s ) E | G | 5 = s ] = E [ + 1 + 1. + 2 + 3 + 3 = 5 = 5 ] 5 Detected in frame_955.jpg: Return & Value function NNNN. Screen M D Detected in frame_956.jpg: Return & Value Function Th AAM- • Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return. Ge (for a MRP) • Discounted sum of rewards from time step t to horizon Expected return from starting in state s V ( s ) E | G | 5 = s ] = E [ + 1 + 1. time step t to horizon + 2 + 3. + 3 + 5 = 5 ] 5 Detected in frame_957.jpg: Return & Value Function NNNN. Horizon: Number of time steps in each episode. Return: Discounted sum of rewards from time step t to horizon. State Value Function: Expected return from starting in state s V ( 5) E | G | 5 = s. Schot Detected in frame_960.jpg: Return & Value. Screening Schot detected in frame-960.000: Return and Value. Screening Schot Return & Value Function NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Return & Value Function NNNN Definition of Horizon Number of time steps in each episode. Otherwise called finite Markov reward process Definition of Return Ge ( for a MRP) Discounted sum of rewards from time step t to horizon. State Value Function, V ( s) ( for a MRP) Expected return from starting in state s V ( 5) E | G | s = s ] = E [ + 1 + 1+1+2+2 + 3 + 3+3+ Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite. Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return, G (for a MRP) • Discounted sum of rewards from time step t to horizon. Expected return from starting in state s V ( s ) = E [ Gs = s ] = E ( +++++++ ³ + 3 + | St = 5 ] Soren   Detected in frame_966.jpg: Return & Value Function [え] Return & Value Function - NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite. Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon. State s V ( s ) = E [ G , st = s ] = E[+ 1 + 1 + 7² + 2 + 3 +3 + · - · | S₁ = 5 ] Detected in frame_969.jpg: Return & Value Function cture-1 NNNN • Definition of Horizon • Number of time steps in each episode. Return & Value Function cture - 1 NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return. Ge (for a MRP) • Discounted sum of rewards from time step t to horizon.  Detected in frame_971.jpg: Return & Value Function cture-1 NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return, G (for a MRP) • Discounted sum of rewards from time step t to horizon. Screen Mirroring  Detected in frame_972.jpg: Return & Value Function 2-1 NANON • Definition of Horizon • Number of time steps in each episode • Can be infinite. Screen Mirroring Document Detected in frame_974.jpg: Return & Value Function A A • Definition of Horizon. Number of time steps in each episode. Discounted sum of rewards from time step t to horizon. Stop Screening Document Return & Value Function NANON • Definition of Horizon • Number of time steps in each episode • Can be infinite. Expected return from starting in state s V ( 5 ) E | G , 5 , 5 ] El + 1 + 1. + 2 + 3 + 3. Return & Value Function • Definition of Horizon • Number of time steps in each episode. infinite ⚫ Otherwise called finite Markov reward process Definition of Return. Ge (for a MRP) • Discounted sum of rewards from time step t to horizon. Definition of Return. G (for a MRP) • Discounted sum of rewards from time step t to horizon. Expected return from starting in state s V ( s ) E | G | 5 = s ] = n + 1 + 1+1+2 + 2 + 3 + 31 Return & Value Function. V (s) E |G|5=s]=n+1-1-2+2+3+31  frame_979.jpg: Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return, Ge (for a MRP) Definition of Return. G (for a MRP) • Discounted sum of rewards from time step t to horizon. Expected return from starting in state s V ( s ) E | G | 5 = 5 ] = B ++++ 2 + 3 + 31 3 Detected in frame_982.jpg: Return & Value Function. Screen D Foam Detected in frame_983.jpg: Return & Value Function AAM • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return. Ge (for a MRP) • Discounted sum of rewards from time step t to horizon. Return & Value Function: Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return, G (for a MRP) • Discounted sum of rewards from time step t to horizon Definition of State Value Function, V ( s) ( for a MRp) • Expected return from starting in state s. Markov reward process Definition of Return. Ge (for a MRP) • Discounted sum of rewards from time step t to horizon. Expected return from starting in state s V ( 5 ) E | G₁ | 5 = s ] = El ++++ 2 + 3 + 3+ 5 = 5 ] Detected in frame_986.jpg: Return & Value Function 2 A 3. State Value Function, V ( s ) ( for a MRP) • Expected return from starting in state s V ( S ) = E [ G , st = s ] = E. Return & Value Function NNNN • Definition of Horizon • Number of time steps in each episode. Return & Value Function NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite. Otherwise called finite Markov reward process Definition of Return. G (for a MRP) • Discounted sum of rewards from time step t to horizon. L Detected in frame_990.jpg: Return & Value Function NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite. Otherwise called finite Markov reward process Screening Screenshot Detected in frame_991.jpg: Return & Value Function 3. Definition of Horizon Number of time steps in each episode. Otherwise called finite Markov reward process Definition of Return. G (for a MRP) If episode lengths are always finite, can use 7 = 1 Discount Factor. Mathematically convenient (avoid infinite returns and values) Humans often act as if there's a discount factor <1 0: Only care about immediate reward = 1: Future reward is as beneficial as immediate reward. Screening D NINN Discount Factor • Mathematically convenient ( avoid infinite returns and values) Humans often act as if there's a discount factor < 1 0: Only care about immediate reward 1: Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1. Human often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward. If episode lengths are always finite, can use y = 1 Doct 100 Рос N Discount Factor • Mathematically convenient ( avoid infinite returns and values) Screen M D Scho Form Si Discount Factor. Mathematically convenient ( avoid infinite returns and values)\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Summary:\")\n",
    "print(summary_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
