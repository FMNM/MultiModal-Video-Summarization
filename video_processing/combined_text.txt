 All right. So, last time we were starting to talk about just sort of the general overview of what reinforcement learning involves. And we introduced the notion of a model, a value, and a policy. So, it's good to just refresh your brain right now about what, what those three things are. Can anybody remember off the top of their head what a value, a model, or a policy was in the context of reinforcement learning? Um, so a policy is a set of actions that, uh, the agent should take in the world. Exactly right. So, the definition of a policy is a mapping from the state you're in to what is the action, um, to take. And it might be a good policy or a bad policy. And the way we evaluate that is in terms of its expected discounted sum of rewards. Does anybody remember what a model was? Yeah. A model is like a representation of the world and how that changes in response to agents' actions. Yes. Right. So, normally we think of a model incorporating either reward model or a decision or, or a dynamics model which specifies in response to the current state and, uh, an action how the world might change. Could be a stochastic model or a deterministic model. Um, and the reward model specifies what is the expected reward, um, that the agent receives from taking a state in a particular action. So, what we're going to talk about today is, um, thinking about if you know a model of the world. So, you know, um, what happens if you take an action in a particular state or what the distribution of next states might be if you take an action, um, how we should make decisions. So, how do we do the planning problem? So, we're not going to talk about learning today. We're just going to talk about the problem of figuring out what is the right thing to do. When your actions may have delayed consequences, which means that you may have to sacrifice immediate reward in order to maximize long-term reward. So, as we just stated, um, the model generally we're going to think about are statistical or mathematical models of the dynamics and the reward function. A policy is a function that maps the agent's eight, uh, the agent states to actions and the value function is the expected discounted sum of rewards, um, from being in a state, um, and or an action and then following a particular policy. So, what we're going to do today is sort of, um, build up from Markov processes, um, up to Markov decision processes. And this build, I think, is sort of a nice one because it sort of allows one to think about what happens in the cases where you might not have control over the world but the world might still be evolving in some way, um, and think about what the reward might be in those sort of processes for an agent that is sort of passively experiencing the world. Um, and then we can start to think about the control problem of how the agent should be choosing to act in the world in order to maximize its expected discounted sum of rewards. So, what we're going to focus about on today and, and most of the rest of the class is this Markov decision process, um, where we think about an agent interacting with the world. So, the agent gets to take actions typically denoted by A. Those affect the state of the world in some way, um, and then the agent receives back a state and a reward. So, last time we talked about the fact that this could in fact be an observation instead of a state but that when we think about the world being Markov, we're going to think of an agent just focusing on the current state. Um, so the most recent observation like, you know, whether or not the robots laser range finder is saying that there are walls to the left or right of it, as opposed to thinking of the full sequence of prior history, of the sequences of actions taken and the observations received. Um, as we talked about last time, you can always incorporate the full history to make something Markov. But most of the time today we'll be thinking about sort of immediate sensors. If it's not clear, feel free to reach out. So, what did the Markov process mean? The Markov process is to say that the state that the agent is using to make their decisions is a sufficient statistic of the history which means that in order to predict the future distribution of states on the next time step, here we're using t to denote time step. But given our current state S t and the action that is taken A t, this is again the action. Um, but this is equivalent to if we had actually remembered the entire history where the history recall was going to be the sequence of all the previous actions and rewards at next states that we have seen up until the current time point. And so essentially it allows us to say that the future is independent of the past given some current aggregate statistic about the present. So, when we think about a Markov process or a Markov chain, we don't think of there being any control yet. There's no actions. Um, but the idea is that you might have a stochastic process that's evolving over time. Um, so whether or not I invest in the stock market, the stock market is changing over time and you could think of that as a Markov process. Um, so I could just sort of be passively observing how the stock market for a particular, the stock value for a particular stock is changing over time. Um, and a Markov chain is, is sort of just the sequence of random states where the transition dynamic satisfies this Markov property. So, formally the definition of a Markov process is that you have, um, a finite or potentially infinite set of states and you have a dynamics model, which specifies the probability of the next state given the previous state. There's no rewards, there's no actions yet. Um, and if you have a finite set of states, you can just write this down as a matrix. Just a transition matrix that says you're starting in some state, what's the probability distribution over next states that you could reach. So, if we go back to the Mars Rover example that we talked about last time, um, in this little Mars Rover example, we thought of a Mars Rover landing on Mars and there might be different sorts of landing sites, um, so maybe our Mars Rover starts off here. And then it can go to the left or right, um, uh, under different actions or we could just think of those actions as being A1 or A2 where it's trying to act in the world. Um, and in this case, uh, the transition dynamics, it doesn't, we don't actually have actions yet and we just think of it as sort of, maybe it already has some way it's moving in the world, the motors are just working. And so in this case, the transition dynamics looks like this, which says that, for example, the way you could read this is you could say, well, the probability that I start in a particular state S1, um, and then I can transition to the next state on the next time step is 0.4. There's a 0.6 chance that I stay in the same state on the next time step. Yeah. Um, which dimension represents the start state? Um, so this is a great question. Well, which dimension, which, which state is the start state? I'm not specifying that here. Um, uh, in general, when we think about Markov chains, we think about looking at their steady state distribution. So their stationary distribution will converge to some distribution over states, that is independent of the start state if you run it for long enough. Oh, sorry. I meant to ask like on that matrix, which dimension represents the initial state of- Oh, you mean like where you are right now? Yeah. So in this particular case, you could have it as, um, the transition of saying if you start in state, uh, let me make sure that I get it right in this case. So if you start in state here, um, so this is your initial start at a state that's 1, and then you take the dot product of that with- I may have- let me see if I get it right in terms of mixing it up. It's either on one side or the other side and then I may have transitioned it. Um, I think you'll have to do it for the other side here. Yep. It'll be flipped. So you would have your initial state, so 1, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, and then times p. And that would give you your next state distribution as prime. Yeah. Um, so what are the probabilities computed off of like the rewards? I guess probably based on the reward of going from state 1 to 2 or- Great question. So it was, you know, what are these transition probabilities looking at? Does it relate to the reward? In this case, we're just thinking of Markov chains, so there's no reward yet and there's no actions. And this is just specifying that there's some state of the, of the process. So it's as if your, let's say your agent, um, had some configuration of its motors. You don't know what that is. It was set down on Mars. And then it just starts moving about. And what this would say is, this is the transition probabilities of if that agent starts in state, I can write it this way. So if it starts in state S1, then the probability that it stays in state S1 is 0.6. So the probability that if you're starting in this particular state here, on the next time step that you're still there is 0.6, because of whatever configuration of the motors were for that robot. So it requires some understanding of the world already. This is specifying that this is how, yeah, this is how the world works. So that's a great question. So we're assuming right now this is, um, that this Markov process is a state of the world. That you are, there is some, that the environment you're in is described as a Markov process, and this describes the dynamics of that process. We're not talking about how you would estimate those. This is really as if this is how that world works. This is like, this is the, this is the world of the fake little Mars Rover. We have any questions about that? Yeah. Does your one-pot vector need to be transposed when you multiply by P? Yes. Yes. Yeah. Okay. Let me just write down incorrect vector notation. Would be like this. One, one, two, three, four, five, six. That would be, that would be a sample starting state you could be in, for example. So this could be your initial state, initial state, and that would mean that your agent is initially in state S1. Okay. And then if you want to know where it might be on the next state, you would multiply that by the transition model P. Depending on the notation and whether you take the transpose of this transition model, it will be on the left or the right. It should always be obvious from context, but if it's not clear, feel free to ask us. And so what would that say? That would say, if you took the, um, the matrix multiplication of this vector which just says you're starting in state S1, what would that look like? Afterwards, it would say that you're in, um, state S1 still with probability 0.6, you're in state S2 with probability 0.4. And this would be your new, and I think that should be transposed. But it's just a one. Um, it would specify the distribution over next states that you would be in. May I have any questions about that? Okay. All right. So this is just specifying that the transition model over how the world works over time. Um, and it's just, uh, I've written it in matrix notation there to be compact, but if it's easier to think about it, it's fine to just think about it in terms of these probability of next states given the previous state. And so you can just enumerate those or you can write it in a, in a matrix form if the, if the number of states happens to be finite. So what would this look like if you wanted to think of what might happen to the agent over time in this case or what the process might look like? You could just sample episodes. So let's say that your initial starting state is S4, and then you could say, well, I can write that as a one-hot vector, I multiply it by my probability, um, and that gives me some probability distribution over the next states that I might be in, and the world will sample one of those. So your agent can't be in multiple states at the same time. So for example, if we were looking at state S1, um, it has a 0.6 chance of staying in S1 or a 0.4 chance of transitioning. So the world will sample one of those two outcomes for you, and it might be state S1. So in this case, we have similar dynamics from S4. From S4, it has a probability of 0.4 going to state S3, probability of 0.4 going to state S4, or a probability of 0.2 of staying in the same place. So if we were going to sample an episode of what might happen to the agent over time, you could start with S4, then maybe it'll transition to S5, maybe it'll go to S6, S7, S7, S7. So you're just sampling from this transition matrix to generate a particular trajectory. So it's like the world, um, you know what the dynamic- the- the dynamics is of the world, and then nature is going to pick one of those outcomes. Like sampling from sort of a probability distribution. Anybody have any questions about that? Okay. So that just gives you a particular episode, um, and we're gonna be interested in episodes because later we're gonna be thinking about rewards over those episodes and how do we compare the rewards we might achieve over those episodes. But for right now, this is just a process. This is just giving you a sequence of states. So next, we're gonna add in rewards. So that was just a Markov chain. And so now, what is a Markov reward process? Um, again, we don't have actions yet, just like before. But now, we also have a reward function. Um, so we still have a dynamics model like before. And now, we have a reward function that says, if you're in a particular state, what is the expected reward you get from being in that state? We can also now have a discount factor, which allows us to trade off between- or allows us to think about how much we weight immediate rewards versus future rewards. So again, just like before, if we have a finite number of states, um, in this case, R can be represented in matrix notation, which is just a vector because it's just the expected reward we get from being in each state. So if we look at the Mars Rover MRP, then we could, um, say that the reward for being an S1 is equal to 1, the reward for being an S7 is equal to 10, and everything else, the reward is 0. Yeah. Um, are rewards always just tied to the state you're in? I think last time you talked about it, also having an action. So why are we not considering that here? Great question. I'm saying that, um, I mentioned last time that rewards for the Markov decision process can either be a function of the state, the state in action, or state action next state. Right now, we're still in Markov reward processes, so there's no action. So in this case, um, the ways you could define rewards would either be over the immediate state or state at next state. Okay. So once we start to think about there being rewards, we can start to think about there being returns and expected returns. Um, so first of all, let's define what a horizon is. Um, a horizon is just the number of time steps in an episode. So it's sort of like how long the agent, um, is acting for or how long it's, how long this process is going on for. Um, and it can be infinite. So if it's not infinite, then we call it a finite Markov decision process. We talked about those briefly last time. Um, uh, but often we think about the case where, um, an agent might be acting forever or this process might be going on forever. There's no termination of it. The stock market is up today, it'll be up tomorrow, we expect it to be up for a long time. Um, we're not necessarily trying to think about evaluating it over a short time period. One might want to think about evaluating it over a very long time period. So within this, um, the definition of a return is just the discounted sum of rewards you get from the current time step to a horizon, and that horizon could be infinite. So a return just says, if I start off in time step t, what is the immediate reward I get? And then I transition maybe to a new state, um, and then I weigh that return reward by gamma, and then I transition again, and I, uh, weigh that one by gamma squared, etc. And then the definition of a value function is just the expected return. If the process is deterministic, these two things will be identical. But in general, if the process is stochastic, they will be different. So what I mean by deterministic is that if you always go to the same next state, um, no matter which, if you started a state, if there's only a single next state you can go to, um, then the expectation is equivalent to a single return. Um, but in the general case, we're going to be interested in these stochastic decision processes, which means averages will be different than particular runs. So for an example of that, well, let me first just talk about discount factor and then I'll give an example. Um, discount factors are a little bit tricky. They're both sort of somewhat motivated and somewhat used for mathematical convenience. Um, so we'll see later one of the benefits of math, uh, benefits of discount factors mathematically is that we can be sure that the value function sort of expected discounted sum of returns, um, is bounded as long as your reward function is bounded. Um, people empirically often act as if there is a discount factor. We weigh future rewards lower than, um, than immediate rewards typically. Businesses often do the same. If Gamma is equal to zero, you only care about immediate reward. So you're the agent is acting myopically. It's not thinking about the future of what could happen later on. Um, and if Gamma is equal to one, then that means that your future rewards are exactly as beneficial to you as the immediate rewards. Now, one thing just to note, if you're only using discount factors for mathematical convenience, um, if your horizon is always guaranteed to be finite, it's fine to use Gamma equal to one in terms of, from the perspective of mathematical convenience. Does anyone have any questions about discount factors? Yeah. My question is, does the discount factor of Gamma always have to progress in a geometric fashion or like is there a reason why you do that? So great question. So it was, you know, the- what we're defining here is that, uh, using a Gamma that progresses through this exponential geometric fashion, is that necessary? Um, it's one nice choice that ends up having very nice mathematical properties. Um, uh, the- one could try using other approaches, but it's certainly the most common one and we'll, um, we'll see later why it has some really nice mathematical properties. Scene 1: Objects detected: person. Visual: there is a woman that is standing in front of a projector screen with her hands on her hips and a black shirt and red skirt with a red and black top and white striped skirt and grey skirt are standing next to a man who is holding a white board that has his hand on his right hand in the other hand with his left side of his chest and he is wearing a gray shirt with black and blue and gray and the same pants and his shirt, and is his pants, a blue, he has a brown and there are a grey shirt that are both are his arms are on the bottom, with red, his legs, in his neck, both with the back, on him and her, while he are in a maroon and him is on, the right, her and with him with two legs and she is in her neck and on a neck with both, she are wearing his and are with?? is he? are, that his, is her with, are? and that? with he and? a shirt is?,? his? her? on and in? she? that with she, there? of him are are her is the? in that she with an? he's?s are is, Scene 2: Objects detected: . Visual: stanford engineering logo on a white background with a red and black logo in the middle of the letters stanford, and an image of a man in a suit and a woman standing on the right side of an engineer's neck with his arms in front of his back to the back of him and behind him, with the text that says, that is a sign that reads, is the words, in his head and the man, on his face, the same, he is behind the other on that he are the front, a person, to him is on, are in that are his and on and he'i are on him?? that, at the bottom of that?,? are, i? and in this is, this? with? a? is? in? on? of? the? at? he? or that man? s? to me, for the sky? i, of me? - and thats? we are? about? it? from the sign, or the name, it, we? she? you? /? out? by? an? this man and with that and i am? what? there? for that on it is that s are at that with an i '? Scene 3: Objects detected: laptop, tv. Visual: a screenshot of a computer screen with a message that reads, take a break from the war Scene 4: Objects detected: laptop. Visual: a screenshot of a computer screen with a message in the middle of the screen and an image of an object on the right side of it Scene 5: Objects detected: laptop. Visual: a screenshot of a computer screen with a message that reads, don't know how to use it Detected in frame_0.jpg: Stanford ENGINEERING Stanford ENGINEERING Detected in frame_1.jpg: Stanford ENGINEERING Stanford ENGINEERING Detected in frame_1000.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1001.jpg: Discount Factor
-
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use = 1
S
100 Рос
NANO
N Discount Factor - • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use = 1 S 100 Рос NANO N Detected in frame_1002.jpg: Discount Factor
lecture-
lecture2-1
lecture2-2
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y= 0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNN..
IN
• Discount Factor lecture- lecture2-1 lecture2-2 • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNN .. IN • Detected in frame_1003.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
7=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen M
Ос
Form
NININN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 7 = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen M Ос Form NININN Detected in frame_1004.jpg: Discount Factor
[
lecture2-2
x
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document Screenshot
Focus
NNNN Discount Factor [ lecture2-2 x • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1005.jpg: Discount Factor
ACTIVE-X
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Sen Men
NNN Discount Factor ACTIVE - X • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Sen Men NNN Detected in frame_1006.jpg: Discount Factor
[
lecture2-2
x
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y= 0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN
. Discount Factor [ lecture2-2 x • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN . Detected in frame_1007.jpg: Discount Factor
x
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor <1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
⚫ If episode lengths are always finite, can use y = 1
Screen M
Ос
Screenshot
Fem
NININN Discount Factor x • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward ⚫ If episode lengths are always finite , can use y = 1 Screen M Ос Screenshot Fem NININN Detected in frame_1008.jpg: Documents
Discount Factor
2-2
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN Documents Discount Factor 2-2 • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1009.jpg: T1 Im
Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor <1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
San M
D
-
IN T1 Im Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 San M D - IN Detected in frame_1010.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
71: Future reward is as beneficial as immediate reward
⚫If episode lengths are always finite, can use y = 1
Screen Moving Document
Screenshot
Focus
NNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 71 : Future reward is as beneficial as immediate reward ⚫If episode lengths are always finite , can use y = 1 Screen Moving Document Screenshot Focus NNNN Detected in frame_1011.jpg: NO
x
Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
⚫ If episode lengths are always finite, can use y = 1
Screen M
Doc
Schot
50 России
NININN NO x Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward ⚫ If episode lengths are always finite , can use y = 1 Screen M Doc Schot 50 России NININN Detected in frame_1012.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot 18 Рос
NNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot 18 Рос NNNN Detected in frame_1013.jpg: Discount Factor
11-
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Serw
X From
NNN Discount Factor 11- • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Serw X From NNN Detected in frame_1014.jpg: Discount Factor
x
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mon Сос
Screenshot
Focus
NNNN
" Discount Factor x • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mon Сос Screenshot Focus NNNN " Detected in frame_1015.jpg: Discount Factor
2-2
་
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN Discount Factor 2-2 ་ • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1016.jpg: Discount Factor
11-
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
•If episode lengths are always finite, can use y = 1
Sen M
X From
NNN Discount Factor 11- • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Sen M X From NNN Detected in frame_1017.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screening D
Screenshot
Focus
NNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screening D Screenshot Focus NNNN Detected in frame_1018.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screening Document
Screenshot
A Focus
NNNN
" Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screening Document Screenshot A Focus NNNN " Detected in frame_1019.jpg: Discount Factor
×
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
NININN Discount Factor × • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot NININN Detected in frame_1020.jpg: Discount Factor
ec
lecture2-1
lecture2-2
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y= 0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use = 1
Stop Screen Mirroring Document
Screenshot
Focus
NNNN Discount Factor ec lecture2-1 lecture2-2 • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use = 1 Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1021.jpg: Discount Factor
T1 im
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use = 1
S
CONN Discount Factor T1 im • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use = 1 S CONN Detected in frame_1022.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
7=1: Future reward is as beneficial as immediate reward
•If episode lengths are always finite, can use = 1
Screen M
Document
Screenshot
Foc
+ Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 7 = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use = 1 Screen M Document Screenshot Foc + Detected in frame_1023.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screening Docum
Screenshot
Facia
'
NININN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screening Docum Screenshot Facia ' NININN Detected in frame_1024.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y= 0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
'
INNINN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot ' INNINN Detected in frame_1025.jpg: ...quen
再
cture2-1
lecture2-2
Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y= 0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
⚫ If episode lengths are always finite, can use = 1
Stop
Screen Mirroring Document
Screenshot
Focus
NNON ... quen 再 cture2-1 lecture2-2 Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward ⚫ If episode lengths are always finite , can use = 1 Stop Screen Mirroring Document Screenshot Focus NNON Detected in frame_1026.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use = 1
20 Роси
IS
NNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use = 1 20 Роси IS NNN Detected in frame_1027.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
7=1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Screen Ming D
Screenshot
Focus
QA
NININN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 7 = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Screen Ming D Screenshot Focus QA NININN Detected in frame_1028.jpg: 21
x
Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor <1
0: Only care about immediate reward
y= 1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen M
D
Scho
Fo
ININ N 21 x Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen M D Scho Fo ININ N Detected in frame_1029.jpg: Discount Factor
x
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN Discount Factor x • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1030.jpg: Discount Factor
T1-
1
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
SD
20 Росси
IN
NNNN Discount Factor T1- 1 • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 SD 20 Росси IN NNNN Detected in frame_1031.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
7=1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Screening Document
Screenshot
Focus
NININN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 7 = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Screening Document Screenshot Focus NININN Detected in frame_1032.jpg: Discount Factor
2-1
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Some M
D
Scro
Росли
ININ N Discount Factor 2-1 • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Some M D Scro Росли ININ N Detected in frame_1033.jpg: Discount Factor
-
NNNN
• Mathematically convenient (avoid infinite returns and values)
• Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
focus Discount Factor - NNNN • Mathematically convenient ( avoid infinite returns and values ) • Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot focus Detected in frame_1034.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Son M
Focu
NNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Son M Focu NNN Detected in frame_1035.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
7=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screening Document
Screenshot
A Focus
NNNN
" Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 7 = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screening Document Screenshot A Focus NNNN " Detected in frame_1036.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use = 1
Seen Mering Рас
Screenshot
Focus
N N N N
' Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use = 1 Seen Mering Рас Screenshot Focus N N N N ' Detected in frame_1037.jpg: Discount Factor
NNNN
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
•If episode lengths are always finite, can use = 1
Screen M
Собщения
A Focus
+ Discount Factor NNNN • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use = 1 Screen M Собщения A Focus + Detected in frame_1038.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
7=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Moring Сос
Screenshot
Focus
NNNN
" Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 7 = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Moring Сос Screenshot Focus NNNN " Detected in frame_1039.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y= 0: Only care about immediate reward
7=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use = 1
Screen Mirroring Document
Screenshot
focus
"
NNNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 7 = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use = 1 Screen Mirroring Document Screenshot focus " NNNNN Detected in frame_1040.jpg: Documents
Discount Factor
lecture
lecture2-1
• Mathematically convenient (avoid infinite returns and values)
• Humans often act as if there's a discount factor < 1
y= 0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN
' Documents Discount Factor lecture lecture2-1 • Mathematically convenient ( avoid infinite returns and values ) • Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Stop Screen Mirroring Document Screenshot Focus NNNN ' Detected in frame_1041.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
y= 1: Future reward is as beneficial as immediate reward
⚫ If episode lengths are always finite, can use = 1
Son M
Do
50 Роси
IN
NNN
Q Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward ⚫ If episode lengths are always finite , can use = 1 Son M Do 50 Роси IN NNN Q Detected in frame_1042.jpg: Discount Factor
x
• Mathematically convenient (avoid infinite returns and values)
• Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN Discount Factor x • Mathematically convenient ( avoid infinite returns and values ) • Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1043.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Screening Document
Fea
NININN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Screening Document Fea NININN Detected in frame_1044.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
• Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNNI Discount Factor • Mathematically convenient ( avoid infinite returns and values ) • Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNNI Detected in frame_1045.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use = 1
D
130 Р
IN
NINN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use = 1 D 130 Р IN NINN Detected in frame_1046.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
• Humans often act as if there's a discount factor < 1
y= 0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN
' Discount Factor • Mathematically convenient ( avoid infinite returns and values ) • Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN ' Detected in frame_1047.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen M
D
Fre
NNN .. Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen M D Fre NNN .. Detected in frame_1048.jpg: Discount Factor
lecture2-1
lecture2-2
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN Discount Factor lecture2-1 lecture2-2 • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1049.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor <1
0: Only care about immediate reward
• = 1: Future reward is as beneficial as immediate reward
•If episode lengths are always finite, can use y = 1
Sen M
D
NNN • Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward • = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Sen M D NNN • Detected in frame_1050.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen M
D
From
NNN .. Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen M D From NNN .. Detected in frame_1051.jpg: Discount Factor
T
་
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screening Docum
Screenshot
A Focus
NNNN
' Discount Factor T ་ • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screening Docum Screenshot A Focus NNNN ' Detected in frame_1052.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
71: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screening Document
Screenshot
Focus
NNN..
" Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 71 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screening Document Screenshot Focus NNN .. " Detected in frame_1053.jpg: Discount Factor
2-2
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y= 0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Stop
Screen Mirroring Document
Screenshot
58 Роси
NNNN
' Discount Factor 2-2 • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Stop Screen Mirroring Document Screenshot 58 Роси NNNN ' Detected in frame_1054.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use = 1
Screen
D
●ININN
"
N Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use = 1 Screen D ● ININN " N Detected in frame_1055.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
71: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use = 1
Screening Document
Screenshot
Focus
NNUN.
N
' Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 71 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use = 1 Screening Document Screenshot Focus NNUN . N ' Detected in frame_1056.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen M
Doct
Screenshot
Росси
NNIS N
S Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen M Doct Screenshot Росси NNIS N S Detected in frame_1057.jpg: Discount Factor
lecture2-1
• Mathematically convenient (avoid infinite returns and values)
• Humans often act as if there's a discount factor <1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Stop
Screen Mirroring Document
Screenshot
Focus
NNNNI Discount Factor lecture2-1 • Mathematically convenient ( avoid infinite returns and values ) • Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Stop Screen Mirroring Document Screenshot Focus NNNNI Detected in frame_1058.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use = 1
Son Mwen
D
"
IN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use = 1 Son Mwen D " IN Detected in frame_1059.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
71: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mon Соси
Screenshot
Focus
..
' Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 71 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mon Соси Screenshot Focus .. ' Detected in frame_1060.jpg: Discount Factor
x
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y= 1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen M
Screenshot
A Focus
'
NININN Discount Factor x • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen M Screenshot A Focus ' NININN Detected in frame_1061.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
y= 0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1062.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use = 1
Do
From
NNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use = 1 Do From NNN Detected in frame_1063.jpg: Discount Factor
.
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Ming Docum
Screenshot
NNNN Discount Factor . • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Ming Docum Screenshot NNNN Detected in frame_1064.jpg: NO
2-1
x
Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Screening D
Fram
NNNN
' NO 2-1 x Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Screening D Fram NNNN ' Detected in frame_1065.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1066.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use = 1
Doc
Роси
IS Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use = 1 Doc Роси IS Detected in frame_1067.jpg: Discount Factor
lecture2-2
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN
+ Discount Factor lecture2-2 • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN + Detected in frame_1068.jpg: Discount Factor
2-1
-
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen M
Собе
Росси
'
IN Discount Factor 2-1 - • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen M Собе Росси ' IN Detected in frame_1069.jpg: Discount Factor
lecture2-2
.
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN Discount Factor lecture2-2 . • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1070.jpg: Discount Factor
11-
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use y = 1
D
20 Рос
NINN
+ Discount Factor 11- • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use y = 1 D 20 Рос NINN + Detected in frame_1071.jpg: Discount Factor
lecture2-2
.
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN
+ Discount Factor lecture2-2 . • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN + Detected in frame_1072.jpg: Discount Factor
2-1
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen M
Document
Рос
'
IN NINN Discount Factor 2-1 • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen M Document Рос ' IN NINN Detected in frame_1073.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
•If episode lengths are always finite, can use y = 1
Screen Mirroring Document Screenshot
Focus
NNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1074.jpg: Discount Factor
71-
• Mathematically convenient (avoid infinite returns and values)
⚫Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use = 1
Sam M
D
IN
NNNN Discount Factor 71- • Mathematically convenient ( avoid infinite returns and values ) ⚫Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use = 1 Sam M D IN NNNN Detected in frame_1075.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mon Document
Screenshot
Focus
" Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mon Document Screenshot Focus " Detected in frame_1076.jpg: Discount Factor
.
2-2
K
NNNN
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
A Focus
" Discount Factor . 2-2 K NNNN • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot A Focus " Detected in frame_1077.jpg: Discount Factor
"
x
2-2
-
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y=0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use = 1
Screen Mirroring Document
Screenshot
Focus
"
NNNNN Discount Factor " x 2-2 - • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use = 1 Screen Mirroring Document Screenshot Focus " NNNNN Detected in frame_1078.jpg: Discount Factor
2-2
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y= 0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN Discount Factor 2-2 • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_1079.jpg: Discount Factor
11-
1
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use = 1
Son Mag
F
IN
NNNN Discount Factor 11- 1 • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward If episode lengths are always finite , can use = 1 Son Mag F IN NNNN Detected in frame_1080.jpg: Discount Factor
M
lecture2-2
.
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y= 0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
A Focus
NNNN Discount Factor M lecture2-2 . • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot A Focus NNNN Detected in frame_1081.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y= 1: Future reward is as beneficial as immediate reward
⚫If episode lengths are always finite, can use y = 1
Screen Merong Doc
Screenshot
18 Росси
'
NINN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward ⚫If episode lengths are always finite , can use y = 1 Screen Merong Doc Screenshot 18 Росси ' NINN Detected in frame_119.jpg: Models. Policies. Values
⚫ Model Mathematical models of dynamics and reward
Policy: Function mapping agent's states to actions
⚫ Value function future rewards from being in a state and/or action
when following a particular policy
الك Models . Policies . Values ⚫ Model Mathematical models of dynamics and reward Policy : Function mapping agent's states to actions ⚫ Value function future rewards from being in a state and / or action when following a particular policy الك Detected in frame_12.jpg: Today's Plan
Last Time.
• Introduction
• Components of an agent: model, value, policy
⚫ This Time:
Making good decisions given a Markov decision process
• Next Time
Policy evaluation when don't have a model of how the world works Today's Plan Last Time . • Introduction • Components of an agent : model , value , policy ⚫ This Time : Making good decisions given a Markov decision process • Next Time Policy evaluation when don't have a model of how the world works Detected in frame_120.jpg: Models, Policies, Values
⚫ Model: Mathematical models of dynamics and reward
⚫ Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
S
10 ро
N Models , Policies , Values ⚫ Model : Mathematical models of dynamics and reward ⚫ Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy S 10 ро N Detected in frame_121.jpg: Models, Policies, Values
истин2-1
⚫ Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mirroring Document
Screenshot
Focus
NINN
" Models , Policies , Values истин2-1 ⚫ Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Screen Mirroring Document Screenshot Focus NINN " Detected in frame_122.jpg: lecture2-1
Models, Policies, Values
⚫ Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mirroring Document
Screenshot
Focus
INNINN lecture2-1 Models , Policies , Values ⚫ Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Screen Mirroring Document Screenshot Focus INNINN Detected in frame_123.jpg: lecture2-1
Models, Policies, Values
NNNN
⚫ Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mirroring Document
Screenshot
Focus lecture2-1 Models , Policies , Values NNNN ⚫ Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Screen Mirroring Document Screenshot Focus Detected in frame_124.jpg: Models, Policies, Values
x
NNNN
• Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Stop
Screen Mirroring Document Screenshot
Focus Models , Policies , Values x NNNN • Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Stop Screen Mirroring Document Screenshot Focus Detected in frame_125.jpg: Models, Policies, Values
2-1
x
⚫ Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
⚫ Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mirrong D
58 Росси Models , Policies , Values 2-1 x ⚫ Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions ⚫ Value function : future rewards from being in a state and / or action when following a particular policy Screen Mirrong D 58 Росси Detected in frame_126.jpg: Models, Policies, Values
-
NNNN
Model: Mathematical models of dynamics and reward
Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mirroring Document
Screenshot
Focus Models , Policies , Values - NNNN Model : Mathematical models of dynamics and reward Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Screen Mirroring Document Screenshot Focus Detected in frame_127.jpg: Models, Policies, Values
c2-2
NNNN
Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mirroring Document
Screenshot
Focus
" Models , Policies , Values c2-2 NNNN Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Screen Mirroring Document Screenshot Focus " Detected in frame_128.jpg: lecture2-2
Models, Policies, Values
NNNN
• Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mirroring Document
Screenshot
Роси lecture2-2 Models , Policies , Values NNNN • Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Screen Mirroring Document Screenshot Роси Detected in frame_129.jpg: M
Models, Policies, Values
⚫ Model: Mathematical models of dynamics and reward
⚫ Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Sein M
Doc
Росси
IN
NNN M Models , Policies , Values ⚫ Model : Mathematical models of dynamics and reward ⚫ Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Sein M Doc Росси IN NNN Detected in frame_13.jpg: Today's Plan
T
A
• Last Time:
Introduction
• Components of an agent: model, value, policy
• This Time:
Making good decisions given a Markov decision process
• Next Time:
⚫Policy evaluation when don't have a model of how the world works
Sex
NINN Today's Plan T A • Last Time : Introduction • Components of an agent : model , value , policy • This Time : Making good decisions given a Markov decision process • Next Time : ⚫Policy evaluation when don't have a model of how the world works Sex NINN Detected in frame_130.jpg: D
Models, Policies, Values
2-1
NNNN
⚫ Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mirroring Document
Screenshot
Focus D Models , Policies , Values 2-1 NNNN ⚫ Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Screen Mirroring Document Screenshot Focus Detected in frame_131.jpg: lecture2-1
Models, Policies, Values
NNNN
• Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Stop
Screen Mirroring Document
Screenshot
Focus lecture2-1 Models , Policies , Values NNNN • Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Stop Screen Mirroring Document Screenshot Focus Detected in frame_132.jpg: Models, Policies, Values
⚫ Model: Mathematical models of dynamics and reward
⚫ Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Sein M
Doc
Росси
-
IN
NNN Models , Policies , Values ⚫ Model : Mathematical models of dynamics and reward ⚫ Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Sein M Doc Росси - IN NNN Detected in frame_133.jpg: D
Models, Policies, Values
2-1
-
NNNN
⚫ Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mirroring Document
Screenshot
Focus D Models , Policies , Values 2-1 - NNNN ⚫ Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Screen Mirroring Document Screenshot Focus Detected in frame_134.jpg: Models, Policies, Values
lecture2-1
lecture2-2
NNNN
• Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Stop
Screen Mirroring Document
Screenshot
Focus Models , Policies , Values lecture2-1 lecture2-2 NNNN • Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Stop Screen Mirroring Document Screenshot Focus Detected in frame_135.jpg: Models, Policies, Values
⚫ Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
⚫ Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mon
D
58 Росси Models , Policies , Values ⚫ Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions ⚫ Value function : future rewards from being in a state and / or action when following a particular policy Screen Mon D 58 Росси Detected in frame_136.jpg: -1
2-2
Models, Policies, Values
NNNN
⚫ Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mirroring Document
Screenshot
Focus -1 2-2 Models , Policies , Values NNNN ⚫ Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Screen Mirroring Document Screenshot Focus Detected in frame_137.jpg: Documents
Models, Policies, Values
lecture2-2
NNNN
⚫ Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Stop
Screen Mirroring Document
Screenshot
Focus Documents Models , Policies , Values lecture2-2 NNNN ⚫ Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Stop Screen Mirroring Document Screenshot Focus Detected in frame_138.jpg: Models, Policies, Values
NNNN
• Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mirroring Document
Screenshot
Focus
" Models , Policies , Values NNNN • Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Screen Mirroring Document Screenshot Focus " Detected in frame_139.jpg: Models, Policies, Values
K
NNN..
⚫ Model: Mathematical models of dynamics and reward
• Policy: Function mapping agent's states to actions
• Value function: future rewards from being in a state and/or action
when following a particular policy
Screen Mirroring Document
Screenshot
Focus Models , Policies , Values K NNN .. ⚫ Model : Mathematical models of dynamics and reward • Policy : Function mapping agent's states to actions • Value function : future rewards from being in a state and / or action when following a particular policy Screen Mirroring Document Screenshot Focus Detected in frame_14.jpg: Today's Plan
-
NNNN
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screen Mirroring Document
Screenshot
Focus Today's Plan - NNNN • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screen Mirroring Document Screenshot Focus Detected in frame_140.jpg: Today Given a model of the world
Markov Processes
■Markov Rewind Punesse (MRP)
■Markov Decision Processes (MDPS)
Evaluation and Control in MDPs Today Given a model of the world Markov Processes ■ Markov Rewind Punesse ( MRP ) ■ Markov Decision Processes ( MDPS ) Evaluation and Control in MDPs Detected in frame_141.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPs
NANN Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPs NANN Detected in frame_142.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
N
lecture2-2
Screen Mirroring Document
Screenshot
A Focus Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS N lecture2-2 Screen Mirroring Document Screenshot A Focus Detected in frame_143.jpg: lecture
Today: Given a model of the world
• Markov Processes
Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
lecture2-2
Screen Mirroring Document
Screenshot
Focus
NNNN lecture Today : Given a model of the world • Markov Processes Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS lecture2-2 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_144.jpg: lecture!
Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
2-2
Stop Screen Mirroring Document
Screenshot
Focus
NNNN
• lecture ! Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS 2-2 Stop Screen Mirroring Document Screenshot Focus NNNN • Detected in frame_145.jpg: lecture!
Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
2-2
Screen Mirroring Document
Screenshot
Focus
NNNN lecture ! Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS 2-2 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_146.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
NNNN
Stop Screen Mirroring Document
Screenshot
Focus
·
$1 Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS NNNN Stop Screen Mirroring Document Screenshot Focus · $ 1 Detected in frame_147.jpg: Today: Given a model of the world
• Markov Processes
Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
NNNN
Screen Mirroring Document
Screenshot
Focus
· Today : Given a model of the world • Markov Processes Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS NNNN Screen Mirroring Document Screenshot Focus · Detected in frame_148.jpg: Documents
Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
lecture2-2
Screen Mirroring Document
Screenshot
Focus
NNNN Documents Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS lecture2-2 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_149.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
2-2
Screen Mirroring Document
Screenshot
Focus
NNNN Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS 2-2 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_15.jpg: Documents
Today's Plan
lecture
NNNN
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screen Mirroring Document
Screenshot
5) Роси Documents Today's Plan lecture NNNN • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screen Mirroring Document Screenshot 5 ) Роси Detected in frame_150.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
Stop Screen Mirroring Document
Screenshot
Focus
NNNN Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_151.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
lecture2-2
Stop Screen Mirroring Document
Screenshot
Focus
NNNN Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS lecture2-2 Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_152.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
2-2
Screen Mirroring Document
Screenshot
Focus
NNNN Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS 2-2 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_153.jpg: Documents
Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
Jecture2-2
Screen Mirroring Document
Screenshot
Focus
NNNN Documents Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS Jecture2-2 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_154.jpg: lecture
Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPs
Stop Screen Mirroring Document
Screenshot
Focus
NNNN lecture Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPs Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_155.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPs
Screen Mirroring Document
Screenshot
A focus
2-2
K Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPs Screen Mirroring Document Screenshot A focus 2-2 K Detected in frame_156.jpg: Documents
lecture
Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
Evaluation and Control in MDPS
lecture2-2
Stop Screen Mirroring Document
Screenshot
Focus
NNNN
· Documents lecture Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) Evaluation and Control in MDPS lecture2-2 Stop Screen Mirroring Document Screenshot Focus NNNN · Detected in frame_157.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPs
Screen Mirroring Document
Screenshot
A focus
2-2 Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPs Screen Mirroring Document Screenshot A focus 2-2 Detected in frame_158.jpg: Documents
lecture
Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
Evaluation and Control in MDPS
lecture2-2
Stop Screen Mirroring Document
Screenshot
Focus
NNNN
· Documents lecture Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) Evaluation and Control in MDPS lecture2-2 Stop Screen Mirroring Document Screenshot Focus NNNN · Detected in frame_159.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
K
Screen Mirroring Document
Screenshot
focus
NNNN Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS K Screen Mirroring Document Screenshot focus NNNN Detected in frame_16.jpg: Documents
Anx
lecture
.
securel-1
lecture2-2
Today's Plan
NNNN
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screen Mirroring Document
Screenshot
Focus Documents Anx lecture . securel - 1 lecture2-2 Today's Plan NNNN • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screen Mirroring Document Screenshot Focus Detected in frame_160.jpg: Documents
lecture
Today: Given a model of the world
lecture2-1
lecture2-2
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
Stop Screen Mirroring Document
Screenshot
A Focus
NNNN Documents lecture Today : Given a model of the world lecture2-1 lecture2-2 • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS Stop Screen Mirroring Document Screenshot A Focus NNNN Detected in frame_161.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
2-2
Screen Mirroring Document
Screenshot
focus
NNNN
" Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS 2-2 Screen Mirroring Document Screenshot focus NNNN " Detected in frame_162.jpg: Documents
lecture
Today: Given a model of the world
lecture2-1
lecture2-2
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
Stop Screen Mirroring Document
Screenshot
A Focus
NNNN Documents lecture Today : Given a model of the world lecture2-1 lecture2-2 • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS Stop Screen Mirroring Document Screenshot A Focus NNNN Detected in frame_163.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPs
2-2
Screen Mirroring Document
Screenshot
focus
NNNN Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPs 2-2 Screen Mirroring Document Screenshot focus NNNN Detected in frame_164.jpg: Jecture
Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
lecture2-2
Stop Screen Mirroring Document
Screenshot
Focus
NNNN Jecture Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS lecture2-2 Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_165.jpg: Today: Given a model of the world
Documents
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
lecture2-2
Stop Screen Mirroring Document
Screenshot
Focus
NNNN
$ 1 Today : Given a model of the world Documents • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS lecture2-2 Stop Screen Mirroring Document Screenshot Focus NNNN $ 1 Detected in frame_166.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
Screen Mirroring Document
Screenshot
Гос
2-2
IN IN IN IN
• Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS Screen Mirroring Document Screenshot Гос 2-2 IN IN IN IN • Detected in frame_167.jpg: lecture
Today: Given a model of the world
lecture2-2
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
NNNN
Stop Screen Mirroring Document
Screenshot
Focus
1 lecture Today : Given a model of the world lecture2-2 • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS NNNN Stop Screen Mirroring Document Screenshot Focus 1 Detected in frame_168.jpg: Documents
Today: Given a model of the world
lecture2-1
lecture2-2
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
Stop Screen Mirroring Document
Screenshot
Focus
NNNN
. Documents Today : Given a model of the world lecture2-1 lecture2-2 • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS Stop Screen Mirroring Document Screenshot Focus NNNN . Detected in frame_169.jpg: Today: Given a model of the world
Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
Screen Mirroring Document
Screenshot
Focu
NNNN Today : Given a model of the world Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS Screen Mirroring Document Screenshot Focu NNNN Detected in frame_17.jpg: Today's Plan
2-1
x
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screen Mining Обем
Focs
INININN Today's Plan 2-1 x • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screen Mining Обем Focs INININN Detected in frame_170.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
2-2
Screen Mirroring Document
Screenshot
Focus
NNNN Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS 2-2 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_171.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
2-2
Screen Mirroring Document
Screenshot
Focus
NNNN
. Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS 2-2 Screen Mirroring Document Screenshot Focus NNNN . Detected in frame_172.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPS)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
lecture2-2
Stop Screen Mirroring Document
Screenshot
Focus
NNNNI Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPS ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS lecture2-2 Stop Screen Mirroring Document Screenshot Focus NNNNI Detected in frame_173.jpg: Today: Given a model of the world
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
2-2
ན
NNNN
Screen Mirroring Document
Screenshot
A Focus
41 Today : Given a model of the world • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS 2-2 ན NNNN Screen Mirroring Document Screenshot A Focus 41 Detected in frame_174.jpg: Today: Given a model of the world
Documents
• Markov Processes
• Markov Reward Processes (MRPs)
• Markov Decision Processes (MDPs)
• Evaluation and Control in MDPS
NANON
Screen Mirroring Document
Screenshot
Focus
· Today : Given a model of the world Documents • Markov Processes • Markov Reward Processes ( MRPs ) • Markov Decision Processes ( MDPs ) • Evaluation and Control in MDPS NANON Screen Mirroring Document Screenshot Focus · Detected in frame_175.jpg: Full Chiservabilly Markus Deusion Press (MDP)
State
Rewardi
World
Adions
Agent
MDF can hop number of me bug problem admings
BE MOR
الممسوت . Full Chiservabilly Markus Deusion Press ( MDP ) State Rewardi World Adions Agent MDF can hop number of me bug problem admings BE MOR الممسوت . Detected in frame_176.jpg: Full Observability, Markov Decision Process (MDP)
States
Reward r
World
Agent
Action a
MDPs can model a huge number of interesting problems and settings
Bandits, single state MDP
Optimal Full Observability , Markov Decision Process ( MDP ) States Reward r World Agent Action a MDPs can model a huge number of interesting problems and settings Bandits , single state MDP Optimal Detected in frame_177.jpg: T
Full Observability: Markov Decision Process (MDP)
States,
Reward r
World
Action a
Agent
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal
D T Full Observability : Markov Decision Process ( MDP ) States , Reward r World Action a Agent • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal D Detected in frame_178.jpg: Full Observability: Markov Decision Process (MDP)
States,
Reward r
World
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
⚫ Bandits: single state MDP
• Optimal ■
Serve Full Observability : Markov Decision Process ( MDP ) States , Reward r World Agent Action a • MDPs can model a huge number of interesting problems and settings ⚫ Bandits : single state MDP • Optimal ■ Serve Detected in frame_179.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
NNUN..
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal Shap
Screen Mirroring
Screenshot focia Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a NNUN .. • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal Shap Screen Mirroring Screenshot focia Detected in frame_18.jpg: Today's Plan
lecture2-2
NNNN
.
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Stop Screen Mirroring Document
Screenshot
Focus Today's Plan lecture2-2 NNNN . • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Stop Screen Mirroring Document Screenshot Focus Detected in frame_180.jpg: lecture
lecture2-2
Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
NNNN
• MDPs can model a huge number of interesting problems and settings
⚫ Bandits: single state MDP
• Optimal shop Screen Mirroring Document Screenshot Fc 'S lecture lecture2-2 Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a NNNN • MDPs can model a huge number of interesting problems and settings ⚫ Bandits : single state MDP • Optimal shop Screen Mirroring Document Screenshot Fc ' S Detected in frame_181.jpg: Full Observability, Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits single state MDP
Optimal Full Observability , Markov Decision Process ( MDP ) State s Reward r World Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits single state MDP Optimal Detected in frame_182.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal
Screen Mon Ос
Screenshot Focus S
ININ N Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen Mon Ос Screenshot Focus S ININ N Detected in frame_183.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Action a
Agent
NNNN
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal
Screen M
Screenshot + Focus S Full Observability : Markov Decision Process ( MDP ) State s Reward r World Action a Agent NNNN • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen M Screenshot + Focus S Detected in frame_184.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
N
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
⚫ Optimal Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a N • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP ⚫ Optimal Detected in frame_185.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal St Screen M
Screenshot Foca 'S
●ININN Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal St Screen M Screenshot Foca ' S ● ININN Detected in frame_186.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal
Screen M
NININN Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen M NININN Detected in frame_187.jpg: Full Observability, Markov Decision Process (MDP)
State's,
Reward r
World
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits single state MDP
Optimal Full Observability , Markov Decision Process ( MDP ) State's , Reward r World Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits single state MDP Optimal Detected in frame_188.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
NNN
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal
Screen Ming О
Screenshot S Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a NNN • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen Ming О Screenshot S Detected in frame_189.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
⚫ Optimal shap Screen Miring D
Screenshot it focus 'S
INNINN Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP ⚫ Optimal shap Screen Miring D Screenshot it focus ' S INNINN Detected in frame_19.jpg: Today's Plan
lecture
cture-
cture2-2
NNNN
•
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
• Policy evaluation when don't have a model of how the world works
Stop
Screen Mirroring Document
Screenshot
Focus Today's Plan lecture cture- cture2-2 NNNN • • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : • Policy evaluation when don't have a model of how the world works Stop Screen Mirroring Document Screenshot Focus Detected in frame_190.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Action a
Agent
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal S
IN Full Observability : Markov Decision Process ( MDP ) State s Reward r World Action a Agent • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal S IN Detected in frame_191.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal ■
Screen Me D
Screenshot i Focus 'S
NININN Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal ■ Screen Me D Screenshot i Focus ' S NININN Detected in frame_192.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
N N N N
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal
Screen Mirroring
Screenshot Focus Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a N N N N • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen Mirroring Screenshot Focus Detected in frame_193.jpg: Full Observability: Markov Decision Process (MDP)
States,
Reward r
World
Agent
Action a
3
MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal Full Observability : Markov Decision Process ( MDP ) States , Reward r World Agent Action a 3 MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Detected in frame_194.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal
Screen M
О
ININ N Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal Screen M О ININ N Detected in frame_195.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
NNNNI
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal shap Screen Ming
Screenshot Focus Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a NNNNI • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal shap Screen Ming Screenshot Focus Detected in frame_196.jpg: Full Observability: Markov Decision Process (MDP)
State s
Reward r
World
Agent
Action a
NNNN
• MDPs can model a huge number of interesting problems and settings
⚫ Bandits: single state MDP
• Optimal■Shop
Screen Mirroring Document Screenshot Focus S Full Observability : Markov Decision Process ( MDP ) State s Reward r World Agent Action a NNNN • MDPs can model a huge number of interesting problems and settings ⚫ Bandits : single state MDP • Optimal ■ Shop Screen Mirroring Document Screenshot Focus S Detected in frame_197.jpg: Full Observability: Markov Decision Process (MDP)
State's
Reward r
World
Agent
Action a
NNNN
• MDPs can model a huge number of interesting problems and settings
⚫ Bandits: single state MDP
• Optimal■Shop
Screen Mirroring Document Screenshot Focus S Full Observability : Markov Decision Process ( MDP ) State's Reward r World Agent Action a NNNN • MDPs can model a huge number of interesting problems and settings ⚫ Bandits : single state MDP • Optimal ■ Shop Screen Mirroring Document Screenshot Focus S Detected in frame_198.jpg: Full Observability: Markov Decision Process (MDP)
ob
State's
Reward r
World
Agent
Action a
NNNN
• MDPs can model a huge number of interesting problems and settings
⚫ Bandits: single state MDP
• Optimal■Shop
Screen Mirroring Document Screenshot Focus S Full Observability : Markov Decision Process ( MDP ) ob State's Reward r World Agent Action a NNNN • MDPs can model a huge number of interesting problems and settings ⚫ Bandits : single state MDP • Optimal ■ Shop Screen Mirroring Document Screenshot Focus S Detected in frame_199.jpg: Full Observability: Markov Decision Process (MDP)
obscve
States
Reward r
World
Agent
Action a
NNNN
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal
Seven Min Full Observability : Markov Decision Process ( MDP ) obscve States Reward r World Agent Action a NNNN • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal Seven Min Detected in frame_2.jpg: Stanford ENGINEERING Stanford ENGINEERING Detected in frame_20.jpg: Today's Plan
2-1
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screen Mining
10 Росси
NININN Today's Plan 2-1 • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screen Mining 10 Росси NININN Detected in frame_200.jpg: Full Observability: Markov Decision Process (MDP)
obscruzti
World
States
Reward r
Agent
Action a
NNNN
• MDPs can model a huge number of interesting problems and settings
⚫ Bandits: single state MDP
• Optimal Sha Seeing Full Observability : Markov Decision Process ( MDP ) obscruzti World States Reward r Agent Action a NNNN • MDPs can model a huge number of interesting problems and settings ⚫ Bandits : single state MDP • Optimal Sha Seeing Detected in frame_201.jpg: Full Observability: Markov Decision Process (MDP)
obscrvation
World
State's
Reward r
Agent
Action a
3
MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal Full Observability : Markov Decision Process ( MDP ) obscrvation World State's Reward r Agent Action a 3 MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Detected in frame_202.jpg: Full Observability: Markov Decision Process (MDP)
obscruation
World
States
Reward r
⚫ Optimal
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
⚫ Bandits: single state MDP
IS Full Observability : Markov Decision Process ( MDP ) obscruation World States Reward r ⚫ Optimal Agent Action a • MDPs can model a huge number of interesting problems and settings ⚫ Bandits : single state MDP IS Detected in frame_203.jpg: Full Observability: Markov Decision Process (MDP)
obscruation
World
States
Reward r
⚫ Optimal
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
⚫ Bandits: single state MDP
IS Full Observability : Markov Decision Process ( MDP ) obscruation World States Reward r ⚫ Optimal Agent Action a • MDPs can model a huge number of interesting problems and settings ⚫ Bandits : single state MDP IS Detected in frame_204.jpg: Full Observability: Markov Decision Process (MDP)
obscrvation
World
States
Reward r
Agent
Action a
NNN.
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal■
See M
О Full Observability : Markov Decision Process ( MDP ) obscrvation World States Reward r Agent Action a NNN . • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal ■ See M О Detected in frame_205.jpg: Full Observability: Markov Decision Process (MDP)
obscruation
States
Reward r
World
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal ■Shop Screen Mirroring Document Screenshot Focus
NNNNN Full Observability : Markov Decision Process ( MDP ) obscruation States Reward r World Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal ■ Shop Screen Mirroring Document Screenshot Focus NNNNN Detected in frame_206.jpg: Full Observability: Markov Decision Process (MDP)
observation
World
States
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal
Screen Mo
IN
ININ N Full Observability : Markov Decision Process ( MDP ) observation World States Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal Screen Mo IN ININ N Detected in frame_207.jpg: Full Observability: Markov Decision Process (MDP)
observation
World
State's
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal
Screen Mo
IN Full Observability : Markov Decision Process ( MDP ) observation World State's Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal Screen Mo IN Detected in frame_208.jpg: Full Observability: Markov Decision Process (MDP)
obseruation
World
States
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal S Sereng
Screenshot [5] Focus S Full Observability : Markov Decision Process ( MDP ) obseruation World States Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal S Sereng Screenshot [ 5 ] Focus S Detected in frame_209.jpg: Full Observability: Markov Decision Process (MDP)
obscrvation
World
States
Reward r
Agent
Action a
NNNN
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal S Screen Ming D
Screenshot 11 [3] Focus Full Observability : Markov Decision Process ( MDP ) obscrvation World States Reward r Agent Action a NNNN • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal S Screen Ming D Screenshot 11 [ 3 ] Focus Detected in frame_21.jpg: Today's Plan
lecture2-2
NNNN
.
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
• Policy evaluation when don't have a model of how the world works
Stop Screen Mirroring Document
Screenshot
Focus Today's Plan lecture2-2 NNNN . • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : • Policy evaluation when don't have a model of how the world works Stop Screen Mirroring Document Screenshot Focus Detected in frame_210.jpg: см2-1
Full Observability: Markov Decision Process (MDP)
obscrvation
World
States
Reward r
Agent
Action a
N N N N
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
⚫ Optimal ■sp Screen Mirroring Document Screenshot Focus S см2-1 Full Observability : Markov Decision Process ( MDP ) obscrvation World States Reward r Agent Action a N N N N • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP ⚫ Optimal ■ sp Screen Mirroring Document Screenshot Focus S Detected in frame_211.jpg: Full Observability: Markov Decision Process (MDP)
obscrvation
World
States
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal ■
Seren Ming
NININN Full Observability : Markov Decision Process ( MDP ) obscrvation World States Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal ■ Seren Ming NININN Detected in frame_212.jpg: Full Observability: Markov Decision Process (MDP)
obscrvation
World
States
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal ■
Sereng
NININN Full Observability : Markov Decision Process ( MDP ) obscrvation World States Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal ■ Sereng NININN Detected in frame_213.jpg: Full Observability: Markov Decision Process (MDP)
observation
World
States
Reward r
Agent
Action a
NNNN
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
⚫ Optimal ■stop Screen Mirroring Document Screenshot focus's Full Observability : Markov Decision Process ( MDP ) observation World States Reward r Agent Action a NNNN • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP ⚫ Optimal ■ stop Screen Mirroring Document Screenshot focus's Detected in frame_214.jpg: Full Observability: Markov Decision Process (MDP)
obscruation
World
States
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal
Screen M Full Observability : Markov Decision Process ( MDP ) obscruation World States Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal Screen M Detected in frame_215.jpg: Full Observability: Markov Decision Process (MDP)
obscruation
World
States
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal Sp Screen Mering Рос
Screenshot i Fecia's
NININN Full Observability : Markov Decision Process ( MDP ) obscruation World States Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal Sp Screen Mering Рос Screenshot i Fecia's NININN Detected in frame_216.jpg: Full Observability: Markov Decision Process (MDP)
obscruation
World
State's
Reward r
Optimal
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
NNNNN Full Observability : Markov Decision Process ( MDP ) obscruation World State's Reward r Optimal Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP NNNNN Detected in frame_217.jpg: Full Observability: Markov Decision Process (MDP)
obscruation
World
States
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal
S
D
ININ N Full Observability : Markov Decision Process ( MDP ) obscruation World States Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal S D ININ N Detected in frame_218.jpg: Full Observability: Markov Decision Process (MDP)
obseruation
World
States
Reward r
Agent
Action a
NNNN
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal
Screen Mirroring
Screenshot if [5] Focual 'S Full Observability : Markov Decision Process ( MDP ) obseruation World States Reward r Agent Action a NNNN • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen Mirroring Screenshot if [ 5 ] Focual ' S Detected in frame_219.jpg: Full Observability: Markov Decision Process (MDP)
obscrvation
World
States
Reward r
Agent
Action a
NNUNNI
• MDPs can model a huge number of interesting problems and settings
⚫ Bandits: single state MDP
• Optimal Sha Screen Moving
Screenshot Focus Full Observability : Markov Decision Process ( MDP ) obscrvation World States Reward r Agent Action a NNUNNI • MDPs can model a huge number of interesting problems and settings ⚫ Bandits : single state MDP • Optimal Sha Screen Moving Screenshot Focus Detected in frame_22.jpg: Today's Plan
• Last Time:
lecture
lecture2-1
lecture2-2
NNNN
•
• Introduction
Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
• Policy evaluation when don't have a model of how the world works
Stop
Screen Mirroring Document
Screenshot
Focus Today's Plan • Last Time : lecture lecture2-1 lecture2-2 NNNN • • Introduction Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : • Policy evaluation when don't have a model of how the world works Stop Screen Mirroring Document Screenshot Focus Detected in frame_220.jpg: Full Observability: Markov Decision Process (MDP)
obseruation
States
Reward r
World
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal■Shop
Screen Mirroring
Screenshot Focus
INININN Full Observability : Markov Decision Process ( MDP ) obseruation States Reward r World Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal ■ Shop Screen Mirroring Screenshot Focus INININN Detected in frame_221.jpg: Full Observability: Markov Decision Process (MDP)
obscrvation
World
States
Reward r
Optimal
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Screen M
IS
NINN Full Observability : Markov Decision Process ( MDP ) obscrvation World States Reward r Optimal Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Screen M IS NINN Detected in frame_222.jpg: Full Observability: Markov Decision Process (MDP)
obscrvation
World
States
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal
Screen M
NINN Full Observability : Markov Decision Process ( MDP ) obscrvation World States Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen M NINN Detected in frame_223.jpg: Full Observability: Markov Decision Process (MDP)
obscruation
World
States
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal■Shop Screen Mirroring Дос
Screenshot Focus
INNINN Full Observability : Markov Decision Process ( MDP ) obscruation World States Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal ■ Shop Screen Mirroring Дос Screenshot Focus INNINN Detected in frame_224.jpg: Full Observability: Markov Decision Process (MDP)
obscrvation
World
States
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal
Screen Mog D
NINN Full Observability : Markov Decision Process ( MDP ) obscrvation World States Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen Mog D NINN Detected in frame_225.jpg: Full Observability: Markov Decision Process (MDP)
obscrvation
World
States
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal
Screen M
Do
NNN Full Observability : Markov Decision Process ( MDP ) obscrvation World States Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen M Do NNN Detected in frame_226.jpg: Full Observability: Markov Decision Process (MDP)
obseruation
World
State's
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal Sp Screen M
Screenshot Focus
NININN Full Observability : Markov Decision Process ( MDP ) obseruation World State's Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal Sp Screen M Screenshot Focus NININN Detected in frame_227.jpg: Full Observability: Markov Decision Process (MDP)
obscrvation
States
Reward r
World
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
⚫ Bandits: single state MDP
⚫ Optimal ■Stop Screen Mirroring Document Screenshot Fou
NNNNN Full Observability : Markov Decision Process ( MDP ) obscrvation States Reward r World Agent Action a • MDPs can model a huge number of interesting problems and settings ⚫ Bandits : single state MDP ⚫ Optimal ■ Stop Screen Mirroring Document Screenshot Fou NNNNN Detected in frame_228.jpg: Full Observability: Markov Decision Process (MDP)
obscrvation
World
States
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
Optimal
Screen Mon
Do
NNN Full Observability : Markov Decision Process ( MDP ) obscrvation World States Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP Optimal Screen Mon Do NNN Detected in frame_229.jpg: Full Observability: Markov Decision Process (MDP)
observation
World
State's
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal ■ Screening D
IS Full Observability : Markov Decision Process ( MDP ) observation World State's Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal ■ Screening D IS Detected in frame_23.jpg: Today's Plan
• Last Time:
cture
x
lecture2-1
lecture2-2
NNNN
• Introduction
Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
• Policy evaluation when don't have a model of how the world works
Stop
Screen Mirroring Document
Screenshot 68 Госм Today's Plan • Last Time : cture x lecture2-1 lecture2-2 NNNN • Introduction Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : • Policy evaluation when don't have a model of how the world works Stop Screen Mirroring Document Screenshot 68 Госм Detected in frame_230.jpg: Full Observability: Markov Decision Process (MDP)
obseruation
World
States,
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal ■
Screen M
Screenshot 11 G Focus 'S
NININN Full Observability : Markov Decision Process ( MDP ) obseruation World States , Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal ■ Screen M Screenshot 11 G Focus ' S NININN Detected in frame_231.jpg: Full Observability: Markov Decision Process (MDP)
obseruation
World
States,
Reward r
Action a
Agent
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal Sp Screen Mirroring Роси
Screenshot Focus S
NININN Full Observability : Markov Decision Process ( MDP ) obseruation World States , Reward r Action a Agent • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal Sp Screen Mirroring Роси Screenshot Focus S NININN Detected in frame_232.jpg: Full Observability; Markov Decision Process (MDP)
obscrvation
World
State's
Reward r
Agent
Action a
⚫ MDPs can model a huge number of interesting problems and settings
Bandits, single state MDP
Optimal Full Observability ; Markov Decision Process ( MDP ) obscrvation World State's Reward r Agent Action a ⚫ MDPs can model a huge number of interesting problems and settings Bandits , single state MDP Optimal Detected in frame_233.jpg: Full Observability: Markov Decision Process (MDP)
obseruation
World
States,
Reward r
Agent
Action a
• MDPs can model a huge number of interesting problems and settings
Bandits: single state MDP
• Optimal Se Screen
Сс
Screenshot 11 Focus S
NININN Full Observability : Markov Decision Process ( MDP ) obseruation World States , Reward r Agent Action a • MDPs can model a huge number of interesting problems and settings Bandits : single state MDP • Optimal Se Screen Сс Screenshot 11 Focus S NININN Detected in frame_234.jpg: السكر الداي عن المسلسل سنة السكر الداي عن المسلسل سنة Detected in frame_235.jpg: Recall Markov Property
• Information statel sufficient statistic of history
State 5, is Markov. if and only if
Future is independent of past given present Recall Markov Property • Information statel sufficient statistic of history State 5 , is Markov . if and only if Future is independent of past given present Detected in frame_236.jpg: Recall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
p(St+1|Stat) = P(St+1|h, at)
Future is independent of past given present
SM
Doc
Focu Recall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : p ( St + 1 | Stat ) = P ( St + 1 | h , at ) Future is independent of past given present SM Doc Focu Detected in frame_237.jpg: Documents
lecture
Recall: Markov Property
lecture-1
lecture2-2
Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at) = P(St+1|ht, at)
Future is independent of past given present
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN
' Documents lecture Recall : Markov Property lecture - 1 lecture2-2 Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus NNNN ' Detected in frame_238.jpg: Documents
Recall: Markov Property
-
lecture2-2
K
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at) = P(St+1|ht, at)
Future is independent of past given present
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN
. Documents Recall : Markov Property - lecture2-2 K • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus NNNN . Detected in frame_239.jpg: Recall: Markov Property
2-2
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|Stat) = P(St+1|ht, at)
Future is independent of past given present
Screen Mirroring Document
Screenshot
NNNN Recall : Markov Property 2-2 • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | Stat ) = P ( St + 1 | ht , at ) Future is independent of past given present Screen Mirroring Document Screenshot NNNN Detected in frame_24.jpg: Today's Plan
• Last Time:
lecture
x
lecture2-1
lecture2-2
NNNN
• Introduction
Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
• Policy evaluation when don't have a model of how the world works
Stop
Screen Mirroring Document
Screenshot
Focus Today's Plan • Last Time : lecture x lecture2-1 lecture2-2 NNNN • Introduction Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : • Policy evaluation when don't have a model of how the world works Stop Screen Mirroring Document Screenshot Focus Detected in frame_240.jpg: Documents
Recall: Markov Property
x
lecture2-2
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at) = P(St+1|ht, at)
Future is independent of past given present
Screen Mirroring Document
Screenshot
18 Рос
NNNN
" Documents Recall : Markov Property x lecture2-2 • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) Future is independent of past given present Screen Mirroring Document Screenshot 18 Рос NNNN " Detected in frame_241.jpg: Documents
Recall: Markov Property
x
lecture2-2
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at) = P(St+1|ht, at)
Future is independent of past given present
Stop
Screen Mirroring Document
Screenshot
50 Роси
NNNNN
" Documents Recall : Markov Property x lecture2-2 • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) Future is independent of past given present Stop Screen Mirroring Document Screenshot 50 Роси NNNNN " Detected in frame_242.jpg: Documents
Recall: Markov Property
lecture2-2
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at) = P(St+1|ht, at)
Future is independent of past given present
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN Documents Recall : Markov Property lecture2-2 • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_243.jpg: Recall: Markov Property
• Information state: sufficient statistic of history
⚫ State St is Markov if and only if:
P(S+ Stat) = P(Shar)
Future is independent of past given present
GAM
NNS
3 Recall : Markov Property • Information state : sufficient statistic of history ⚫ State St is Markov if and only if : P ( S + Stat ) = P ( Shar ) Future is independent of past given present GAM NNS 3 Detected in frame_244.jpg: Recall: Markov Property
x
lecture2-1
lecture2-2
Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|Stat) = P(St+1|ht, at)
Future is independent of past given present
NNNN
Stop Screen Mirroring Document
Screenshot
Focus
. Recall : Markov Property x lecture2-1 lecture2-2 Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | Stat ) = P ( St + 1 | ht , at ) Future is independent of past given present NNNN Stop Screen Mirroring Document Screenshot Focus . Detected in frame_245.jpg: Documents
Recall: Markov Property
lecture2-1
lecture2-2
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at) = P(St+1|ht, at)
Future is independent of past given present
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN Documents Recall : Markov Property lecture2-1 lecture2-2 • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_246.jpg: Recall: Markov Property
Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|Stat) = P(St+1|ht, at)
Future is independent of past given present
Sen Men
Doc
Foca
A
N Recall : Markov Property Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | Stat ) = P ( St + 1 | ht , at ) Future is independent of past given present Sen Men Doc Foca A N Detected in frame_247.jpg: Recall: Markov Property
lecture2-2
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at) = P(St+1|ht, at)
Future is independent of past given present
NNNN
Screen Mirroring Document
Screenshot
Focus
" Recall : Markov Property lecture2-2 • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) Future is independent of past given present NNNN Screen Mirroring Document Screenshot Focus " Detected in frame_248.jpg: Recall: Markov Property
x
lecture2-1
lecture2-2
.
Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|Stat) = P(St+1|ht, at)
Future is independent of past given present
NNNN
Stop Screen Mirroring Document
Screenshot
Focus
. Recall : Markov Property x lecture2-1 lecture2-2 . Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | Stat ) = P ( St + 1 | ht , at ) Future is independent of past given present NNNN Stop Screen Mirroring Document Screenshot Focus . Detected in frame_249.jpg: Documents
Recall: Markov Property
x
lecture2-1
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at) = P(St+1|ht, at)
Future is independent of past given present
Stop
Screen Mirroring Document
Screenshot
Focus
NNNNNN
" Documents Recall : Markov Property x lecture2-1 • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus NNNNNN " Detected in frame_25.jpg: Today's Plan
lecture-
lecture2-1
NNNN
•
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Stop Screen Mirroring Document
Screenshot
Focus Today's Plan lecture- lecture2-1 NNNN • • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Stop Screen Mirroring Document Screenshot Focus Detected in frame_250.jpg: Recall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at) = P(St+1|hat)
Future is independent of past given present
Soren Mong Ос
Screenshot
Foc
'
NININN Recall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) = P ( St + 1 | hat ) Future is independent of past given present Soren Mong Ос Screenshot Foc ' NININN Detected in frame_251.jpg: Recall: Markov Property
Documents
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at) = P(St+1|ht, at)
Future is independent of past given present
NNNN
Stop
Screen Mirroring Document
Screenshot
Focus
" Recall : Markov Property Documents • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) Future is independent of past given present NNNN Stop Screen Mirroring Document Screenshot Focus " Detected in frame_252.jpg: Recall: Markov Property
K
Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|Stat) = P(St+1|ht, at)
Future is independent of past given present
Screen Mirroring Document
Screenshot
A focus
NNNN Recall : Markov Property K Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | Stat ) = P ( St + 1 | ht , at ) Future is independent of past given present Screen Mirroring Document Screenshot A focus NNNN Detected in frame_253.jpg: Recall: Markov Property
x
lecture2-1
Jecture2-2
K
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at) = P(St+1|ht, at)
for
Future is independent of past given present
NNNN
Stop
Screen Mirroring Document
Screenshot
Focus
. Recall : Markov Property x lecture2-1 Jecture2-2 K • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) for Future is independent of past given present NNNN Stop Screen Mirroring Document Screenshot Focus . Detected in frame_254.jpg: Recall: Markov Property
x
lecture2-2
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at) = P(St+1|ht, at)
times
Future is independent of past given present
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN
" Recall : Markov Property x lecture2-2 • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) times Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus NNNN " Detected in frame_255.jpg: Recall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1 Stat) = P(St+1|ht, at)
timester
Future is independent of past given present
Screen Ming D
Screenshot
AF
NNNN
' Recall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timester Future is independent of past given present Screen Ming D Screenshot AF NNNN ' Detected in frame_256.jpg: Recall: Markov Property
*
K
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
Focus
NININN
. Recall : Markov Property * K • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot Focus NININN . Detected in frame_257.jpg: Recall: Markov Property
lecture2-1
lecture2-2
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
NNNN
Stop
Screen Mirroring Document
Screenshot
Focus
" Recall : Markov Property lecture2-1 lecture2-2 • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present NNNN Stop Screen Mirroring Document Screenshot Focus " Detected in frame_258.jpg: Recall: Markov Property
Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1 Star) = P(St+1|ht, at)
timestep
Future is independent of past given present
Song Doc
F
IN
NNNN Recall : Markov Property Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 Star ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Song Doc F IN NNNN Detected in frame_259.jpg: Recall: Markov Property
2-2
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
NNNN
Screen Mirroring Document
Screenshot
Focus
" Recall : Markov Property 2-2 • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present NNNN Screen Mirroring Document Screenshot Focus " Detected in frame_26.jpg: Today's Plan
N
NNNNI
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screening D
Screenshot Today's Plan N NNNNI • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screening D Screenshot Detected in frame_260.jpg: Documents
Recall: Markov Property
x
lecture2-2
ཎ
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1 St, at) = P(St+1|ht, at)
timestep
Future is independent of past given present
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN
. Documents Recall : Markov Property x lecture2-2 ཎ • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 St , at ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus NNNN . Detected in frame_261.jpg: lecture-
x
lecture2-1
cture2-2
Recall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1 St, at) = P(St+1|ht, at)
timestep
Future is independent of past given present
NNNN
Shop
Screen Mirroring Document
Screenshot
Focus
" lecture- x lecture2-1 cture2-2 Recall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 St , at ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present NNNN Shop Screen Mirroring Document Screenshot Focus " Detected in frame_262.jpg: Recall: Markov Property
2-2
• Information state: sufficient statistic of history
• State St is Markov if and only if:
acl
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
NNNN
Screen Mirroring Document
Screenshot
A Focus
" Recall : Markov Property 2-2 • Information state : sufficient statistic of history • State St is Markov if and only if : acl P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present NNNN Screen Mirroring Document Screenshot A Focus " Detected in frame_263.jpg: Recall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
achon
P(St+1|Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
NNNN
Screen Mirroring Document
Screenshot
Focus
" Recall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : achon P ( St + 1 | Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present NNNN Screen Mirroring Document Screenshot Focus " Detected in frame_264.jpg: Recall: Markov Property
Information state: sufficient statistic of history
• State St is Markov if and only if:
·action
P(St+1 St, at) P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
Focus
NNNN
" Recall : Markov Property Information state : sufficient statistic of history • State St is Markov if and only if : · action P ( St + 1 St , at ) P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot Focus NNNN " Detected in frame_265.jpg: Recall: Markov Property
2-2
• Information state: sufficient statistic of history
• State S, is Markov if and only if:
P(St+1|St. a)
timestep
action
P(St+1|ht, at)
Future is independent of past given present
NNNN
Screen Mirroring Document Screenshot
focus
" Recall : Markov Property 2-2 • Information state : sufficient statistic of history • State S , is Markov if and only if : P ( St + 1 | St . a ) timestep action P ( St + 1 | ht , at ) Future is independent of past given present NNNN Screen Mirroring Document Screenshot focus " Detected in frame_266.jpg: Recall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at)
timestep
achon
P(St+1|ht, at)
Future is independent of past given present
Screen Mirroring Document
Screenshot
Focus
NNNN Recall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) timestep achon P ( St + 1 | ht , at ) Future is independent of past given present Screen Mirroring Document Screenshot Focus NNNN Detected in frame_267.jpg: Recall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
·action
P(St+1 St, at) = P(St+1|ht, at)
timestep
Future is independent of past given present
NNNN
Screen Mirroring Document
Screenshot
Focus
. Recall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : · action P ( St + 1 St , at ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present NNNN Screen Mirroring Document Screenshot Focus . Detected in frame_268.jpg: Recall: Markov Property
• Information state: sufficient statistic of history
• State S, is Markov if and only if:
action
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
K
Screen Mirroring Document
Screenshot
focus
NNNN Recall : Markov Property • Information state : sufficient statistic of history • State S , is Markov if and only if : action P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present K Screen Mirroring Document Screenshot focus NNNN Detected in frame_269.jpg: 2-1
Recall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
P(St+1|St, at)
timestep
achon
کا
P(St+1|ht, at)
Future is independent of past given present
Screen Mirroring Document
Screenshot
Focus
-
NNNN
' 2-1 Recall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : P ( St + 1 | St , at ) timestep achon کا P ( St + 1 | ht , at ) Future is independent of past given present Screen Mirroring Document Screenshot Focus - NNNN ' Detected in frame_27.jpg: Today's Plan
x
8
NNNN
•
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screen Mirroring Document
Screenshot
Focus Today's Plan x 8 NNNN • • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screen Mirroring Document Screenshot Focus Detected in frame_270.jpg: Decum
Recall: Markov Property
2-1
c2-2
• Information state: sufficient statistic of history
• State St is Markov if and only if:
achon
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
S.
Screen Mirroring Document
Screenshot
A Focus
NNNN
" Decum Recall : Markov Property 2-1 c2-2 • Information state : sufficient statistic of history • State St is Markov if and only if : achon P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present S. Screen Mirroring Document Screenshot A Focus NNNN " Detected in frame_271.jpg: Recall: Markov Property
2-1
So A
• Information state: sufficient statistic of history
• State St is Markov if and only if:
achon
کا
P(St+1|St, at) = P(St+1|ht, at)
timestep
Future is independent of past given present
NNNN
Screen Mirroring Document
Screenshot
A Focus
" Recall : Markov Property 2-1 So A • Information state : sufficient statistic of history • State St is Markov if and only if : achon کا P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present NNNN Screen Mirroring Document Screenshot A Focus " Detected in frame_272.jpg: Documents
Recall: Markov Property
ct2-1
So 20
• Information state: sufficient statistic of history
• State St is Markov if and only if:
achon
کا
P(St+1|Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
Focus
NNNN
. Documents Recall : Markov Property ct2-1 So 20 • Information state : sufficient statistic of history • State St is Markov if and only if : achon کا P ( St + 1 | Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot Focus NNNN . Detected in frame_273.jpg: Decum
Recall: Markov Property
2-1
c2-2
So 80 6.
• Information state: sufficient statistic of history
• State St is Markov if and only if:
achon
کا
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
A Focus
NNNN
" Decum Recall : Markov Property 2-1 c2-2 So 80 6 . • Information state : sufficient statistic of history • State St is Markov if and only if : achon کا P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot A Focus NNNN " Detected in frame_274.jpg: Recall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
·action.
So do r. s
✓
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
F
NNNN Recall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : · action . So do r . s ✓ P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot F NNNN Detected in frame_275.jpg: Recall: Markov Property
-
NNNN
So do ro S,....
• Information state: sufficient statistic of history
• State st is Markov if and only if:
achon
کا
P(St+1|Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
FOCU
" Recall : Markov Property - NNNN So do ro S , .... • Information state : sufficient statistic of history • State st is Markov if and only if : achon کا P ( St + 1 | Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot FOCU " Detected in frame_276.jpg: Recall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
achon
کا
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
F
So do r. s,....
S
NNNN
' Recall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : achon کا P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot F So do r . s , .... S NNNN ' Detected in frame_277.jpg: Recall: Markov Property
-
NNNN
So do ro S,.....
Sy
• Information state: sufficient statistic of history
• State st is Markov if and only if:
achon
کا
P(St+1|Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
Focu
" Recall : Markov Property - NNNN So do ro S , ..... Sy • Information state : sufficient statistic of history • State st is Markov if and only if : achon کا P ( St + 1 | Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot Focu " Detected in frame_278.jpg: Documents
Recall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
achon
کا
P(St+1|St, at) = P(St+1|ht, at)
timestep
Future is independent of past given present
ecture2-2
So do ro S,.....
ST
Screen Mirroring Document
Screenshot
CA) Focus
NNNN
" Documents Recall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : achon کا P ( St + 1 | St , at ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present ecture2-2 So do ro S , ..... ST Screen Mirroring Document Screenshot CA ) Focus NNNN " Detected in frame_279.jpg: Recall: Markov Property
2-1
lecture2-2
• Information state: sufficient statistic of history
• State St is Markov if and only if:
action
کا
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
So do to S,.....
Sy
Stop
Screen Mirroring Document
Screenshot
Focus
NNNNNN
" Recall : Markov Property 2-1 lecture2-2 • Information state : sufficient statistic of history • State St is Markov if and only if : action کا P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present So do to S , ..... Sy Stop Screen Mirroring Document Screenshot Focus NNNNNN " Detected in frame_28.jpg: Today's Plan
исти
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screen Mirroring Document
Screenshot
A FOCU
INNINN Today's Plan исти • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screen Mirroring Document Screenshot A FOCU INNINN Detected in frame_280.jpg: Recall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
achon
کے
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mining D
Fo
So do r. 5,...
ST
NNNN
" Recall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : achon کے P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mining D Fo So do r . 5 , ... ST NNNN " Detected in frame_281.jpg: Recall: Markov Property
2-2
NNNN
So do r. s,....
ST
• Information state: sufficient statistic of history
• State St is Markov if and only if:
·achon
کا
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
Focu
" Recall : Markov Property 2-2 NNNN So do r . s , .... ST • Information state : sufficient statistic of history • State St is Markov if and only if : · achon کا P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot Focu " Detected in frame_282.jpg: Recall: Markov Property
2-1
-
K
NNNN
So do ro S,....
• Information state: sufficient statistic of history
• State St is Markov if and only if:
achon
کا
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
Focus
" Recall : Markov Property 2-1 - K NNNN So do ro S , .... • Information state : sufficient statistic of history • State St is Markov if and only if : achon کا P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot Focus " Detected in frame_283.jpg: 2-2
K
Recall: Markov Property
• Information state: sufficient statistic of history
NNNNI
So do ro S,....
ST
• State St is Markov if and only if:
·action
کا
P(St+1|St, at)
timestep
P(St+1|ht, at)
Future is independent of past given present
Screen Mirroring Document Screenshot
Focu
" 2-2 K Recall : Markov Property • Information state : sufficient statistic of history NNNNI So do ro S , .... ST • State St is Markov if and only if : · action کا P ( St + 1 | St , at ) timestep P ( St + 1 | ht , at ) Future is independent of past given present Screen Mirroring Document Screenshot Focu " Detected in frame_284.jpg: Documents
Recall: Markov Property
lecture2-2
NNNN
So do r. s,....
Sy
Information state: sufficient statistic of history
• State St is Markov if and only if:
·action.
کا
P(St+1 St, at) = P(St+1|ht, at)
timestep
Future is independent of past given present
Stop
Screen Mirroring Document
Screenshot
Focus
" Documents Recall : Markov Property lecture2-2 NNNN So do r . s , .... Sy Information state : sufficient statistic of history • State St is Markov if and only if : · action . کا P ( St + 1 St , at ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus " Detected in frame_285.jpg: ...
2-2
K
Recall: Markov Property
So do r. s,....
• Information state: sufficient statistic of history
• State St is Markov if and only if:
·achon
کا
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
focus
ST
NNNN ... 2-2 K Recall : Markov Property So do r . s , .... • Information state : sufficient statistic of history • State St is Markov if and only if : · achon کا P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot focus ST NNNN Detected in frame_286.jpg: Recall: Markov Property
-
NNN
So do r. s,....
ST
• Information state: sufficient statistic of history
• State St is Markov if and only if:
achon
کا
P(St+1 St, at) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
Focus
" Recall : Markov Property - NNN So do r . s , .... ST • Information state : sufficient statistic of history • State St is Markov if and only if : achon کا P ( St + 1 St , at ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot Focus " Detected in frame_287.jpg: Documents
Recall: Markov Property
So do to S,.....
Information state: sufficient statistic of history
• State St is Markov if and only if:
achon
کا
P(St+1 St, at) = P(St+1|ht, at)
timestep
Future is independent of past given present
Stop
Screen Mirroring Document
Screenshot
Focus
Sy
NNN Documents Recall : Markov Property So do to S , ..... Information state : sufficient statistic of history • State St is Markov if and only if : achon کا P ( St + 1 St , at ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Stop Screen Mirroring Document Screenshot Focus Sy NNN Detected in frame_288.jpg: Recall: Markov Property
2-1
...
2
NNNN
So do r. s,....
ST
• Information state: sufficient statistic of history
• State St is Markov if and only if:
·achon
کا
P(St+1 Stat) = P(St+1|ht, at)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
fo
" Recall : Markov Property 2-1 ... 2 NNNN So do r . s , .... ST • Information state : sufficient statistic of history • State St is Markov if and only if : · achon کا P ( St + 1 Stat ) = P ( St + 1 | ht , at ) timestep Future is independent of past given present Screen Mirroring Document Screenshot fo " Detected in frame_289.jpg: ecall: Markov Property
• Information state: sufficient statistic of history
• State St is Markov if and only if:
achon
کا
So do r. s,....
Sy
P(St+1 St. a) = P(St+1| heat)
timestep
Future is independent of past given present
Screen Mirroring Document
Screenshot
Роси
NNNN ecall : Markov Property • Information state : sufficient statistic of history • State St is Markov if and only if : achon کا So do r . s , .... Sy P ( St + 1 St. a ) = P ( St + 1 | heat ) timestep Future is independent of past given present Screen Mirroring Document Screenshot Роси NNNN Detected in frame_29.jpg: Today's Plan
lecture
2-
NNNN
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Stop Screen Mirroring Document
Screenshot
Focus Today's Plan lecture 2- NNNN • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Stop Screen Mirroring Document Screenshot Focus Detected in frame_290.jpg: Mirhen Pieces в м wkew Смин
Mamuryless gundam process
■Dilimtion of Maikov Prou
No
او - عائد عامر صايعي يولد له سستسمن مسروق و
wais, no actions
If a number (W) of stats. Cress
P(A) PA)
PSN)
Plaja Pale
PISMAY
(PEN) PLN)
PISNIS Mirhen Pieces в м wkew Смин Mamuryless gundam process ■ Dilimtion of Maikov Prou No او - عائد عامر صايعي يولد له سستسمن مسروق و wais , no actions If a number ( W ) of stats . Cress P ( A ) PA ) PSN ) Plaja Pale PISMAY ( PEN ) PLN ) PISNIS Detected in frame_291.jpg: Markov Process or Markov Chain
Memoryless random process
• Sequence of random states with Markov property
• Definition of Markov Process
5.15 a (finite) set of states (s = 5)
=
P is dynamics/transition model that specifices plsss = 5)
Note no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(55) P(25)
P(SNS)
P(2) P(522)
P(5N $2)
P=
P(51 SN) P(SSN)
P(SNISN) Markov Process or Markov Chain Memoryless random process • Sequence of random states with Markov property • Definition of Markov Process 5.15 a ( finite ) set of states ( s = 5 ) = P is dynamics / transition model that specifices plsss = 5 ) Note no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( 55 ) P ( 25 ) P ( SNS ) P ( 2 ) P ( 522 ) P ( 5N $ 2 ) P = P ( 51 SN ) P ( SSN ) P ( SNISN ) Detected in frame_292.jpg: Markov Process or Markov Chain
.
Memoryless random process
⚫ Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
⚫ If finite number (N) of states, can express P as a matrix
P(51|$1)
P($251)
P(SNS)\
P($1|$2)
P($2 $2)
P(SN $2)
P =
P(S1 SN) P(52 SN)
P(SN SN)
D
XF Markov Process or Markov Chain . Memoryless random process ⚫ Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions ⚫ If finite number ( N ) of states , can express P as a matrix P ( 51 | $ 1 ) P ( $ 251 ) P ( SNS ) \ P ( $ 1 | $ 2 ) P ( $ 2 $ 2 ) P ( SN $ 2 ) P = P ( S1 SN ) P ( 52 SN ) P ( SN SN ) D XF Detected in frame_293.jpg: Markov Process or Markov Chain
Memoryless random process
⚫Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
•P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S|$1)
P(52|51)
P(SN 51)\
P(51 $2)
P($2 $2)
P(SN $2)
P =
P(S1 SN) P(52 SN)
P(SN SN)
S
D
Sorshot
For
IS
NIN N Markov Process or Markov Chain Memoryless random process ⚫Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S | $ 1 ) P ( 52 | 51 ) P ( SN 51 ) \ P ( 51 $ 2 ) P ( $ 2 $ 2 ) P ( SN $ 2 ) P = P ( S1 SN ) P ( 52 SN ) P ( SN SN ) S D Sorshot For IS NIN N Detected in frame_294.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
⚫P is dynamics/transition model that specifices p(5+1 = s' st = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S151) P($251)
P(SN 51)\
P(5152) P(5252)
P(SN $2)
P =
P(51 SN) P(52 SN)
P(SN SN)
150 Р
N Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S151 ) P ( $ 251 ) P ( SN 51 ) \ P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( 51 SN ) P ( 52 SN ) P ( SN SN ) 150 Р N Detected in frame_295.jpg: Markov Process or Markov Chain
• Memoryless random process
⚫Sequence of random states with Markov property
Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(515) P(525)
P(SNS)\
P(552) P(5252)
P(SN $2)
P=
P(SSN) P(S2SN)
P(SN SN))
ININ N Markov Process or Markov Chain • Memoryless random process ⚫Sequence of random states with Markov property Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( 515 ) P ( 525 ) P ( SNS ) \ P ( 552 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) ) ININ N Detected in frame_296.jpg: Markov Process or Markov Chain
-
Memoryless random process
Sequence of random states with Markov property
⚫ Definition of Markov Process
⚫S is a (finite) set of states (SES)
=
•P is dynamics/transition model that specifices p(5.5' = 5)
Note no rewards, no actions
• If finite number (N) of states, can express P as a matrix
P(5) P(525)
P(SNS)
P(552) P(5252)
P(SN (52)
P =
P(S|SN) P(S|SN)
P(SN SN) Markov Process or Markov Chain - Memoryless random process Sequence of random states with Markov property ⚫ Definition of Markov Process ⚫S is a ( finite ) set of states ( SES ) = • P is dynamics / transition model that specifices p ( 5.5 ' = 5 ) Note no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( 5 ) P ( 525 ) P ( SNS ) P ( 552 ) P ( 5252 ) P ( SN ( 52 ) P = P ( S | SN ) P ( S | SN ) P ( SN SN ) Detected in frame_297.jpg: Markov Process or Markov Chain
Memoryless random process
• Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
•P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S|S1) P(525)
P(SNS)\
P(12) P(252)
444
P(SN $2)
P =
P(S|SN) P(S2SN)
P(SN SN)
Scot Markov Process or Markov Chain Memoryless random process • Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S | S1 ) P ( 525 ) P ( SNS ) \ P ( 12 ) P ( 252 ) 444 P ( SN $ 2 ) P = P ( S | SN ) P ( S2SN ) P ( SN SN ) Scot Detected in frame_298.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
⚫P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S|S1)
P(51 $2)
P(251)
P(SN 51)\
P(52 $2)
P(SN $2)
P =
P(S1 SN) P(52 SN)
P(SN SN)
S
For Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S | S1 ) P ( 51 $ 2 ) P ( 251 ) P ( SN 51 ) \ P ( 52 $ 2 ) P ( SN $ 2 ) P = P ( S1 SN ) P ( 52 SN ) P ( SN SN ) S For Detected in frame_299.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(55)
P(5252)
P(SNS)
P(51 52)
P(5252)
P(SN $2)
P =
P(SSN) P(S2SN)
P(SN SN)
S
NNNNN
3 Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( 55 ) P ( 5252 ) P ( SNS ) P ( 51 52 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) S NNNNN 3 Detected in frame_3.jpg: Stanford ENGINEERING Stanford ENGINEERING Detected in frame_30.jpg: Today's Plan
-
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
⚫ Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Se
Decant
Fo
IN Today's Plan - • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : ⚫ Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Se Decant Fo IN Detected in frame_300.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
⚫ Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
•P is dynamics/transition model that specifices p(5.5' | S = 5)
• Note no rewards, no actions
=
• If finite number (N) of states; can express P as a matrix
P(5) P(5)
P(SNS)
P(552) P(5252)
P(SN $2)
P=
P(SSN) P(S2SN)
P(SN SN) Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property ⚫ Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) • P is dynamics / transition model that specifices p ( 5.5 ' | S = 5 ) • Note no rewards , no actions = • If finite number ( N ) of states ; can express P as a matrix P ( 5 ) P ( 5 ) P ( SNS ) P ( 552 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) Detected in frame_301.jpg: Markov Process or Markov Chain
Memoryless random process
• Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(151) P($251)
P($152) P(5252)
P =
P(S15N) P(52 5N)
Sc
TIT
P(SNS)\
P(SN $2)
P(SN SN)
NAJN Markov Process or Markov Chain Memoryless random process • Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( 151 ) P ( $ 251 ) P ( $ 152 ) P ( 5252 ) P = P ( S15N ) P ( 52 5N ) Sc TIT P ( SNS ) \ P ( SN $ 2 ) P ( SN SN ) NAJN Detected in frame_302.jpg: Markov Process or Markov Chain
9
www.
• Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
•P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P($1 $1) P($251)
P(SNS)\
P(51 $2) P(5252)
P(SN $2)
P =
P(SSN) P(S2SN)
P(SN SN)
Screening
Schot
From
NINN Markov Process or Markov Chain 9 www . • Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( $ 1 $ 1 ) P ( $ 251 ) P ( SNS ) \ P ( 51 $ 2 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) Screening Schot From NINN Detected in frame_303.jpg: T
Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P($151)
P($152) P(52 $2)
P($251)
P(SNS)\
P(SN $2)
P =
P(S1 SN) P(52 SN)
P(SN SN)
S
F
NINN T Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( $ 151 ) P ( $ 152 ) P ( 52 $ 2 ) P ( $ 251 ) P ( SNS ) \ P ( SN $ 2 ) P = P ( S1 SN ) P ( 52 SN ) P ( SN SN ) S F NINN Detected in frame_304.jpg: Markov Process or Markov Chain
-
NANO
N
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
•P is dynamics/transition model that specifices p(5+1 = s' st = 5)
• Note: no rewards, no actions
• If finite number (N) of states, can express P as a matrix
(P(515) P(525)
P(SNS)\
P($1 $2)
P(5252)
P(SN $2)
P=
P(SSN) P(S2SN)
P(SN SN)/
Do
15230 Ризи Markov Process or Markov Chain - NANO N Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) • Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix ( P ( 515 ) P ( 525 ) P ( SNS ) \ P ( $ 1 $ 2 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) / Do 15230 Ризи Detected in frame_305.jpg: Markov Process or Markov Chain
NAA
Memoryless random process
• Sequence of random states with Markov property
•Definition of Markov Process
⚫S is a (finite) set of states (SES)
•P is dynamics/transition model that specifices p(5+1 = 5 | 5 = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(515) P(5251)
P(SNS)\
P(5152) P(5252)
P(SN $2)
P=
P(SSN) P(SSN)
P(SN SN)
3 Markov Process or Markov Chain NAA Memoryless random process • Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( SES ) • P is dynamics / transition model that specifices p ( 5 + 1 = 5 | 5 = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( 515 ) P ( 5251 ) P ( SNS ) \ P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( SSN ) P ( SN SN ) 3 Detected in frame_306.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
• P is dynamics/transition model that specifices p(5+1 =s' st = 5)
• Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P($1 $1)
P($251)
P(51 $2)
P(5252)
P =
P(SNS)\
P(SN|$2)
P(SN SN)/
Shot
From
P(515N) P(52SN)
Sw
Do
NINN Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) • P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) • Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( $ 1 $ 1 ) P ( $ 251 ) P ( 51 $ 2 ) P ( 5252 ) P = P ( SNS ) \ P ( SN | $ 2 ) P ( SN SN ) / Shot From P ( 515N ) P ( 52SN ) Sw Do NINN Detected in frame_307.jpg: T
Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
• P is dynamics/transition model that specifices p(5+1 =s' st = 5)
• Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P($1 $1)
P($251)
P(SNS)\
P(51 $2)
P(5252)
P(SN|$2)
P =
P(S1 SN) P(52 SN)
P(SN SN)
S
From
NINN T Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) • P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) • Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( $ 1 $ 1 ) P ( $ 251 ) P ( SNS ) \ P ( 51 $ 2 ) P ( 5252 ) P ( SN | $ 2 ) P = P ( S1 SN ) P ( 52 SN ) P ( SN SN ) S From NINN Detected in frame_308.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
• P is dynamics/transition model that specifices p(5+1 =s' st = 5)
• Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P(5151)
P($251)
P(SNS)\
P(51 $2)
P(5252)
P(SN|$2)
P =
P(515N) P(52 SN)
P(SN SN)
S
D
Foca
NINN Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) • P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) • Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( 5151 ) P ( $ 251 ) P ( SNS ) \ P ( 51 $ 2 ) P ( 5252 ) P ( SN | $ 2 ) P = P ( 515N ) P ( 52 SN ) P ( SN SN ) S D Foca NINN Detected in frame_309.jpg: Markov Process or Markov Chain
2-1
-
Memoryless random process
• Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
•P is dynamics/transition model that specifices p(5+1 = 5' st = 5)
Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P(ss) P(251)
P(SNS)\
P(51 $2)
P(5252)
P(SN $2)
P =
P(SSN) P(S2SN)
P(SN SN)
бо
Вос
Sc
120 то
NNN
I Markov Process or Markov Chain 2-1 - Memoryless random process • Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' st = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( ss ) P ( 251 ) P ( SNS ) \ P ( 51 $ 2 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) бо Вос Sc 120 то NNN I Detected in frame_31.jpg: Today's Plan
lecture
2-
NNNN
•
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
• Policy evaluation when don't have a model of how the world works
Stop Screen Mirroring Document
Screenshot
Focus Today's Plan lecture 2- NNNN • • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : • Policy evaluation when don't have a model of how the world works Stop Screen Mirroring Document Screenshot Focus Detected in frame_310.jpg: Markov Process or Markov Chain
.
Memoryless random process
⚫ Sequence of random states with Markov property
Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
• P is dynamics/transition model that specifices p(5+1 = 5 | 5 = 5)
Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P(515) P(5251)
P(SNS)\
P(5152) P(5252)
P(SN $2)
P =
P(SSN) P(52 SN)
P(SN SN))
3 Markov Process or Markov Chain . Memoryless random process ⚫ Sequence of random states with Markov property Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) • P is dynamics / transition model that specifices p ( 5 + 1 = 5 | 5 = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( 515 ) P ( 5251 ) P ( SNS ) \ P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( 52 SN ) P ( SN SN ) ) 3 Detected in frame_311.jpg: T
Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s € S)
⚫P is dynamics/transition model that specifices p(5+1 =s' st = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(51 $1)
P(152)
P(251)
P(SN 51)\
P($2 $2)
P(SN $2)
P =
P(51 SN) P(52 SN)
P(SN SN)
10 Росси
XXX.
" T Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s € S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( 51 $ 1 ) P ( 152 ) P ( 251 ) P ( SN 51 ) \ P ( $ 2 $ 2 ) P ( SN $ 2 ) P = P ( 51 SN ) P ( 52 SN ) P ( SN SN ) 10 Росси XXX . " Detected in frame_312.jpg: Markov Process or Markov Chain
Memoryless random process
• Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
==
•P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S|si) P(5252)
P(SNISL)\
P(552) P(5252)
P(SN $2)
P=
P(S|SN) P(S2SN)
P(SN SN))
3 Markov Process or Markov Chain Memoryless random process • Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) == • P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S | si ) P ( 5252 ) P ( SNISL ) \ P ( 552 ) P ( 5252 ) P ( SN $ 2 ) P = P ( S | SN ) P ( S2SN ) P ( SN SN ) ) 3 Detected in frame_313.jpg: Markov Process or Markov Chain
Memoryless random process
• Sequence of random states with Markov property
Definition of Markov Process
⚫S is a (finite) set of states (s. 5)
=
•P is dynamics/transition model that specifices p(5.55 = 5)
Note: no rewards, no actions
• If finite number (N) of states, can express P as a matrix
P(55) P(525)
P(552) P(5252)
THE
P(SNS)
P(SN $2)
P =
(P(SSN) P(S2SN)
P(SN SN) Markov Process or Markov Chain Memoryless random process • Sequence of random states with Markov property Definition of Markov Process ⚫S is a ( finite ) set of states ( s . 5 ) = • P is dynamics / transition model that specifices p ( 5.55 = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( 55 ) P ( 525 ) P ( 552 ) P ( 5252 ) THE P ( SNS ) P ( SN $ 2 ) P = ( P ( SSN ) P ( S2SN ) P ( SN SN ) Detected in frame_314.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = s' st = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(151) P(52 $1)
P(SNS)\
P(51 $2)
P(5252)
P(SN $2)
P =
P(SSN) P(S2SN)
P(SN SN)
120 Рос
N Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( 151 ) P ( 52 $ 1 ) P ( SNS ) \ P ( 51 $ 2 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) 120 Рос N Detected in frame_315.jpg: T
Markov Process or Markov Chain
Memoryless random process
⚫ Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
• P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S|S1)
P($251)
P(SNS)\
P(51 $2)
P(5252)
P(SN $2)
P =
P(515N) P(52 SN)
P(SN SN)
S
10 Рос
NNNN T Markov Process or Markov Chain Memoryless random process ⚫ Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S | S1 ) P ( $ 251 ) P ( SNS ) \ P ( 51 $ 2 ) P ( 5252 ) P ( SN $ 2 ) P = P ( 515N ) P ( 52 SN ) P ( SN SN ) S 10 Рос NNNN Detected in frame_316.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫ S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
Note: no rewards, no actions
⚫ If finite number (N) of states, can express P as a matrix
P(55)
P(5252)
P(SNS)
P(552) P(5252)
P(SN $2)
P=
P(S|SN) P(S2SN)
P(SN SN)
15
IN INN Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫ S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note : no rewards , no actions ⚫ If finite number ( N ) of states , can express P as a matrix P ( 55 ) P ( 5252 ) P ( SNS ) P ( 552 ) P ( 5252 ) P ( SN $ 2 ) P = P ( S | SN ) P ( S2SN ) P ( SN SN ) 15 IN INN Detected in frame_317.jpg: Markov Process or Markov Chain
Memoryless random process
• Sequence of random states with Markov property
⚫ Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
•P is dynamics/transition model that specifices p(5.15 | 5 = 5)
Note no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(55) P(525)
P(SNS)
P(5152) P(5252)
P(SN $2)
P=
P(SSN) P(S2SN)
P(SN SN) Markov Process or Markov Chain Memoryless random process • Sequence of random states with Markov property ⚫ Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) • P is dynamics / transition model that specifices p ( 5.15 | 5 = 5 ) Note no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( 55 ) P ( 525 ) P ( SNS ) P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) Detected in frame_318.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
Definition of Markov Process
⚫S is a (finite) set of states (5 € S)
⚫P is dynamics/transition model that specifices p(541 = 5' 5 = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(5151) P(5251)
P(SNS)
P(51 $2) P(5252)
P(SN $2)
P =
P(SSN) P(S2SN)
P(SN SN))
NINN
3 Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property Definition of Markov Process ⚫S is a ( finite ) set of states ( 5 € S ) ⚫P is dynamics / transition model that specifices p ( 541 = 5 ' 5 = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( 5151 ) P ( 5251 ) P ( SNS ) P ( 51 $ 2 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) ) NINN 3 Detected in frame_319.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
•P is dynamics/transition model that specifices p(St+1 = 5' 5 = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(ss) P(525)
P(SNS)\
P(5152) P(5252)
P(SN $2)
P=
P(S|SN) P(S2 SN)
P(SN SN)
120 Ро Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( St + 1 = 5 ' 5 = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( ss ) P ( 525 ) P ( SNS ) \ P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( S | SN ) P ( S2 SN ) P ( SN SN ) 120 Ро Detected in frame_32.jpg: Today's Plan
2-1
NNNNI
·
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screen Mirroring Document
Screenshot
Focus Today's Plan 2-1 NNNNI · • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screen Mirroring Document Screenshot Focus Detected in frame_320.jpg: Markov Process or Markov Chain
A-A
• Memoryless random process
⚫ Sequence of random states with Markov property
Definition of Markov Process
⚫S is a (finite) set of states (5 € 5)
• P is dynamics/transition model that specifices p(5+1 = 5 | 5 = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(515) P(5251)
P(SNS)\
P(5152) P(5252)
P(SN $2)
P =
P(SSN) P(52 SN)
P(SN SN) Markov Process or Markov Chain A - A • Memoryless random process ⚫ Sequence of random states with Markov property Definition of Markov Process ⚫S is a ( finite ) set of states ( 5 € 5 ) • P is dynamics / transition model that specifices p ( 5 + 1 = 5 | 5 = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( 515 ) P ( 5251 ) P ( SNS ) \ P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( 52 SN ) P ( SN SN ) Detected in frame_321.jpg: T
Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
⚫P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S151) P($251)
P(51 $2) P(5252)
P(SN 51)\
P(SN|52)
P =
P(S1 SN) P(52 SN)
P(SN SN)
20 Рос
RNS T Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S151 ) P ( $ 251 ) P ( 51 $ 2 ) P ( 5252 ) P ( SN 51 ) \ P ( SN | 52 ) P = P ( S1 SN ) P ( 52 SN ) P ( SN SN ) 20 Рос RNS Detected in frame_322.jpg: T
Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
⚫P is dynamics/transition model that specifices p(5+1 = s' st = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S|$1) P($251)
P(SN Si)\
P(5152) P(5252)
P(SN $2)
P =
P(515N) P(52 SN)
P(SN SN)
Book
S
Fo
RNS T Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S | $ 1 ) P ( $ 251 ) P ( SN Si ) \ P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( 515N ) P ( 52 SN ) P ( SN SN ) Book S Fo RNS Detected in frame_323.jpg: T
Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
⚫P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S151) P($251)
P(51 $2)
P($2 $2)
P(SN 51)\
P(SN|52)
P =
P(515N) P(52 SN)
P(SN SN)
S
Foc
RNS T Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S151 ) P ( $ 251 ) P ( 51 $ 2 ) P ( $ 2 $ 2 ) P ( SN 51 ) \ P ( SN | 52 ) P = P ( 515N ) P ( 52 SN ) P ( SN SN ) S Foc RNS Detected in frame_324.jpg: Markov Process or Markov Chain
Memoryless random process
⚫ Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
•P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
• Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P(S|51)
P(5251)
P(SNS)
P($1 $2)
P($252)
P(SN $2)
P=
P(S|SN) P(S2SN)
P(SN SN))
10 Р
San M
'
IN
NANO Markov Process or Markov Chain Memoryless random process ⚫ Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) • Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( S | 51 ) P ( 5251 ) P ( SNS ) P ( $ 1 $ 2 ) P ( $ 252 ) P ( SN $ 2 ) P = P ( S | SN ) P ( S2SN ) P ( SN SN ) ) 10 Р San M ' IN NANO Detected in frame_325.jpg: Markov Process or Markov Chain
• Memoryless random process
• Sequence of random states with Markov property
•Definition of Markov Process
⚫S is a (finite) set of states (5 € 5)
• P is dynamics/transition model that specifices p(5+) =s's, = 5)
• Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P(S|S) P(525)
P(SNS)
P(5152) P(5252)
P(SN $2)
P =
P(SSN) P(52 SN)
P(SN SN)
3 Markov Process or Markov Chain • Memoryless random process • Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( 5 € 5 ) • P is dynamics / transition model that specifices p ( 5+ ) = s's , = 5 ) • Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( S | S ) P ( 525 ) P ( SNS ) P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( 52 SN ) P ( SN SN ) 3 Detected in frame_326.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P($1 $1)
P($251)
P(SNS)\
P(51 $2)
P(5252)
P(SN $2)
P =
P(S1 5N) P(52 SN)
P(SN SN)
Son Mwing D
10 Р
NNN Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( $ 1 $ 1 ) P ( $ 251 ) P ( SNS ) \ P ( 51 $ 2 ) P ( 5252 ) P ( SN $ 2 ) P = P ( S1 5N ) P ( 52 SN ) P ( SN SN ) Son Mwing D 10 Р NNN Detected in frame_327.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P(515) P(525)
P(SNISL)
P(5152) P(5252)
P(SN $2)
P=
P(SSN) P(S2SN)
P(SN SN)/
NIN N
3 Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( 515 ) P ( 525 ) P ( SNISL ) P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) / NIN N 3 Detected in frame_328.jpg: Markov Process or Markov Chain
• Memoryless random process
• Sequence of random states with Markov property
•Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
•P is dynamics/transition model that specifices p(5+) =s's, = 5)
• Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P(ss) P(5251)
P(SNS)
P(5152) P(5252)
P(SN $2)
P =
P(SSN) P(52 SN)
P(SN SN)
3 Markov Process or Markov Chain • Memoryless random process • Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) • P is dynamics / transition model that specifices p ( 5+ ) = s's , = 5 ) • Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( ss ) P ( 5251 ) P ( SNS ) P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( 52 SN ) P ( SN SN ) 3 Detected in frame_329.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫ P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P($1 $1)
P($251)
P(SN 51)
P(5152)
P(5252)
P(SN $2)
P =
P(S1 SN) P(52 SN)
P(SN SN)
(20 Ро Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫ P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( $ 1 $ 1 ) P ( $ 251 ) P ( SN 51 ) P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( S1 SN ) P ( 52 SN ) P ( SN SN ) ( 20 Ро Detected in frame_33.jpg: Today's Plan
lecture
x
lecture2-1
2-2
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫Policy evaluation when don't have a model of how the world works
Stop Screen Mirroring Document
Screenshot
Focus
NNNNNN Today's Plan lecture x lecture2-1 2-2 • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫Policy evaluation when don't have a model of how the world works Stop Screen Mirroring Document Screenshot Focus NNNNNN Detected in frame_330.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫ P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P(S1 $1)
P(5251)
P(SNS)\
P(5152)
P(5252)
P(SN $2)
P =
P(51 SN) P(52 SN)
P(SN SN)
120 Рос
N Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫ P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( S1 $ 1 ) P ( 5251 ) P ( SNS ) \ P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( 51 SN ) P ( 52 SN ) P ( SN SN ) 120 Рос N Detected in frame_331.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫ P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P($151) P($251)
P(SN 51)
P($152) P(5252)
P(SN $2)
P =
P(S1 SN) P(52 SN)
P(SN SN)
120 Рос
ININN
N Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫ P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( $ 151 ) P ( $ 251 ) P ( SN 51 ) P ( $ 152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( S1 SN ) P ( 52 SN ) P ( SN SN ) 120 Рос ININN N Detected in frame_332.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S|5i) P(5252)
P(SNISL)\
P($152) P(5252)
P(SN $2)
P=
P(SSN) P(S2SN)
P(SN SN)
3 Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S | 5i ) P ( 5252 ) P ( SNISL ) \ P ( $ 152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) 3 Detected in frame_333.jpg: Markov Process or Markov Chain
Memoryless random process
• Sequence of random states with Markov property
Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
⚫P is dynamics/transition model that specifices p(5+1 = 5 | 5 = 5)
Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
(P(S|$1)
P(5251)
P(SNS)\
P(5152) P(5252)
P(SN|52)
P=
P(SSN) P(52 SN)
P(SN SN))
NNNN
3 Markov Process or Markov Chain Memoryless random process • Sequence of random states with Markov property Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = 5 | 5 = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix ( P ( S | $ 1 ) P ( 5251 ) P ( SNS ) \ P ( 5152 ) P ( 5252 ) P ( SN | 52 ) P = P ( SSN ) P ( 52 SN ) P ( SN SN ) ) NNNN 3 Detected in frame_334.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
• P is dynamics/transition model that specifices p(5+1 =s' st = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P($1 $1)
P($251)
P(SN 51)\
P(5152)
P($2 $2)
P(SN $2)
P =
P(515N) P(52 SN)
P(SN SN),
S
IS Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( $ 1 $ 1 ) P ( $ 251 ) P ( SN 51 ) \ P ( 5152 ) P ( $ 2 $ 2 ) P ( SN $ 2 ) P = P ( 515N ) P ( 52 SN ) P ( SN SN ) , S IS Detected in frame_335.jpg: Markov Process or Markov Chain
Memoryless random process
⚫Sequence of random states with Markov property
• Definition of Markov Process
⚫ S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 =s' st = 5)
Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P($1 $1) P(52 51)
P(SN $1)\
P(51 $2) P($2 $2)
P(SN $2)
P =
P(SSN) P(52SN)
P(SN SN)
NNNN Markov Process or Markov Chain Memoryless random process ⚫Sequence of random states with Markov property • Definition of Markov Process ⚫ S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( $ 1 $ 1 ) P ( 52 51 ) P ( SN $ 1 ) \ P ( 51 $ 2 ) P ( $ 2 $ 2 ) P ( SN $ 2 ) P = P ( SSN ) P ( 52SN ) P ( SN SN ) NNNN Detected in frame_336.jpg: Markov Process or Markov Chain
Memoryless random process
⚫Sequence of random states with Markov property
• Definition of Markov Process
⚫ S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 =s' st = 5)
Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P($1 $1)
P($251)
P(SN $1)\
P($152) P(5252)
P(SN $2)
P =
P(SSN) P(52SN)
P(SN SN)
NNNN Markov Process or Markov Chain Memoryless random process ⚫Sequence of random states with Markov property • Definition of Markov Process ⚫ S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( $ 1 $ 1 ) P ( $ 251 ) P ( SN $ 1 ) \ P ( $ 152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( 52SN ) P ( SN SN ) NNNN Detected in frame_337.jpg: Markov Process or Markov Chain
x
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P(S|si) P(5251)
P(SNS)
P(552) P(5252)
P(SN $2)
P=
P(SSN) P(52 SN)
P(SN SN))
3 Markov Process or Markov Chain x Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( S | si ) P ( 5251 ) P ( SNS ) P ( 552 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( 52 SN ) P ( SN SN ) ) 3 Detected in frame_338.jpg: Markov Process or Markov Chain
• Memoryless random process
Sequence of random states with Markov property
Definition of Markov Process
⚫S is a (finite) set of states (SES)
•P is dynamics/transition model that specifices p(5.55 = 5)
• Note: no rewards, no actions
=
If finite number (N) of states, can express P as a matrix
P(S) P(5)
P(SNS)
P(552) P(5252)
P(SN (52)
P =
P(SSN) P(S2SN)
P(SN SN) Markov Process or Markov Chain • Memoryless random process Sequence of random states with Markov property Definition of Markov Process ⚫S is a ( finite ) set of states ( SES ) • P is dynamics / transition model that specifices p ( 5.55 = 5 ) • Note : no rewards , no actions = If finite number ( N ) of states , can express P as a matrix P ( S ) P ( 5 ) P ( SNS ) P ( 552 ) P ( 5252 ) P ( SN ( 52 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) Detected in frame_339.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
•P is dynamics/transition model that specifices p(5+1 =s' st = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S|S) P(52|51)
P(SNS)\
P($1|$2) P(5252)
P(SN $2)
P =
P(S1 SN) P(52 5N)
P(SN SN)
N Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S | S ) P ( 52 | 51 ) P ( SNS ) \ P ( $ 1 | $ 2 ) P ( 5252 ) P ( SN $ 2 ) P = P ( S1 SN ) P ( 52 5N ) P ( SN SN ) N Detected in frame_34.jpg: Today's Plan
3
• Last Time:
Introduction
Components of an agent: model, value, policy
• This Time:
⚫ Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works Today's Plan 3 • Last Time : Introduction Components of an agent : model , value , policy • This Time : ⚫ Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Detected in frame_340.jpg: T
Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
•If finite number (N) of states, can express P as a matrix
P(S|S1) P($251)
P(SN 51)\
P($152) P(5252)
P(SN $2)
P =
P(515N) P(52 SN)
P(SN SN)
Fa T Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( S | S1 ) P ( $ 251 ) P ( SN 51 ) \ P ( $ 152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( 515N ) P ( 52 SN ) P ( SN SN ) Fa Detected in frame_341.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
Note: no rewards, no actions
• If finite number (N) of states, can express P as a matrix
P(55)
P(5252)
P(SNS)\
P(5152) P(5252)
P(SN $2)
P=
P(SSN) P(52 SN)
P(SN SN) Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( 55 ) P ( 5252 ) P ( SNS ) \ P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( 52 SN ) P ( SN SN ) Detected in frame_342.jpg: Markov Process or Markov Chain
Memoryless random process
• Sequence of random states with Markov property
⚫ Definition of Markov Process
⚫S is a (finite) set of states (SES)
P is dynamics/transition model that specifices p(5.55 = 5)
• Note no rewards, no actions
=
⚫ If finite number (N) of states, can express P as a matrix
P(5) P(25)
P(SNS)\
P(552) P(5252)
P(SN 52)
P=
P(51 SN) P(S2SN)
P(SN SN) Markov Process or Markov Chain Memoryless random process • Sequence of random states with Markov property ⚫ Definition of Markov Process ⚫S is a ( finite ) set of states ( SES ) P is dynamics / transition model that specifices p ( 5.55 = 5 ) • Note no rewards , no actions = ⚫ If finite number ( N ) of states , can express P as a matrix P ( 5 ) P ( 25 ) P ( SNS ) \ P ( 552 ) P ( 5252 ) P ( SN 52 ) P = P ( 51 SN ) P ( S2SN ) P ( SN SN ) Detected in frame_343.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (5 € S)
⚫P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S|$1) P(5251)
P(SNS)
P(5152) P(5252)
P(SN $2)
P=
P(SSN) P(S2 SN)
P(SN SN))
Sc
' Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( 5 € S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S | $ 1 ) P ( 5251 ) P ( SNS ) P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2 SN ) P ( SN SN ) ) Sc ' Detected in frame_344.jpg: T
Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
• P is dynamics/transition model that specifices p(5+1 = 5' 5 = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(551) P(5251)
P(SNS)\
P(5152) P(5252)
P(SN $2)
P=
P(SSN) P(S2SN)
P(SN SN)
F T Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( 551 ) P ( 5251 ) P ( SNS ) \ P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) F Detected in frame_345.jpg: Markov Process or Markov Chain
A A
Memoryless random process
⚫ Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
•P is dynamics/transition model that specifices p(5.1 = 5 | 5 = 5)
• Note: no rewards, no actions
• If finite number (N) of states, can express P as a matrix
P(551) P(5251)
P(SNS)\
P(5152) P(5252)
P(SN $2)
P=
P(SSN) P(S2SN)
P(SN SN)
3 Markov Process or Markov Chain A A Memoryless random process ⚫ Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) • P is dynamics / transition model that specifices p ( 5.1 = 5 | 5 = 5 ) • Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( 551 ) P ( 5251 ) P ( SNS ) \ P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) 3 Detected in frame_346.jpg: Markov Process or Markov Chain
Memoryless random process
Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
• P is dynamics/transition model that specifices p(5+1 = s' st = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P($151) P($251)
P(SN $1)\
P(51 $2)
P(52 $2)
P(SN $2)
P =
P(S1 SN) P(52 SN)
P(SN SN)
SeeMixing
10 Р Markov Process or Markov Chain Memoryless random process Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( $ 151 ) P ( $ 251 ) P ( SN $ 1 ) \ P ( 51 $ 2 ) P ( 52 $ 2 ) P ( SN $ 2 ) P = P ( S1 SN ) P ( 52 SN ) P ( SN SN ) SeeMixing 10 Р Detected in frame_347.jpg: Markov Process or Markov Chain
2-2
Memoryless random process
• Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
•P is dynamics/transition model that specifices p(5+1 = 5' st = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S|51) P(52|51)
P(SNS)
P($1|$2) P(5252)
P(SN $2)
P=
P(SSN) P(SSN)
P(SN SN)
S
NNN Markov Process or Markov Chain 2-2 Memoryless random process • Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' st = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S | 51 ) P ( 52 | 51 ) P ( SNS ) P ( $ 1 | $ 2 ) P ( 5252 ) P ( SN $ 2 ) P = P ( SSN ) P ( SSN ) P ( SN SN ) S NNN Detected in frame_348.jpg: Markov Process or Markov Chain
A AM
Memoryless random process
⚫ Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
⚫P is dynamics/transition model that specifices p(51+1=5|5 = 5)
• Note: no rewards, no actions
• If finite number (N) of states, can express P as a matrix
P(sis) P(251)
P(SN 51)\
P(12) P(252)
P(SN $2)
P=
P(SSN) P(S2SN)
P(SN SN))
3 Markov Process or Markov Chain A AM Memoryless random process ⚫ Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) ⚫P is dynamics / transition model that specifices p ( 51 + 1 = 5 | 5 = 5 ) • Note : no rewards , no actions • If finite number ( N ) of states , can express P as a matrix P ( sis ) P ( 251 ) P ( SN 51 ) \ P ( 12 ) P ( 252 ) P ( SN $ 2 ) P = P ( SSN ) P ( S2SN ) P ( SN SN ) ) 3 Detected in frame_349.jpg: Markov Process or Markov Chain
HOME
Memoryless random process
Sequence of random states with Markov property
Definition of Markov Process
⚫S is a (finite) set of states (s = 5)
• P is dynamics/transition model that specifices p(5+1 =s' st = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P($1 $1)
P($251)
P(SN 51)\
P($12)
P($2 $2)
P(SN 52)
P=
P(515N) P(52 SN)
P(SN SN)
S
Schot
From
NNNN
N Markov Process or Markov Chain HOME Memoryless random process Sequence of random states with Markov property Definition of Markov Process ⚫S is a ( finite ) set of states ( s = 5 ) • P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( $ 1 $ 1 ) P ( $ 251 ) P ( SN 51 ) \ P ( $ 12 ) P ( $ 2 $ 2 ) P ( SN 52 ) P = P ( 515N ) P ( 52 SN ) P ( SN SN ) S Schot From NNNN N Detected in frame_35.jpg: Today's Plan
x
lecture2-2
NNNN
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screen Mirroring Document
Screenshot
Focus Today's Plan x lecture2-2 NNNN • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screen Mirroring Document Screenshot Focus Detected in frame_350.jpg: Markov Process or Markov Chain
-
Memoryless random process
• Sequence of random states with Markov property
• Definition of Markov Process
⚫S is a (finite) set of states (s = S)
•P is dynamics/transition model that specifices p(5+1 = s' st = 5)
• Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(S|51) P(52|51)
P(SNS)\
P($152) P(5252)
P(SN $2)
P=
P(S|SN) P(S2SN)
P(SN SN)
S
NANO
'
IN Markov Process or Markov Chain - Memoryless random process • Sequence of random states with Markov property • Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) • P is dynamics / transition model that specifices p ( 5 + 1 = s ' st = 5 ) • Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( S | 51 ) P ( 52 | 51 ) P ( SNS ) \ P ( $ 152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( S | SN ) P ( S2SN ) P ( SN SN ) S NANO ' IN Detected in frame_351.jpg: Markov Process or Markov Chain
A.
Memoryless random process
• Sequence of random states with Markov property
Definition of Markov Process
⚫S is a (finite) set of states (5 € 5)
•P is dynamics/transition model that specifices p(5+) =s's, = 5)
• Note: no rewards, no actions
⚫ If finite number (N) of states, can express P as a matrix
P(5151) P(5251)
P(SNS)\
P(5152) P(5252)
P(SN $2)
P=
P(S1 SN) P(52 SN)
P(SN SN) Markov Process or Markov Chain A. Memoryless random process • Sequence of random states with Markov property Definition of Markov Process ⚫S is a ( finite ) set of states ( 5 € 5 ) • P is dynamics / transition model that specifices p ( 5+ ) = s's , = 5 ) • Note : no rewards , no actions ⚫ If finite number ( N ) of states , can express P as a matrix P ( 5151 ) P ( 5251 ) P ( SNS ) \ P ( 5152 ) P ( 5252 ) P ( SN $ 2 ) P = P ( S1 SN ) P ( 52 SN ) P ( SN SN ) Detected in frame_352.jpg: Markov Process or Markov Chain
Memoryless random process
⚫ Sequence of random states with Markov property
⚫ Definition of Markov Process
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices p(5+1 = 5' 5, = 5)
Note: no rewards, no actions
If finite number (N) of states, can express P as a matrix
P(ss)
P(512) P($2 $2)
P($251)
P(SNS)\
P(SN $2)
P
P(515N) P(525N)
P(SN SN)
S
IS
NNNN Markov Process or Markov Chain Memoryless random process ⚫ Sequence of random states with Markov property ⚫ Definition of Markov Process ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices p ( 5 + 1 = 5 ' 5 , = 5 ) Note : no rewards , no actions If finite number ( N ) of states , can express P as a matrix P ( ss ) P ( 512 ) P ( $ 2 $ 2 ) P ( $ 251 ) P ( SNS ) \ P ( SN $ 2 ) P P ( 515N ) P ( 525N ) P ( SN SN ) S IS NNNN Detected in frame_353.jpg: Example Mars Power Mark Com Taman
IL
F
43
is
VUE 04
Π
UA
1151
0
0
0
0
P
04 02 04 (Y
0
ነነ
0
n
0
0
D 0
M002 04 Q
044 Example Mars Power Mark Com Taman IL F 43 is VUE 04 Π UA 1151 0 0 0 0 P 04 02 04 ( Y 0 ነነ 0 n 0 0 D 0 M002 04 Q 044 Detected in frame_354.jpg: Example. Mars Rover Markov Chain Transition Matrix. P
51
53
:54
56
$7
LL
102
92
02
BZ
02
16
10.6. 04 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
.0
0
0
0.4
0.2 0.4
0
0
D
0
0
0.4 0.2 0.4
■DIN
N Example . Mars Rover Markov Chain Transition Matrix . P 51 53 : 54 56 $ 7 LL 102 92 02 BZ 02 16 10.6 . 04 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 .0 0 0 0.4 0.2 0.4 0 0 D 0 0 0.4 0.2 0.4 ■ DIN N Detected in frame_355.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
04
53
0.4
04
SA
0.4
55
0.4
0.4
56
0.4
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4 0
0
0
0
0
0.4
0.2 0.4
NNN
N Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 04 53 0.4 04 SA 0.4 55 0.4 0.4 56 0.4 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NNN N Detected in frame_356.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
04
53
0.4
04
SA
0.4
55
0.4
0.4
56
0.4
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
S
10
NNN
N Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 04 53 0.4 04 SA 0.4 55 0.4 0.4 56 0.4 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S 10 NNN N Detected in frame_357.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
0.4
S4
0.4
55
04
$6
0.4
0.4
57
0.4
04
0.4
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
Sereng Ор
Screenshot Fo
'
NON INN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 0.4 S4 0.4 55 04 $ 6 0.4 0.4 57 0.4 04 0.4 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Sereng Ор Screenshot Fo ' NON INN Detected in frame_358.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
$3
0.4
S4
0.4
55
04
$6
0.4
0.4
57
0.4
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0 0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
Screen Mon Рос
Screenshot Fo
'
NON INN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 $ 3 0.4 S4 0.4 55 04 $ 6 0.4 0.4 57 0.4 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mon Рос Screenshot Fo ' NON INN Detected in frame_359.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S1
0.4
$2
0.4
$3
0.4
S4
0.4
0.4
0.4
0.4
56
57
0.4
0.4
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
Screen M
D
Screenshot Focus
NININ Example : Mars Rover Markov Chain Transition Matrix , P S1 0.4 $ 2 0.4 $ 3 0.4 S4 0.4 0.4 0.4 0.4 56 57 0.4 0.4 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen M D Screenshot Focus NININ Detected in frame_36.jpg: Today's Plan
2-1
x
NNNN
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screen M
Dom
Screenshot
Focus Today's Plan 2-1 x NNNN • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screen M Dom Screenshot Focus Detected in frame_360.jpg: T
Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
S4
56
57
0.4
0.4
0.4
0.4
0.4
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
SM
Schet Fon
RR
N T Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 S4 56 57 0.4 0.4 0.4 0.4 0.4 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 SM Schet Fon RR N Detected in frame_361.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S1
0.4
$2
0.4
$3
0.4
S4
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2
0.4
Screen M
Овошения
Screenshot Focus
'
NININ Example : Mars Rover Markov Chain Transition Matrix , P S1 0.4 $ 2 0.4 $ 3 0.4 S4 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen M Овошения Screenshot Focus ' NININ Detected in frame_362.jpg: T
Example: Mars Rover Markov Chain Transition Matrix, P
$1
53
54
56
ST
0.4
0.4
0.4
0.4
0.4
0.4
0.4
04
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
SM
Stoc
N T Example : Mars Rover Markov Chain Transition Matrix , P $ 1 53 54 56 ST 0.4 0.4 0.4 0.4 0.4 0.4 0.4 04 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 SM Stoc N Detected in frame_363.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
0.4
0.4
S4
0.4
55
$6
57
04
0.4
0.4
0.4
0.4
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0 0.4
0.2
0.4
0
0
0
0
0 0.4 0.2 0.4
Screening Роси
Screenshot Focus
NOIN IN N Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 0.4 0.4 S4 0.4 55 $ 6 57 04 0.4 0.4 0.4 0.4 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening Роси Screenshot Focus NOIN IN N Detected in frame_364.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0,4
$2
0.4
$3
0.4
S4
55
56
ST
0.4
0.4
0.4
0.4
0.4
0.4
04
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4
0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
S
De
IN
NIN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0,4 $ 2 0.4 $ 3 0.4 S4 55 56 ST 0.4 0.4 0.4 0.4 0.4 0.4 04 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S De IN NIN Detected in frame_365.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
Sz
0.4
$3
S4
$5
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
Screen M
Вобщения
Screenshot Focus
NANN
" Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 Sz 0.4 $ 3 S4 $ 5 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen M Вобщения Screenshot Focus NANN " Detected in frame_366.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
0.4
54
55
56
$7
0.4
0.4
0.4
0.4
0.4
0.4
04
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
S
D
DON Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 0.4 54 55 56 $ 7 0.4 0.4 0.4 0.4 0.4 0.4 04 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S D DON Detected in frame_367.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
S4
0.4
0.4
0.4
04
0.4
56
57
0.4
04
0.4
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0 0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
SM
D
Screenshot Fo
NNNN
' Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 S4 0.4 0.4 0.4 04 0.4 56 57 0.4 04 0.4 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 SM D Screenshot Fo NNNN ' Detected in frame_368.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
0.4
0.4
&
54
0.4
55
56
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
S
D
NNNNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 0.4 0.4 & 54 0.4 55 56 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S D NNNNN Detected in frame_369.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
&
SA
$6
57
0.4
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0 0
0
0
0
0.4
0.2 0.4
0
0 0
0
0
0.4 0.2 0.4
Screening
Document
Scrmmshot Focm
'
NNNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 & SA $ 6 57 0.4 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening Document Scrmmshot Focm ' NNNN Detected in frame_37.jpg: Today's Plan
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Stop Screen Mirroring Document
Screenshot
focus
NININN Today's Plan • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Stop Screen Mirroring Document Screenshot focus NININN Detected in frame_370.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
04
$2
0.4
$3
0.4
54
0.4
0.4
56
04
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
S
Scan (20) Fe
NNN
a Example : Mars Rover Markov Chain Transition Matrix , P S₁ 04 $ 2 0.4 $ 3 0.4 54 0.4 0.4 56 04 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S Scan ( 20 ) Fe NNN a Detected in frame_371.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
&
55
S4
56
0.4
0.4
0.4
0.4
10.4
0.4
ST
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0 0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
Screening D
Sir F
NANN
' Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 & 55 S4 56 0.4 0.4 0.4 0.4 10.4 0.4 ST 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening D Sir F NANN ' Detected in frame_372.jpg: си
Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
0.4
S4
0.4
0.4
S5
$6
57
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
(0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
Stop
Screening
Document
0 0.4 0.2 0.4
Screenshot Focus
"
IN NINN си Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 0.4 S4 0.4 0.4 S5 $ 6 57 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 ( 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 Stop Screening Document 0 0.4 0.2 0.4 Screenshot Focus " IN NINN Detected in frame_373.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
S₂
0.4
53
0.4
S4
0.4
0.4
1.4
$5
56
0.4
ST
0.6
0.2
0.2
0.2
0.2
0.2
06
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
..
3 Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 S₂ 0.4 53 0.4 S4 0.4 0.4 1.4 $ 5 56 0.4 ST 0.6 0.2 0.2 0.2 0.2 0.2 06 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 .. 3 Detected in frame_374.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
10.4
$3
54
$5
$6
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
(0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
Screen Mirroring
Document
Screenshot
A Focus
N N N N Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 10.4 $ 3 54 $ 5 $ 6 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 ( 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mirroring Document Screenshot A Focus N N N N Detected in frame_375.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S1
0.4
Sz
&
9, az
$3
S4
55
56
0.4
0.4
0.4
0:4
0:4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0.
0
0
0.4 0.2 0.4
Seven M
Socia
IN IN INN Example : Mars Rover Markov Chain Transition Matrix , P S1 0.4 Sz & 9 , az $ 3 S4 55 56 0.4 0.4 0.4 0 : 4 0 : 4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 Seven M Socia IN IN INN Detected in frame_376.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
$3
56
9, az
S4
55
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0 0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
Soreang Di
Socia
'
NININN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 $ 3 56 9 , az S4 55 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Soreang Di Socia ' NININN Detected in frame_377.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
34
54
0.4
0.4
0.4
0.4
0.4
04
$6
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
Screen M
D
Screenshot Focus
NNNNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 34 54 0.4 0.4 0.4 0.4 0.4 04 $ 6 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen M D Screenshot Focus NNNNN Detected in frame_378.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
S₂
04
الله
0.4
54
$5
0.4
04
0.4
14
04
56
0.4
0.6
0.2
0.2
0.2
02
0.2
06
(0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
a az
15
NINX Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 S₂ 04 الله 0.4 54 $ 5 0.4 04 0.4 14 04 56 0.4 0.6 0.2 0.2 0.2 02 0.2 06 ( 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 a az 15 NINX Detected in frame_379.jpg: NNNN
Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
$3
54
$5
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
Screen Mirroring
Document
Screenshot Focus
" NNNN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 $ 3 54 $ 5 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mirroring Document Screenshot Focus " Detected in frame_38.jpg: Today's Plan
x
2-1
NNNN
"
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screen Mirroring Document
Screenshot
Focus Today's Plan x 2-1 NNNN " • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screen Mirroring Document Screenshot Focus Detected in frame_380.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
&
S4
0.4
0.4
0.4
55
0.4
0.4
56
0.4
0.4
ai az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
10 Росія
IN
NNNN
a Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 & S4 0.4 0.4 0.4 55 0.4 0.4 56 0.4 0.4 ai az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 10 Росія IN NNNN a Detected in frame_381.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
S4
55
0.4
0.4
0.4
0.4
0.4
0.4
56
0.4
91 az
ST
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Sorr
D
Shoes
NNNN
+ Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 S4 55 0.4 0.4 0.4 0.4 0.4 0.4 56 0.4 91 az ST 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Sorr D Shoes NNNN + Detected in frame_382.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
Sz
0.4
0.4
$3
0.4
S4
$5
0.4
0.4
0.4
0.4
04
$6
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
Screening
Document
Screenshot Focus
NININ
+ Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 Sz 0.4 0.4 $ 3 0.4 S4 $ 5 0.4 0.4 0.4 0.4 04 $ 6 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening Document Screenshot Focus NININ + Detected in frame_383.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
Sz
0.4
0.4
$3
0.4
S4
$5
0.4
0.4
0.4
0.4
04
$6
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0 0
0.4 0.2 0.4
Screening
Document
Screenshot Focus
NININ
+ Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 Sz 0.4 0.4 $ 3 0.4 S4 $ 5 0.4 0.4 0.4 0.4 04 $ 6 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening Document Screenshot Focus NININ + Detected in frame_384.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
Sz
$3
S4
0.4
0.4
0.4
0.4
0.4
0.4
$5
0.4
$6
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
Screen Mirroring
Document
Screenshot Focus
IN INN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 Sz $ 3 S4 0.4 0.4 0.4 0.4 0.4 0.4 $ 5 0.4 $ 6 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mirroring Document Screenshot Focus IN INN Detected in frame_385.jpg: NNNN
Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
S4
$5
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Screen Mirroring
Screenshot Focus NNNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 S4 $ 5 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mirroring Screenshot Focus Detected in frame_386.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
Sz
53
S4
04
0.4
0.4
ai az
$5
0.4
56
$7
0.4
04
0.6
0:2
0.2
0.2
02
0.2
06
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
IN IN IN NO Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 Sz 53 S4 04 0.4 0.4 ai az $ 5 0.4 56 $ 7 0.4 04 0.6 0 : 2 0.2 0.2 02 0.2 06 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 IN IN IN NO Detected in frame_387.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
Sz
0.4
$3
0.4
S4
$5
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
Screen Moving Document
Screenshot Focus
"
IN IN INN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 Sz 0.4 $ 3 0.4 S4 $ 5 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Moving Document Screenshot Focus " IN IN INN Detected in frame_388.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
ai az
Si
0.4
$2
0.4
53
0.4
54
0.4
55
56
57
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0.
0
0
0.4 0.2 0.4
S
Socia Example : Mars Rover Markov Chain Transition Matrix , P ai az Si 0.4 $ 2 0.4 53 0.4 54 0.4 55 56 57 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 S Socia Detected in frame_389.jpg: NNN N
Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
$3
0.4
0.4
04
24
S4
0.4
0.4
55
0.4
04
56
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0 0.4
0.2
0.4
Sa Mg D NNN N Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 $ 3 0.4 0.4 04 24 S4 0.4 0.4 55 0.4 04 56 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Sa Mg D Detected in frame_39.jpg: Today's Plan
c2-2
K
NNNN
"
• Last Time:
• Introduction
• Components of an agent: model, value, policy
• This Time:
• Making good decisions given a Markov decision process
• Next Time:
⚫ Policy evaluation when don't have a model of how the world works
Screen Mirroring Document
Screenshot
Focu Today's Plan c2-2 K NNNN " • Last Time : • Introduction • Components of an agent : model , value , policy • This Time : • Making good decisions given a Markov decision process • Next Time : ⚫ Policy evaluation when don't have a model of how the world works Screen Mirroring Document Screenshot Focu Detected in frame_390.jpg: NNNN
Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
Sz
0.4
$3
E
S4
$5
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
Stop
Screen Mirroring
Document
Screenshot Focus
• NNNN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 Sz 0.4 $ 3 E S4 $ 5 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Stop Screen Mirroring Document Screenshot Focus • Detected in frame_391.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
D.4
$2
04
الله
0.4
04
54
0.4
$5
0.4
04
56
0.4
di az
0.6
0.2
0.2
0.2
02
0.2
0,6
10.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
.0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
3 Example : Mars Rover Markov Chain Transition Matrix , P Si D.4 $ 2 04 الله 0.4 04 54 0.4 $ 5 0.4 04 56 0.4 di az 0.6 0.2 0.2 0.2 02 0.2 0,6 10.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 Detected in frame_392.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
0.4
$2
$3
0.4
0.4
04
34
S4
0.4
0.4
55
0.4
0.4
56
0.4
a, az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0 0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2 0.4
Swing
NNNN
' Example : Mars Rover Markov Chain Transition Matrix , P 0.4 $ 2 $ 3 0.4 0.4 04 34 S4 0.4 0.4 55 0.4 0.4 56 0.4 a , az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Swing NNNN ' Detected in frame_393.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
S4
$5
0.4
0.4
0.4
0.4
0.4
56
0.4
0.4
9, az
57
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
Screen Mining D
Screenshot Focus
IN IN INN
" Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 S4 $ 5 0.4 0.4 0.4 0.4 0.4 56 0.4 0.4 9 , az 57 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mining D Screenshot Focus IN IN INN " Detected in frame_394.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
Sz
0.4
$3
S4
0.4
0.4
0.4
0.4
$5
0.4
0.4
$6
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
Screening D
0 0.4 0.2 0.4
Screenshot Focus
'
INNINN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 Sz 0.4 $ 3 S4 0.4 0.4 0.4 0.4 $ 5 0.4 0.4 $ 6 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 Screening D 0 0.4 0.2 0.4 Screenshot Focus ' INNINN Detected in frame_395.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
&
S4
$5
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
Stop
Screen Mirroring
Document
Screenshot Focus
'
NININ Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 & S4 $ 5 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Stop Screen Mirroring Document Screenshot Focus ' NININ Detected in frame_396.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
$3
S4
$5
56
57
0.4
0,4
0.4
0.4
0.4
04
14
04
0.6
0.2
0.2
0.2
02
0.2
06
(0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
'
15
TINN Example : Mars Rover Markov Chain Transition Matrix , P Si $ 3 S4 $ 5 56 57 0.4 0,4 0.4 0.4 0.4 04 14 04 0.6 0.2 0.2 0.2 02 0.2 06 ( 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 ' 15 TINN Detected in frame_397.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
53
34
a, az
S4
$6
ST
0.4
0.4
04
0.4
04
0.4
0.4
0.4
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
Screen Ming
D
Screenshot. Foczis
NININN
+ Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 53 34 a , az S4 $ 6 ST 0.4 0.4 04 0.4 04 0.4 0.4 0.4 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Ming D Screenshot . Foczis NININN + Detected in frame_398.jpg: T
Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
$3
0.4
0.4
54
0.4
04
55
0A
0.4
56
0.4
04
91 az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
Sen M
D
NN INN T Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 $ 3 0.4 0.4 54 0.4 04 55 0A 0.4 56 0.4 04 91 az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Sen M D NN INN Detected in frame_399.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
$2
53
&
9, az
57
0.4
0.4
54
0.4
0.4
04
0.4
55
04
0.4
56
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0 0
0
0
0.4 0.2 0.4
SM
D
Serhat Fon
N IN INN
+ Example : Mars Rover Markov Chain Transition Matrix , P Si $ 2 53 & 9 , az 57 0.4 0.4 54 0.4 0.4 04 0.4 55 04 0.4 56 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 SM D Serhat Fon N IN INN + Detected in frame_4.jpg: Stanford ENGINEERING Stanford ENGINEERING Detected in frame_400.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
$3
S4
$6
a, az
$5
57
0.4
0.4
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
Screen Mirroring
Document
Screenshot Focus
'
NININN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 $ 3 S4 $ 6 a , az $ 5 57 0.4 0.4 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mirroring Document Screenshot Focus ' NININN Detected in frame_401.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
04
الله
a, az
0.4
S4
55
56
ST
0.4
0.4
0:4
0.4
4
04
0.6
0.2
0.2
0.2
0.2
0.2
0.6
(0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
NINA Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 04 الله a , az 0.4 S4 55 56 ST 0.4 0.4 0 : 4 0.4 4 04 0.6 0.2 0.2 0.2 0.2 0.2 0.6 ( 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NINA Detected in frame_402.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
$3
&
56
9, az
54
55
57
0.4
0.4
0.4
0.4
0.4
10.4
04
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Screening D
Serhat Fon
NININN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 $ 3 & 56 9 , az 54 55 57 0.4 0.4 0.4 0.4 0.4 10.4 04 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening D Serhat Fon NININN Detected in frame_403.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
$3
S4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0 0.4 0.2 0.4
Screening Вр
Screenshot Fo
'
• IN IN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 $ 3 S4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening Вр Screenshot Fo ' • IN IN Detected in frame_404.jpg: NNNN
Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
$3
✓
S4
$5
0.4
0.4
0.4
0.4
0.4
0.4
0.4
04
$6
0.4
9, az
57
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0 0
0.4 0.2
0.4
Screen Mon
Document
Screenshot Focus NNNN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 $ 3 ✓ S4 $ 5 0.4 0.4 0.4 0.4 0.4 0.4 0.4 04 $ 6 0.4 9 , az 57 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mon Document Screenshot Focus Detected in frame_405.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$2
0.4
$3
0.4
S4
55
0.4
0.4
0.4
0.4
$6
0.4
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
Stop Screen Ming
D
Screenshot Focus
IN IN INN Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 2 0.4 $ 3 0.4 S4 55 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Stop Screen Ming D Screenshot Focus IN IN INN Detected in frame_406.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$2
0.4
$3
0.4
S4
55
0.4
0.4
0.4
0.4
04
$6
0.4
91 az
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
Screen Mining
D
Screenshot. Focus
'
IN IN INN Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 2 0.4 $ 3 0.4 S4 55 0.4 0.4 0.4 0.4 04 $ 6 0.4 91 az $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mining D Screenshot . Focus ' IN IN INN Detected in frame_407.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
ai az
041
$2
$3
S4
$5
56
04
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
5 from
0.4
0,6
3
' Example : Mars Rover Markov Chain Transition Matrix , P ai az 041 $ 2 $ 3 S4 $ 5 56 04 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 5 from 0.4 0,6 3 ' Detected in frame_408.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
51
0.4
$2
0.4
$3
a, az
S4
55
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4 0
0 0
0
0 0.4
0.2 0.4
Screening
F
'
INNINN Example : Mars Rover Markov Chain Transition Matrix , P 51 0.4 $ 2 0.4 $ 3 a , az S4 55 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening F ' INNINN Detected in frame_409.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
α, az
S4
$5
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
Screen Mirroring
Document
Screenshot focus
'
NNNNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 α , az S4 $ 5 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mirroring Document Screenshot focus ' NNNNN Detected in frame_410.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
ai az
Si
04
$2
04
الله
0.4
S4
0.4
11.4
$5
56
DA
0.6
02
0.2
0.2
0.2
0.2
0.6
10.6 0.4 0 0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
15 Example : Mars Rover Markov Chain Transition Matrix , P ai az Si 04 $ 2 04 الله 0.4 S4 0.4 11.4 $ 5 56 DA 0.6 02 0.2 0.2 0.2 0.2 0.6 10.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 15 Detected in frame_411.jpg: N N N N
Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
24
S4
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
Screen M
Do
Scro Focus N N N N Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 24 S4 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen M Do Scro Focus Detected in frame_412.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
52
0.4
53
0.4
0.4
Ex
S4
0.4
55
04
0.4
56
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
Son M
Sc Example : Mars Rover Markov Chain Transition Matrix , P $ 1 52 0.4 53 0.4 0.4 Ex S4 0.4 55 04 0.4 56 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Son M Sc Detected in frame_413.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
&
9, az
S4
55
56
ST
0.4
0.4
0.4
0.4
0.4
0.4
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0 0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0 0
0
0.4 0.2 0.4
Sorin Ming
Street Fo
N N N N
' Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 & 9 , az S4 55 56 ST 0.4 0.4 0.4 0.4 0.4 0.4 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Sorin Ming Street Fo N N N N ' Detected in frame_414.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
S4
55
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
91 az
ST
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
Sing D
Shot Forum
'
IN IN INN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 S4 55 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 91 az ST 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Sing D Shot Forum ' IN IN INN Detected in frame_415.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
S3
&
S4
0.4
0.4
0.4
0.4
9, az
57
0.4
04
0.4
55
0.4
56
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Screen Mirroring D
Screenshot
'
IN IN INN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 S3 & S4 0.4 0.4 0.4 0.4 9 , az 57 0.4 04 0.4 55 0.4 56 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mirroring D Screenshot ' IN IN INN Detected in frame_416.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
ગ
04
$2
$3
S4
55
0.4
0.4
0.4
0.4
0.4
56
0.4
ai az
06
0.2
0.2
0.2
0.2
0.2
0,6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
.0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
..
15 Example : Mars Rover Markov Chain Transition Matrix , P ગ 04 $ 2 $ 3 S4 55 0.4 0.4 0.4 0.4 0.4 56 0.4 ai az 06 0.2 0.2 0.2 0.2 0.2 0,6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 .. 15 Detected in frame_417.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
0.4
S4
0.4
55
0.4
$6
0.4
0.4
0.4
0.4
0.4
a az
57
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
S
D
She Focus
'
IN IN INN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 0.4 S4 0.4 55 0.4 $ 6 0.4 0.4 0.4 0.4 0.4 a az 57 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S D She Focus ' IN IN INN Detected in frame_418.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
Sz
✓
55
9, az
53
54
56
ST
0.4
0.4
0.4
04
0.4
0.4
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0.
0
0 0.4
0.2
0.4
Вос
NİN INN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 Sz ✓ 55 9 , az 53 54 56 ST 0.4 0.4 0.4 04 0.4 0.4 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 Вос NİN INN Detected in frame_419.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S1
$2
34
S4
$5
56
9, az
0.4
$3
ST
0.4
0.4
0.4
0.4
0.4
0.4
10.4
04
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
Swing D
Sche
NININN Example : Mars Rover Markov Chain Transition Matrix , P S1 $ 2 34 S4 $ 5 56 9 , az 0.4 $ 3 ST 0.4 0.4 0.4 0.4 0.4 0.4 10.4 04 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Swing D Sche NININN Detected in frame_420.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
34
S4
$6
9, az
ST
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
Screening Docu
So F
'
IN NINN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 34 S4 $ 6 9 , az ST 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening Docu So F ' IN NINN Detected in frame_421.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
S4
0.4
0.4
0.4
0.4
0.4
$6
9, az
ST
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
Screen ring Docum
Screenshot Focus
'
NNNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 S4 0.4 0.4 0.4 0.4 0.4 $ 6 9 , az ST 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen ring Docum Screenshot Focus ' NNNN Detected in frame_422.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
51
0.4
$2
0:4
الله
S4
0.4
$5
0.4
56
0.4
a; az
04
0.6
0.2
0.2
0.2
02
0.2
0,6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
..
3 Example : Mars Rover Markov Chain Transition Matrix , P 51 0.4 $ 2 0 : 4 الله S4 0.4 $ 5 0.4 56 0.4 a ; az 04 0.6 0.2 0.2 0.2 02 0.2 0,6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 .. 3 Detected in frame_423.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
a, az
S4
$5
$6
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
Screen Mirroring
Document
Screenshot Focus
INNINN
" Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 a , az S4 $ 5 $ 6 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mirroring Document Screenshot Focus INNINN " Detected in frame_424.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
$3
0.4
0.4
0.4
34
54
0.4
ai az
55
56
57
0:4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
==
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
SF
NNN
N Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 $ 3 0.4 0.4 0.4 34 54 0.4 ai az 55 56 57 0 : 4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P == 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 SF NNN N Detected in frame_425.jpg: NNNN
Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
&
S4
55
0.4
0.4
0.4
0.4
04
0.4
56
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Son Mowing D
Sh NNNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 & S4 55 0.4 0.4 0.4 0.4 04 0.4 56 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Son Mowing D Sh Detected in frame_426.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
$2
$3
34
S4
$6
a, az
0.4
0.4
55
57
0.4
0.4
04
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Screening D
Shot Fo
NININN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 $ 2 $ 3 34 S4 $ 6 a , az 0.4 0.4 55 57 0.4 0.4 04 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening D Shot Fo NININN Detected in frame_427.jpg: NNNN
Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
0.4
24
54
55
0.4
04
0.4
0.4
0.4
$6
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Stop
Screen Mirroring D
Screenshot) Focus NNNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 0.4 24 54 55 0.4 04 0.4 0.4 0.4 $ 6 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Stop Screen Mirroring D Screenshot ) Focus Detected in frame_428.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
04
$3
S4
0.4
0.4
$5
0.4
56
0.4
di az
$7
06
0.2
0.2
0.2
0.2
0.2
06
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
TY IN N Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 04 $ 3 S4 0.4 0.4 $ 5 0.4 56 0.4 di az $ 7 06 0.2 0.2 0.2 0.2 0.2 06 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 TY IN N Detected in frame_429.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
S4
0.4
0.4
0.4
0.4
0.4
$6
9, az
$7
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
S
Дос
0 0.4 0.2 0.4
She focus
NNNNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 S4 0.4 0.4 0.4 0.4 0.4 $ 6 9 , az $ 7 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 S Дос 0 0.4 0.2 0.4 She focus NNNNN Detected in frame_430.jpg: T
Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
53
0.4
54
0.4
0.4
0.4
55
04
04
56
0.4
ai az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
(0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Sering
Sco
IN
NINA T Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 53 0.4 54 0.4 0.4 0.4 55 04 04 56 0.4 ai az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 ( 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Sering Sco IN NINA Detected in frame_431.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
$3
34
9, az
S4
55
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
S
Доск
Sch. Focus
NNNNN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 $ 3 34 9 , az S4 55 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S Доск Sch . Focus NNNNN Detected in frame_432.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
$2
$3
24
$6
9, az
0.4
0.4
S4
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0 0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2 0.4
Bren
Воск
Smshot Focus
'
INNNNNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 $ 2 $ 3 24 $ 6 9 , az 0.4 0.4 S4 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Bren Воск Smshot Focus ' INNNNNN Detected in frame_433.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$2
0.4
$3
E
56
9, az
S4
$5
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0 0
0
0
0
0.4
0.2 0.4
0
0 0
0 0
0.4 0.2 0.4
Screening
Собщения
Screenshot Focus
'
NNNNNNN Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 2 0.4 $ 3 E 56 9 , az S4 $ 5 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening Собщения Screenshot Focus ' NNNNNNN Detected in frame_434.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
04
$2
0.4
$3
S4
0.4
04
14
$5
0.4
04
di az
56
$7
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
0.6
0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
TYNN
3 Example : Mars Rover Markov Chain Transition Matrix , P Si 04 $ 2 0.4 $ 3 S4 0.4 04 14 $ 5 0.4 04 di az 56 $ 7 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 TYNN 3 Detected in frame_435.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
$3
91 az
S4
$5
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0 0 0.4 0.2
0.4
Son M
Document
Ser Foca
NININN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 $ 3 91 az S4 $ 5 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Son M Document Ser Foca NININN Detected in frame_436.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
$2
$3
E
$6
a, az
0.4
S4
$5
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
Stop
Screen Mirroring Document
Screenshot Focus
NNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 $ 2 $ 3 E $ 6 a , az 0.4 S4 $ 5 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Stop Screen Mirroring Document Screenshot Focus NNN Detected in frame_437.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
04
$2
di az
$3
S4
$5
56
$7
04
0.4
ሲሳ
0.4
0.4
14
04
0.6
0.2
0.2
0.2
0.2
0.2
06
10.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
NNN
3 Example : Mars Rover Markov Chain Transition Matrix , P Si 04 $ 2 di az $ 3 S4 $ 5 56 $ 7 04 0.4 ሲሳ 0.4 0.4 14 04 0.6 0.2 0.2 0.2 0.2 0.2 06 10.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NNN 3 Detected in frame_438.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
0.4
$3
0.4
S4
$5
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
Screening D
Screenshot) Focus
'
NNNNNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 0.4 $ 3 0.4 S4 $ 5 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening D Screenshot ) Focus ' NNNNNN Detected in frame_439.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
04
53
9, az
S4
55
56
57
0.4
0.4
0.4
0.4
10.4
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0.
0
0
0.4 0.2 0.4
Sc
IN
NINN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 04 53 9 , az S4 55 56 57 0.4 0.4 0.4 0.4 10.4 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 Sc IN NINN Detected in frame_440.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$2
0.4
$3
0.4
04
34
54
0.4
0.4
55
0.4
04
56
0.4
9, az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4.
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Screen Miring D
Sish Focus
'
NNNNN Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 2 0.4 $ 3 0.4 04 34 54 0.4 0.4 55 0.4 04 56 0.4 9 , az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 . 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Miring D Sish Focus ' NNNNN Detected in frame_441.jpg: T
Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
53
&
54
9, az
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0.
0
0
0.4 0.2 0.4
Вос
Sca
IN
NIN T Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 53 & 54 9 , az 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 Вос Sca IN NIN Detected in frame_442.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
0.4
0.4
34
S4
0.4
0.4
55
0.4
$6
a az
57
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0 0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
Screen Ming Doct
Schet fo
NININN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 0.4 0.4 34 S4 0.4 0.4 55 0.4 $ 6 a az 57 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Ming Doct Schet fo NININN Detected in frame_443.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
$3
54
$5
56
57
04
0.4
ሲቀ
0.4
0.4
04
0.6
0.2
0.2
0.2
02
0.2
06
70.6 0.4 0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
25 Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 $ 3 54 $ 5 56 57 04 0.4 ሲቀ 0.4 0.4 04 0.6 0.2 0.2 0.2 02 0.2 06 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 25 Detected in frame_444.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
56
a, az
54
S5
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
10.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0 0.4 0.2
0.4
S
Dict
NININN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 56 a , az 54 S5 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 10.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S Dict NININN Detected in frame_445.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
0.4
0.4
S4
0.4
0.4
$5
0.4
0.4
$6
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Stop
Screen Mirroring Document
Screenshot
'
IN NINN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 0.4 0.4 S4 0.4 0.4 $ 5 0.4 0.4 $ 6 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Stop Screen Mirroring Document Screenshot ' IN NINN Detected in frame_446.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
04
الله
04
S4
0.4
04
21.4
$5
04
04
56
0.4
ai az
57
0.6
0.2
0.2
0.2
0.2
0.2
06
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0:4
3 Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 04 الله 04 S4 0.4 04 21.4 $ 5 04 04 56 0.4 ai az 57 0.6 0.2 0.2 0.2 0.2 0.2 06 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0 : 4 3 Detected in frame_447.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
$3
S4
55
0.4
0.4
0.4
0.4
0.4
04
56
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
Screen
D
Screenshot Focus
'
IN NINN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 $ 3 S4 55 0.4 0.4 0.4 0.4 0.4 04 56 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen D Screenshot Focus ' IN NINN Detected in frame_448.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
0.4
0.4
54
0.4
0.4
55
04
56
0.4
0.4
ai az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0.
0
0
0.4 0.2
0.4
Sen M
Sc
NNN
N Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 0.4 0.4 54 0.4 0.4 55 04 56 0.4 0.4 ai az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 Sen M Sc NNN N Detected in frame_449.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
53
&
56
9, az
54
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
Screen Mowing D
Screenshot Focus
NNNNN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 53 & 56 9 , az 54 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mowing D Screenshot Focus NNNNN Detected in frame_450.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
53
0.4
0.4
0.4
&
54
0.4
0.4
$5
a, az
57
0.4
04
0.4
56
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0 0
0.4 0.2 0.4
Screen M
Doct
Socia
IN
NIN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 53 0.4 0.4 0.4 & 54 0.4 0.4 $ 5 a , az 57 0.4 04 0.4 56 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen M Doct Socia IN NIN Detected in frame_451.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
ગ
04
$2
$3
54
$5
0.4
0.4
04
0.4
04
56
0:4
ai az
57
06
0.2
0.2
0.2
02
0.2
06
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
°
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
3
' Example : Mars Rover Markov Chain Transition Matrix , P ગ 04 $ 2 $ 3 54 $ 5 0.4 0.4 04 0.4 04 56 0 : 4 ai az 57 06 0.2 0.2 0.2 02 0.2 06 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 ° P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 ' Detected in frame_452.jpg: NANNI
Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
S4
$5
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
9, az
57
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
°
J
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
b
0
0
0 0
0.4 0.2
0.4
Screening
Соск
Screenshot Focus
+ NANNI Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 S4 $ 5 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 9 , az 57 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 ° J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 b 0 0 0 0 0.4 0.2 0.4 Screening Соск Screenshot Focus + Detected in frame_453.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
ગ
04
$2
$3
54
$5
0.4
0.4
04
0.4
04
56
0:4
ai az
57
06
0.2
0.2
0.2
02
0.2
06
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
00
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
B
0
0
0
0
0.4 0.2 0.4
b
3 Example : Mars Rover Markov Chain Transition Matrix , P ગ 04 $ 2 $ 3 54 $ 5 0.4 0.4 04 0.4 04 56 0 : 4 ai az 57 06 0.2 0.2 0.2 02 0.2 06 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 00 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 B 0 0 0 0 0.4 0.2 0.4 b 3 Detected in frame_454.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$2
0.4
$3
0.4
S4
$5
0.4
0.4
0.4
0.4
$6
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0 0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
b
Screening D
Screenshot Focus
"
IN IN IN Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 2 0.4 $ 3 0.4 S4 $ 5 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screening D Screenshot Focus " IN IN IN Detected in frame_455.jpg: T
Example: Mars Rover Markov Chain Transition Matrix, P
S1
0.4
$2
0.4
$3
0.4
0.4
S4
0.4
55
0:4
0.4
56
0.4
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
J
P
=
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Son M
D
Socia
NNN T Example : Mars Rover Markov Chain Transition Matrix , P S1 0.4 $ 2 0.4 $ 3 0.4 0.4 S4 0.4 55 0 : 4 0.4 56 0.4 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Son M D Socia NNN Detected in frame_456.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
0.4
$3
0.4
S4
$5
0.4
0.4
0.4
0.4
$6
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Screening Соб
Screenshot A Focus
IN INVINN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 0.4 $ 3 0.4 S4 $ 5 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screening Соб Screenshot A Focus IN INVINN Detected in frame_457.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
Sz
$3
S4
04
0.4
0.4
04
0.4
56
0.4
ai az
0.6
0.2
0.2
0.2
0.2
0.2
06
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
..
3
' Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 Sz $ 3 S4 04 0.4 0.4 04 0.4 56 0.4 ai az 0.6 0.2 0.2 0.2 0.2 0.2 06 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b .. 3 ' Detected in frame_458.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
Sz
0.4
$3
S4
55
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
S
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0 0
0.4 0.2 0.4
b
Screening Соб
Screenshot. Foctia
NININ
+ Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 Sz 0.4 $ 3 S4 55 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 S 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screening Соб Screenshot . Foctia NININ + Detected in frame_459.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
ai az
0.4
$2
0.4
الله
0.4
S4
$5
0.4
04
0.4
57
0.6
0.2
0.2
0.2
02
0.2
06
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
3 Example : Mars Rover Markov Chain Transition Matrix , P ai az 0.4 $ 2 0.4 الله 0.4 S4 $ 5 0.4 04 0.4 57 0.6 0.2 0.2 0.2 02 0.2 06 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 Detected in frame_460.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
S4
55
0.4
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4
0.2 0.4
0
0
0
P=
0
0
0.4 0.2
0.4
0
0
0
0
0 0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
b
Screen Mining
De
Screenshot Focus
NANN
" Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 S4 55 0.4 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screen Mining De Screenshot Focus NANN " Detected in frame_461.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
53
S4
$5
0.4
0.4
0.4
di az
57
0.4
04
0.6
0.2
0.2
0.2
02
0.2
0,6
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
DONNA
3 Example : Mars Rover Markov Chain Transition Matrix , P Si 53 S4 $ 5 0.4 0.4 0.4 di az 57 0.4 04 0.6 0.2 0.2 0.2 02 0.2 0,6 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 DONNA 3 Detected in frame_462.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
0.4
$3
S4
55
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2 0.4
0
0
0
J
P =
0
0
0.4 0.2
0.4
0
0
0
0
0 0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Screen Mining Do
Screenshot Focus
NANN
" Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 0.4 $ 3 S4 55 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screen Mining Do Screenshot Focus NANN " Detected in frame_463.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
di az
Si
53
S4
$5
56
57
0.4
0.4
0.4
04
0.4
0.6
0.2
0.2
0.2
02
0.2
0,6
0.6
0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
DONNA
3 Example : Mars Rover Markov Chain Transition Matrix , P di az Si 53 S4 $ 5 56 57 0.4 0.4 0.4 04 0.4 0.6 0.2 0.2 0.2 02 0.2 0,6 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 DONNA 3 Detected in frame_464.jpg: NANN
Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$5
0.4
0.4
0.4
0.4
0.4
$2
0.4
0.4
$3
0.4
S4
$6
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Screen Mirroring
Document
ScroFocus NANN Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 5 0.4 0.4 0.4 0.4 0.4 $ 2 0.4 0.4 $ 3 0.4 S4 $ 6 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screen Mirroring Document ScroFocus Detected in frame_465.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
04
$2
$3
S4
04
0.4
0.4
14
$5
0.4
04
56
0.4
di az
$7
0.6
0.2
0.2
0.2
0.2
0.2
06
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
..
15 Example : Mars Rover Markov Chain Transition Matrix , P Si 04 $ 2 $ 3 S4 04 0.4 0.4 14 $ 5 0.4 04 56 0.4 di az $ 7 0.6 0.2 0.2 0.2 0.2 0.2 06 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 .. 15 Detected in frame_466.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$2
$3
S4
0.4
0.4
0.4
0.4
0.4
0.4
$5
0.4
04
56
0.4
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Screen Mirroring Сс
Screenshot Focus
IN NINN
. Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 2 $ 3 S4 0.4 0.4 0.4 0.4 0.4 0.4 $ 5 0.4 04 56 0.4 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screen Mirroring Сс Screenshot Focus IN NINN . Detected in frame_467.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
04
Sz
$3
S4
04
0.4
0.4
14
$5
0.4
04
56
0.4
di az
$7
0.6
0.2
0.2
0.2
0.2
0.2
06
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
..
15 Example : Mars Rover Markov Chain Transition Matrix , P Si 04 Sz $ 3 S4 04 0.4 0.4 14 $ 5 0.4 04 56 0.4 di az $ 7 0.6 0.2 0.2 0.2 0.2 0.2 06 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 .. 15 Detected in frame_468.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$2
0.4
0.4
$3
0.4
S4
0.4
0.4
0.4
$5
0.4
04
56
0.4
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Screen Ming Сс
Screenshot Focus
IN NINN
. Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 2 0.4 0.4 $ 3 0.4 S4 0.4 0.4 0.4 $ 5 0.4 04 56 0.4 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screen Ming Сс Screenshot Focus IN NINN . Detected in frame_469.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
SI
0.4
$2
0.4
الله
0.4
S4
0.4
0.4
1.4
56
0.4
di az
$7
0.6
0.2
0.2
0.2
02
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
15
..
INN Example : Mars Rover Markov Chain Transition Matrix , P SI 0.4 $ 2 0.4 الله 0.4 S4 0.4 0.4 1.4 56 0.4 di az $ 7 0.6 0.2 0.2 0.2 02 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 15 .. INN Detected in frame_470.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$2
0.4
$3
S4
0.4
0.4
0.4
0.4
0.4
$6
a, az
57
0.4
04
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
Screen Mirroring
Screenshot
Focus
'
INNINN Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 2 0.4 $ 3 S4 0.4 0.4 0.4 0.4 0.4 $ 6 a , az 57 0.4 04 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screen Mirroring Screenshot Focus ' INNINN Detected in frame_471.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
SI
0.4
$2
04
الله
0.4
S4
0.4
0.4
3.4
0.4
56
0.4
di az
$7
06
0.2
0.2
0.2
02
0.2
0,6
(0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
3
INN Example : Mars Rover Markov Chain Transition Matrix , P SI 0.4 $ 2 04 الله 0.4 S4 0.4 0.4 3.4 0.4 56 0.4 di az $ 7 06 0.2 0.2 0.2 02 0.2 0,6 ( 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 INN Detected in frame_472.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$2
0.4
$3
&
S4
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
b
Screen Mirroring
Document
Screenshot
Focus
IN NINN Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 2 0.4 $ 3 & S4 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screen Mirroring Document Screenshot Focus IN NINN Detected in frame_473.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
SI
0.4
$2
04
الله
0.4
S4
0.4
0.4
3.4
0.4
56
0.4
di az
$7
06
0.2
0.2
0.2
02
0.2
0.6
(0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
3
INN Example : Mars Rover Markov Chain Transition Matrix , P SI 0.4 $ 2 04 الله 0.4 S4 0.4 0.4 3.4 0.4 56 0.4 di az $ 7 06 0.2 0.2 0.2 02 0.2 0.6 ( 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 INN Detected in frame_474.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
Sz
0.4
$3
S4
0.4
0.4
0.4
0.4
0.4
$6
9, az
57
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0 0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
b
Screening
Screenshot Focus
"
INNINN Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 Sz 0.4 $ 3 S4 0.4 0.4 0.4 0.4 0.4 $ 6 9 , az 57 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screening Screenshot Focus " INNINN Detected in frame_475.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
04
$3
0.4
04
54
0.4
14
$5
LA
$6
ai az
ST
0.4
04
0.6
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
S
15 Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 04 $ 3 0.4 04 54 0.4 14 $ 5 LA $ 6 ai az ST 0.4 04 0.6 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S 15 Detected in frame_476.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
Sz
0.4
$3
0.4
S4
55
0.4
0.4
0.4
0.4
04
$6
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
J
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0 0.4 0.2 0.4
b
Screening
Document
Screenshot Focus
NNNN
" Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 Sz 0.4 $ 3 0.4 S4 55 0.4 0.4 0.4 0.4 04 $ 6 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screening Document Screenshot Focus NNNN " Detected in frame_477.jpg: [
Example: Mars Rover Markov Chain Transition Matrix, P
a, az
Si
0.4
$2
04
$3
0.4
04
54
0.4
$5
56
ST
0.4
14
04
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
b
S
'
15 [ Example : Mars Rover Markov Chain Transition Matrix , P a , az Si 0.4 $ 2 04 $ 3 0.4 04 54 0.4 $ 5 56 ST 0.4 14 04 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S ' 15 Detected in frame_478.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
E
S1
0.4
Sz
0.4
53
0.4
0.4
S4
0.4
0.4
55
0:4
56
0.4
0.4
9, az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0 0
0.4 0.2 0.4
b
Son M
SP
NINA
N Example : Mars Rover Markov Chain Transition Matrix , P E S1 0.4 Sz 0.4 53 0.4 0.4 S4 0.4 0.4 55 0 : 4 56 0.4 0.4 9 , az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Son M SP NINA N Detected in frame_479.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
53
&
54
55
0.4
0.4
0:4
04
04
56
0.4
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4
0.2 0.4
0
0
0.
0
0
0.4 0.2 0.4
b
Screen M
Dic
Screens
IN
NNN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 53 & 54 55 0.4 0.4 0 : 4 04 04 56 0.4 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 b Screen M Dic Screens IN NNN Detected in frame_480.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
$3
&
56
9, az
54
55
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
Cooopo
0.6
0.2
0.2
0.2
0.2
0.2
0.6
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4 0
0
0
0
0
0.4
0.2 0.4
0
0
0.
0
0
0.4
0.2 0.4
b
S
NININ Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 $ 3 & 56 9 , az 54 55 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 Cooopo 0.6 0.2 0.2 0.2 0.2 0.2 0.6 S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 b S NININ Detected in frame_481.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
ST
Sz
$3
SA
55
00
L
56
9, az
S
Compos
1.2
0.2
0.2
11.2
02
06
10.6 0.4 0
0
0
0
0
S₁
0.4 0.2 0.4
0
0
0
0
D
0
0.4 0.2
0.4
0
0
0
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
' Example : Mars Rover Markov Chain Transition Matrix , P ST Sz $ 3 SA 55 00 L 56 9 , az S Compos 1.2 0.2 0.2 11.2 02 06 10.6 0.4 0 0 0 0 0 S₁ 0.4 0.2 0.4 0 0 0 0 D 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 ' Detected in frame_482.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
a, az
Si
0.4
$2
0.4
53
$4
$5
56
57
0.4
0.4
0.4
0.4
04
04
0.4
06
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4
0.2 0.4
b
S
Cooopool
N Example : Mars Rover Markov Chain Transition Matrix , P a , az Si 0.4 $ 2 0.4 53 $ 4 $ 5 56 57 0.4 0.4 0.4 0.4 04 04 0.4 06 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S Cooopool N Detected in frame_483.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
Ex
a, az
S4
55
56
57
0.4
0.4
0.4
0.4
0.4
0.4
04
0.4
Coorpool
a
0.6
0.2
0.2
0.2
0.2
0.2
0.6
5,
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
J
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Screen M
D
Street Fo
NININN
' Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 Ex a , az S4 55 56 57 0.4 0.4 0.4 0.4 0.4 0.4 04 0.4 Coorpool a 0.6 0.2 0.2 0.2 0.2 0.2 0.6 5 , 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screen M D Street Fo NININN ' Detected in frame_484.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S1
0.4
$2
0.4
53
&
S4
0.4
0.4
0.4
0.4
04
0.4
$6
0.4
9, az
57
P
0.6
-
0.2
0.2
0.2
0.2
0.2
0.6
९
70.6 0.4 0
0
S₁
0
0
0
0.4 0.2 0.4
0.
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0 0.4 0.2 0.4
b
Screening D
Shot Focus
'
IN IN INN
. Example : Mars Rover Markov Chain Transition Matrix , P S1 0.4 $ 2 0.4 53 & S4 0.4 0.4 0.4 0.4 04 0.4 $ 6 0.4 9 , az 57 P 0.6 - 0.2 0.2 0.2 0.2 0.2 0.6 ९ 70.6 0.4 0 0 S₁ 0 0 0 0.4 0.2 0.4 0 . 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screening D Shot Focus ' IN IN INN . Detected in frame_485.jpg: T
Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$2
P
0.6
-
Cooopool
0.4
$3
&
9, az
S4
$5
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.2
0.2
0.2
0.2
0.2
0.6
S'
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2
0.4
0
0
0
J
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4 0.2
0.4
b
NNN
N T Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 2 P 0.6 - Cooopool 0.4 $ 3 & 9 , az S4 $ 5 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.2 0.2 0.2 0.2 0.2 0.6 S ' 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b NNN N Detected in frame_486.jpg: NNIN N
Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$2
0.4
$3
34
S4
9, az
55
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
S'
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
a
Cooopon
SES NNIN N Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 2 0.4 $ 3 34 S4 9 , az 55 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 S ' 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b a Cooopon SES Detected in frame_487.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Coopool
a
Si
0.4
$2
ai az
53
S4
$5
Si
$7
04
0.4
0.4
04
14
04
0.6
0.2
0.2
0.2
02
0.2
0,6
S'
10.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
.0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
IN IN N
15 Example : Mars Rover Markov Chain Transition Matrix , P Coopool a Si 0.4 $ 2 ai az 53 S4 $ 5 Si $ 7 04 0.4 0.4 04 14 04 0.6 0.2 0.2 0.2 02 0.2 0,6 S ' 10.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 IN IN N 15 Detected in frame_488.jpg: T
Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
52
9, az
$3
S4
$5
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
NNN
0.6
0.2
0.2
0.2
0.2
0.2
0.6
P
S'
70.6 0.4 0
0
0
0
S₁
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4
0.2
0.4
0
0
0
J
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
b
Foc
Cooopool
N T Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 52 9 , az $ 3 S4 $ 5 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 NNN 0.6 0.2 0.2 0.2 0.2 0.2 0.6 P S ' 70.6 0.4 0 0 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Foc Cooopool N Detected in frame_489.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
S4
0.4
0.4
0.4
0.4
$5
0.4
0.4
$6
0.4
10.4
a, az
57
0.6
a
P
-
Cooopool
NNN NI
0.2
0.2
0.2
0.2
0.2
0.6
S'
0.6 0.4 0
0
0
0
S₁
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
b
Ser
' Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 S4 0.4 0.4 0.4 0.4 $ 5 0.4 0.4 $ 6 0.4 10.4 a , az 57 0.6 a P - Cooopool NNN NI 0.2 0.2 0.2 0.2 0.2 0.6 S ' 0.6 0.4 0 0 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Ser ' Detected in frame_490.jpg: 9, az
Example: Mars Rover Markov Chain Transition Matrix. P
Sz
الله
00
SA
$5
4
S
0.5
па
1.2
0.2
112
02
3'
10.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0.4
0.2 0.4
0
0
0
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
a
Compost 9 , az Example : Mars Rover Markov Chain Transition Matrix . P Sz الله 00 SA $ 5 4 S 0.5 па 1.2 0.2 112 02 3 ' 10.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 a Compost Detected in frame_491.jpg: NNN
Example: Mars Rover Markov Chain Transition Matrix, P
51
0.4
$2
0.4
53
0.4
04
S4
0.4
55
04
0.4
$6
0.4
a, az
ST
0.6
a
-
Cooopool
0.2
0.2
0.2
0.2
0.2
0.6
S'
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Som f NNN Example : Mars Rover Markov Chain Transition Matrix , P 51 0.4 $ 2 0.4 53 0.4 04 S4 0.4 55 04 0.4 $ 6 0.4 a , az ST 0.6 a - Cooopool 0.2 0.2 0.2 0.2 0.2 0.6 S ' /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Som f Detected in frame_492.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
0.4
&
S4
55
$6
a, az
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
a
0.6
0.2
0.2
0.2
0.2
0.2
0.6
-
S'
0.6 0.4 0
0
0
0
0
S₁
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
b
Screening
Doct
Screenshot Fo
NININ Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 0.4 & S4 55 $ 6 a , az 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 a 0.6 0.2 0.2 0.2 0.2 0.2 0.6 - S ' 0.6 0.4 0 0 0 0 0 S₁ 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screening Doct Screenshot Fo NININ Detected in frame_493.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
51
0.4
$2
0.6
P
-
Cooopool
0.4
$3
ai az
S4
55
56
ST
0.4
0.4
0:4
0.4
0.4
0.4
0.4
0.2
0.2
0.2
0.2
0.2
0.6
S'
70.6 0.4 0
0
0
0
S₁
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Sco
NNN
N Example : Mars Rover Markov Chain Transition Matrix , P 51 0.4 $ 2 0.6 P - Cooopool 0.4 $ 3 ai az S4 55 56 ST 0.4 0.4 0 : 4 0.4 0.4 0.4 0.4 0.2 0.2 0.2 0.2 0.2 0.6 S ' 70.6 0.4 0 0 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Sco NNN N Detected in frame_494.jpg: NIN
Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
24
S4
$5
56
9, az
57
0.4
0.4
04
0.4
04
0.4
04
P
0.6
-
Cooopool
0.2
0.2
0.2
0.2
0.2
0.6
S'
70.6 0.4) 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
b NIN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 24 S4 $ 5 56 9 , az 57 0.4 0.4 04 0.4 04 0.4 04 P 0.6 - Cooopool 0.2 0.2 0.2 0.2 0.2 0.6 S ' 70.6 0.4 ) 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Detected in frame_495.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
9, 92
ST
Sz
a
Coorpool
LA
53
$4
$5
$7
00
LLA
12
9.2
02
11.2
0.2
S'
10.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0.4 0.2
0.4
0
0
0
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4
0.2 0.4
b Example : Mars Rover Markov Chain Transition Matrix , P 9 , 92 ST Sz a Coorpool LA 53 $ 4 $ 5 $ 7 00 LLA 12 9.2 02 11.2 0.2 S ' 10.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Detected in frame_496.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
a az
$3
54
$5
$6
57
04
0.4
0.4
0.4
0.4
Si
0.4
$2
0.6
a
Cooopon
-
NNN
0.2
0.2
0.2
0.2
0.2
0,6
S'
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
b
3 Example : Mars Rover Markov Chain Transition Matrix , P a az $ 3 54 $ 5 $ 6 57 04 0.4 0.4 0.4 0.4 Si 0.4 $ 2 0.6 a Cooopon - NNN 0.2 0.2 0.2 0.2 0.2 0,6 S ' 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b 3 Detected in frame_497.jpg: ●NNA
Example: Mars Rover Markov Chain Transition Matrix, P
S₁
0.4
$2
0.4
53
0.4
0.4
Ex
S4
0.4
04
55
04
0.4
56
0.4
9, az
a
0.6
Cooo pool
-
0.2
0.2
0.2
0.2
0.2
0.6
S'
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Semi
Simmbad facia ● NNA Example : Mars Rover Markov Chain Transition Matrix , P S₁ 0.4 $ 2 0.4 53 0.4 0.4 Ex S4 0.4 04 55 04 0.4 56 0.4 9 , az a 0.6 Cooo pool - 0.2 0.2 0.2 0.2 0.2 0.6 S ' 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Semi Simmbad facia Detected in frame_498.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
04
$2
a
Coopool
04
الله
di az
0.4
54
$5
56
0.4
0.4
10.4
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
06
S'
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0:4
b
NINA
3 Example : Mars Rover Markov Chain Transition Matrix , P Si 04 $ 2 a Coopool 04 الله di az 0.4 54 $ 5 56 0.4 0.4 10.4 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 06 S ' 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0 : 4 b NINA 3 Detected in frame_499.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
53
0.4
0.4
Ex
54
0.4
ai az
56
0.4
0.4
0.4
55
Cooopool
0.6
P
-
0.2
0.2
0.2
0.2
0.2
0.6
S'
S₁
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4 0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
b
Sc
NNNN Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 53 0.4 0.4 Ex 54 0.4 ai az 56 0.4 0.4 0.4 55 Cooopool 0.6 P - 0.2 0.2 0.2 0.2 0.2 0.6 S ' S₁ /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Sc NNNN Detected in frame_500.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
$3
☑
S4
0.4
0.4
0.4
0.4
55
0.4
0.4
56
0.4
9, az
a
0.6
0.2
0.2
0.2
0.2
0.2
:0.6
-
S'
0.6 0.4 0
0
0
0
0
S₁
0.4 0.2 0.4
0
0
0
0
อ
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4 0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
NINN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 $ 3 ☑ S4 0.4 0.4 0.4 0.4 55 0.4 0.4 56 0.4 9 , az a 0.6 0.2 0.2 0.2 0.2 0.2 : 0.6 - S ' 0.6 0.4 0 0 0 0 0 S₁ 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NINN Detected in frame_501.jpg: Compos
Example: Mars Rover Markov Chain Transition Matrix, P
a, az
Sz
الله
0
$4
$5
ST
06
1.2
9:2
0.2
112
0:2
a
P.
'3'
10.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
D
0 0.4 0.2
0.4
0
0
0
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4 0
0
0
0
0
0.4
0.2 0.4
b
' Compos Example : Mars Rover Markov Chain Transition Matrix , P a , az Sz الله 0 $ 4 $ 5 ST 06 1.2 9 : 2 0.2 112 0 : 2 a P. ' 3 ' 10.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 D 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b ' Detected in frame_502.jpg: NNN
Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.41
$2
04
$3
54
0.4
0.4
04
0.4
56
0:4
a az
57
0.6
a
-
Coorpool
15
0.2
0.2
0.2
0.2
0.2
0.6
S'
0.6 0.4 0
0
0
0
S₁
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
b
Se NNN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.41 $ 2 04 $ 3 54 0.4 0.4 04 0.4 56 0 : 4 a az 57 0.6 a - Coorpool 15 0.2 0.2 0.2 0.2 0.2 0.6 S ' 0.6 0.4 0 0 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Se Detected in frame_503.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
51
0.4
$2
0.4
53
0.4
0.4
Ex
54
0.4
0.4
55
ai az
56
04
0.4
Coooooo
0.6
P
-
0.2
0.2
0.2
0.2
0.2
06
S'
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2 0.4
0
0
0
J
P =
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4
0.2 0.4
0
0
0
0 0
0.4 0.2 0.4
b
Вос
Sca
NNN Example : Mars Rover Markov Chain Transition Matrix , P 51 0.4 $ 2 0.4 53 0.4 0.4 Ex 54 0.4 0.4 55 ai az 56 04 0.4 Coooooo 0.6 P - 0.2 0.2 0.2 0.2 0.2 06 S ' 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Вос Sca NNN Detected in frame_504.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
$3
0.4
0.4
54
0.4
0.4
55
04
56
0.4
0.4
ai az
ST
Cooopoo
P
0.6
-
0.2
0.2
0.2
0.2
0.2
0.6
S'
0.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0.
0
0 0.4
0.2
0.4
b
Fons
NININA Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 $ 3 0.4 0.4 54 0.4 0.4 55 04 56 0.4 0.4 ai az ST Cooopoo P 0.6 - 0.2 0.2 0.2 0.2 0.2 0.6 S ' 0.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 b Fons NININA Detected in frame_505.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
53
&
S4
0.4
0.4
0.4
0.4
0.4
56
9, az
57
0.4
0.4
Coopool
0.6
P
-
0.2
0.2
0.2
0.2
0.2
0.6
S'
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
NNNN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 53 & S4 0.4 0.4 0.4 0.4 0.4 56 9 , az 57 0.4 0.4 Coopool 0.6 P - 0.2 0.2 0.2 0.2 0.2 0.6 S ' /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NNNN Detected in frame_506.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
0.4
$2
$3
54
$5
04
0.4
04
0.4
56
0.4
di az
$7
0.6
a
P
-
0.2
0.2
0.2
02
0.2
06
S'
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4 Example : Mars Rover Markov Chain Transition Matrix , P 0.4 $ 2 $ 3 54 $ 5 04 0.4 04 0.4 56 0.4 di az $ 7 0.6 a P - 0.2 0.2 0.2 02 0.2 06 S ' 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Detected in frame_507.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
52
0.6
a
Cooopool
-
0.4
$3
S4
0.4
0.4
04
0.4
0.4
56
0.4
9 az
57
0.2
0.2
0.2
0.2
0.2
0.6
S'
0.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0.
0
0 0.4
0.2 0.4
b
SM
100 Леска
IN NINN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 52 0.6 a Cooopool - 0.4 $ 3 S4 0.4 0.4 04 0.4 0.4 56 0.4 9 az 57 0.2 0.2 0.2 0.2 0.2 0.6 S ' 0.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 b SM 100 Леска IN NINN Detected in frame_508.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
0.4
$3
&
S4
55
0.4
0.4
0.4
0.4
0.4
04
56
0.4
9, az
57
Coorpool
0.6
a
P
-
0.2
0.2
0.2
0.2
0.2
0.6
S'
70.6 0.41 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
NNN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 0.4 $ 3 & S4 55 0.4 0.4 0.4 0.4 0.4 04 56 0.4 9 , az 57 Coorpool 0.6 a P - 0.2 0.2 0.2 0.2 0.2 0.6 S ' 70.6 0.41 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NNN Detected in frame_509.jpg: Example. Mars Rover Markov Chain Transition Matrix, P
온
Sz
53
SA
9, 42
55
56
ST
a
06
P.
Compost
1.Z
0.2
LA
0:2
S'
/0.6 0.4 0
0
0
51
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
ปี
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4 Example . Mars Rover Markov Chain Transition Matrix , P 온 Sz 53 SA 9 , 42 55 56 ST a 06 P. Compost 1.Z 0.2 LA 0 : 2 S ' /0.6 0.4 0 0 0 51 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 ปี P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Detected in frame_510.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
04
$3
0.4
0.4
&
54
0.4
a, az
56
04
0.4
0.4
$5
a
0.6
0.2
0.2
0.2
0.2
0.2
0.6
-
S'
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2 0.4
S
15
NNNN Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 04 $ 3 0.4 0.4 & 54 0.4 a , az 56 04 0.4 0.4 $ 5 a 0.6 0.2 0.2 0.2 0.2 0.2 0.6 - S ' S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S 15 NNNN Detected in frame_511.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S1
0.4
$2
0.4
$3
0.4
Ex
S4
0.4
55
0.4
56
0.4
ai az
ST
Cooopoo
0.6
a
P
-
0.4
0.4
0.4
0.2
0.2
0.2
0.2
0.2
0.6
S'
0.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2 0.4
0
0
0
J
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Son M
Screenshot Facu
NNN Example : Mars Rover Markov Chain Transition Matrix , P S1 0.4 $ 2 0.4 $ 3 0.4 Ex S4 0.4 55 0.4 56 0.4 ai az ST Cooopoo 0.6 a P - 0.4 0.4 0.4 0.2 0.2 0.2 0.2 0.2 0.6 S ' 0.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Son M Screenshot Facu NNN Detected in frame_512.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Si
0.4
$2
04
a
الله
di az
S4
55
56
ST
0.4
0.4
04
14
04
0.6
0.2
0.2
0.2
0.2
0.2
06
S'
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
NNA
3
4 Example : Mars Rover Markov Chain Transition Matrix , P Si 0.4 $ 2 04 a الله di az S4 55 56 ST 0.4 0.4 04 14 04 0.6 0.2 0.2 0.2 0.2 0.2 06 S ' 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NNA 3 4 Detected in frame_513.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
S1
0.4
$2
0.4
$3
0.4
Ex
S4
0.4
55
0.4
56
0.4
ai az
ST
Coorpoo
a
0.6
-
0.4
0.4
0.4
0.2
0.2
0.2
0.2
0.2
0.6
S'
S₁
70.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2 0.4
0
0
0
J
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Son M
Screenshot Facu
NNN Example : Mars Rover Markov Chain Transition Matrix , P S1 0.4 $ 2 0.4 $ 3 0.4 Ex S4 0.4 55 0.4 56 0.4 ai az ST Coorpoo a 0.6 - 0.4 0.4 0.4 0.2 0.2 0.2 0.2 0.2 0.6 S ' S₁ 70.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Son M Screenshot Facu NNN Detected in frame_514.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
$1
0.4
$2
0.4
$3
0.4
54
56
a, az
55
57
0.4
0.4
0.4
0.4
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
a
P
-
S'
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
NNNN
• Example : Mars Rover Markov Chain Transition Matrix , P $ 1 0.4 $ 2 0.4 $ 3 0.4 54 56 a , az 55 57 0.4 0.4 0.4 0.4 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 a P - S ' /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NNNN • Detected in frame_515.jpg: Example. Mars Rover Markov Chain Transition Matrix. P
a, az
$1
Sz
53
SA
55
ST
A
Ша
24
06
1.2
1.Z
0.2
8.2
02
a
P.
S'
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
Coorpool Example . Mars Rover Markov Chain Transition Matrix . P a , az $ 1 Sz 53 SA 55 ST A Ша 24 06 1.2 1.Z 0.2 8.2 02 a P. S ' /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Coorpool Detected in frame_525.jpg: Example. Mars Rover Markov Chain Transition Matrix. P
PC
5T
53
1012
A
55
$7
8.2
.02
f
3'
/0.6 0.4 0 0
0
0
0
0.4 0.2 0.4
0
0
0
0
0.4
0.2 0.4
0
D
0
P=
0
0
04
02 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
D
0
0
0.4
0.2 0.4
b
Lབབ་phe
N SEND Example . Mars Rover Markov Chain Transition Matrix . P PC 5T 53 1012 A 55 $ 7 8.2 .02 f 3 ' /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 D 0 P = 0 0 04 02 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 D 0 0 0.4 0.2 0.4 b L བབ་ phe N SEND Detected in frame_526.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Pls
$1
0.4
55
56
ai az
53
S4
ST
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
Cooopool
a
0.6
-
0.2
0.2
0.2
0.2
0.2
0.6
S'
0.6 0.4 0
0
0
0
S₁
0
0.4 0.2 0.4
0
0
0
0
อ
0 0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0 0
0
0
0.4 0.2 0.4
Screening
SF
NNN
IS Example : Mars Rover Markov Chain Transition Matrix , P Pls $ 1 0.4 55 56 ai az 53 S4 ST 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 Cooopool a 0.6 - 0.2 0.2 0.2 0.2 0.2 0.6 S ' 0.6 0.4 0 0 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening SF NNN IS Detected in frame_527.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
Pls,
Si
0.4
Sz
34
S4
$3
0.4
0.4
0.4
0.4
0.4
$5
04
0.4
56
0.4
a, az
57
a
1000000
0.6
0.2
0.2
0.2
0.2
0.2
0.6
-
S'
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
IN
NNNN Example : Mars Rover Markov Chain Transition Matrix , P Pls , Si 0.4 Sz 34 S4 $ 3 0.4 0.4 0.4 0.4 0.4 $ 5 04 0.4 56 0.4 a , az 57 a 1000000 0.6 0.2 0.2 0.2 0.2 0.2 0.6 - S ' 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b IN NNNN Detected in frame_528.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 15,
&
Sz
53
SA
m
55
04
56
α, az
S
0.2
9.2
02
IZ
0.2
3'
S₁
10.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P=
0
0 0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
#
0
0
0
0
0.4
0.2 0.4
a
рок
Coorpos Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 15 , & Sz 53 SA m 55 04 56 α , az S 0.2 9.2 02 IZ 0.2 3 ' S₁ 10.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 # 0 0 0 0 0.4 0.2 0.4 a рок Coorpos Detected in frame_529.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 1s)
$1
04
$2
옩
$3
SA
04
04
0.4
0.4
$5
04
a, az
56
0.4
a
-
0.6
02
0.2
0.2
0.2
0.2
0,6
S'
0.6 0.4 0
0
0
0
S₁
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
b
15
NNNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 1s ) $ 1 04 $ 2 옩 $ 3 SA 04 04 0.4 0.4 $ 5 04 a , az 56 0.4 a - 0.6 02 0.2 0.2 0.2 0.2 0,6 S ' 0.6 0.4 0 0 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b 15 NNNN Detected in frame_530.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=
S1
0.4
$2
0.4
Ex
53
S4
0.4
0.4
0.4
0.4
0.4
0.4
56
0.4
a, az
ST
Coopoo
a
P
0.6
0.2
0.2
0.2
0.2
0.2
0.6
S'
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Scre
Document
Schet Fo
IN IN IN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = S1 0.4 $ 2 0.4 Ex 53 S4 0.4 0.4 0.4 0.4 0.4 0.4 56 0.4 a , az ST Coopoo a P 0.6 0.2 0.2 0.2 0.2 0.2 0.6 S ' 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Scre Document Schet Fo IN IN IN Detected in frame_531.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
Si
04
S₂
53
54
$5
56
04
0.4
04
04.
0.4
ai az
57
3
0.6
0.2
0.2
0.2
0.2
0.2
0,6
S'
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
.0
0
0.4
0.2 0.4 0
0
0
0
0
0.4
0.2 0.4
a Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 Si 04 S₂ 53 54 $ 5 56 04 0.4 04 04 . 0.4 ai az 57 3 0.6 0.2 0.2 0.2 0.2 0.2 0,6 S ' 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 a Detected in frame_532.jpg: NANON
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
S₂
0.4
$3
0.4
0.4
&
54
0.4
$5
0.4
0.4
56
0.4
ai az
57
0.6
a
P
-
Cooopool
0.2
0.2
0.2
0.2
0.2
0.6
S'
S₁
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4 0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Волки на 150 Россия NANON Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 S₂ 0.4 $ 3 0.4 0.4 & 54 0.4 $ 5 0.4 0.4 56 0.4 ai az 57 0.6 a P - Cooopool 0.2 0.2 0.2 0.2 0.2 0.6 S ' S₁ /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Волки на 150 Россия Detected in frame_533.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
0.4
$2
0.4
$3
54
0.4
0.4
0.4
0.4
55
0.4
0.4
$6
0.4
9, az
57
a
Coooooo!
0.6
0.2
0.2
0.2
0.2
0.2
0.6
S'
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
S
Do
Schet Foc
IN İNİN N Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 0.4 $ 2 0.4 $ 3 54 0.4 0.4 0.4 0.4 55 0.4 0.4 $ 6 0.4 9 , az 57 a Coooooo ! 0.6 0.2 0.2 0.2 0.2 0.2 0.6 S ' 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S Do Schet Foc IN İNİN N Detected in frame_534.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
Si
04
S₂
53
04
04
Ex
54
$5
0.4
0.4
04
56
0.4
ai az
57
3
0.6
0.2
0.2
0.2
0.2
0.2
0,6
S'
0.6 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
.0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
a Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 Si 04 S₂ 53 04 04 Ex 54 $ 5 0.4 0.4 04 56 0.4 ai az 57 3 0.6 0.2 0.2 0.2 0.2 0.2 0,6 S ' 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 a Detected in frame_535.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
04
S₂
0.4
$3
9, az
54
56
0.4
0.4
04
0.4
0.4
0.4
0.4
a
0.6
0.2
0.2
0.2
0.2
0.2
0.6
-
S'
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
b
S
NIN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 04 S₂ 0.4 $ 3 9 , az 54 56 0.4 0.4 04 0.4 0.4 0.4 0.4 a 0.6 0.2 0.2 0.2 0.2 0.2 0.6 - S ' S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S NIN Detected in frame_536.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
0.4
$3
0.4
24
S4
$6
9, az
57
0.4
0.4
0.4
0.4
0.4
04
0.4
Cooopool
0.6
a
P
-
0.2
0.2
0.2
0.2
0.2
0.6
S'
S₁
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
b
Soren Ming Docu
She Focus
NNNN
.
' Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 0.4 $ 3 0.4 24 S4 $ 6 9 , az 57 0.4 0.4 0.4 0.4 0.4 04 0.4 Cooopool 0.6 a P - 0.2 0.2 0.2 0.2 0.2 0.6 S ' S₁ /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Soren Ming Docu She Focus NNNN . ' Detected in frame_537.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
$3
54
55
04
0.4
0.4
04
0.4
04
56
0.4
di az
.
3
'
0.6
0.2
0.2
0.2
0.2
0.2
0,6
S'
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0 0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
a
Cocopool Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 $ 3 54 55 04 0.4 0.4 04 0.4 04 56 0.4 di az . 3 ' 0.6 0.2 0.2 0.2 0.2 0.2 0,6 S ' 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 a Cocopool Detected in frame_538.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
$3
0.4
0.4
54
0.4
0.4
55
0.4
56
0.4
ai az
57
Coooooo!
0.6
a
P
-
0.2
0.2
0.2
0.2
0.2
0.6
S'
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
b
IN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 $ 3 0.4 0.4 54 0.4 0.4 55 0.4 56 0.4 ai az 57 Coooooo ! 0.6 a P - 0.2 0.2 0.2 0.2 0.2 0.6 S ' S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b IN Detected in frame_539.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
$3
0.4
84
S4
0.4
55
9, az
56
57
0.4
0.4
04
0.4
a
06
0.2
0.2
0.2
0.2
0.2
06
S'
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0.4
0.2 0.4
0
0
0
P=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
'
15
NIN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 $ 3 0.4 84 S4 0.4 55 9 , az 56 57 0.4 0.4 04 0.4 a 06 0.2 0.2 0.2 0.2 0.2 06 S ' 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 ' 15 NIN Detected in frame_540.jpg: Example. Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
9, az
ST
Sz
53
الله
04
SA
$5
114
5
1.2
0.2
11.2
0.2
06
a
P.
'3'
10.6 0.4 0
0
0
0
0
0.4 0.2 0:4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
ง
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
.0
0
0
0.4 0.2
0.4
b Example . Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 9 , az ST Sz 53 الله 04 SA $ 5 114 5 1.2 0.2 11.2 0.2 06 a P. ' 3 ' 10.6 0.4 0 0 0 0 0 0.4 0.2 0 : 4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 ง P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 .0 0 0 0.4 0.2 0.4 b Detected in frame_541.jpg: NNN
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
$2
✓
53
54
0.4
0.4
04
0.4
S5
04
0.4
56
0.4
a, az
57
..
0.6
a
-
0.2
0.2
0.2
0.2
0.2
0.6
S'
S₁
70.6 0.4. 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
Se
Cooopool NNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 $ 2 ✓ 53 54 0.4 0.4 04 0.4 S5 04 0.4 56 0.4 a , az 57 .. 0.6 a - 0.2 0.2 0.2 0.2 0.2 0.6 S ' S₁ 70.6 0.4 . 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Se Cooopool Detected in frame_542.jpg: NA
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
54
ai az
$3
$5
56
57
04
0.4
0.4
D.4
04
1.4
0.4
..
15
•
0.6
0.2
0.2
0.2
0.2
0.2
0.6
S'
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
.0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
a
Cooopon NA Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 54 ai az $ 3 $ 5 56 57 04 0.4 0.4 D.4 04 1.4 0.4 .. 15 • 0.6 0.2 0.2 0.2 0.2 0.2 0.6 S ' 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 a Cooopon Detected in frame_543.jpg: NNN
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
$2
✓
53
54
0.4
0.4
04
0.4
S5
04
0.4
56
0.4
a, az
57
..
0.6
a
-
0.2
0.2
0.2
0.2
0.2
0.6
S'
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4
0.2 0.4
Cooopool NNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 $ 2 ✓ 53 54 0.4 0.4 04 0.4 S5 04 0.4 56 0.4 a , az 57 .. 0.6 a - 0.2 0.2 0.2 0.2 0.2 0.6 S ' S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Cooopool Detected in frame_544.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
$3
0.4
0.4
54
0.4
55
04
0.4
$6
0.4
ai az
57
Cooopool
a
0.6
-
0.2
0.2
0.2
0.2
0.2
0.6
S'
0.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
Sc
NNN
IN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 $ 3 0.4 0.4 54 0.4 55 04 0.4 $ 6 0.4 ai az 57 Cooopool a 0.6 - 0.2 0.2 0.2 0.2 0.2 0.6 S ' 0.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Sc NNN IN Detected in frame_545.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
04
$3
0.4
S4
$5
0.4
0.4
0.4
56
0.4
a, az
0.6
0.21
0.2
0.2
02
0.2
0.6
a
P
Cocopool
S'
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
3 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 04 $ 3 0.4 S4 $ 5 0.4 0.4 0.4 56 0.4 a , az 0.6 0.21 0.2 0.2 02 0.2 0.6 a P Cocopool S ' 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 Detected in frame_546.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
$3
0.4
0.4
54
0.4
55
0.4
0.4
56
0.4
0.4
ai az
57
Coorpool
a
0.6
-
0.2
0.2
0.2
02
0.2
0.6
S'
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2 0.4
0
0
0
J
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
b
NNN
IS Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 $ 3 0.4 0.4 54 0.4 55 0.4 0.4 56 0.4 0.4 ai az 57 Coorpool a 0.6 - 0.2 0.2 0.2 02 0.2 0.6 S ' 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b NNN IS Detected in frame_547.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
S₂
0.4
$3
0.4
S4
0.4
0.4
0.4
10.4
0.4
$6
0.4
0.4
a, az
57
a
0.6
Cooooool
0.2
0.2
0.2
0.2
0.2
0.6
IN IN IN IN
-
S'
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4
0.2
0.4
0
0
0
J
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4 0
0
0
0
0 0.4
0.2
0.4
b
Screening
Document
Foca Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 S₂ 0.4 $ 3 0.4 S4 0.4 0.4 0.4 10.4 0.4 $ 6 0.4 0.4 a , az 57 a 0.6 Cooooool 0.2 0.2 0.2 0.2 0.2 0.6 IN IN IN IN - S ' 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screening Document Foca Detected in frame_548.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
04
$3
0.4
S4
$5
0.4
0.4
0.4
56
0:4
a, az
ST
0.6
0.21
0.2
0.2
02
0.2
0,6
a
P
. .
S'
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Cooopool
3 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 04 $ 3 0.4 S4 $ 5 0.4 0.4 0.4 56 0 : 4 a , az ST 0.6 0.21 0.2 0.2 02 0.2 0,6 a P . . S ' 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Cooopool 3 Detected in frame_549.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
$3
54
0.4
0.4
0.4
55
0.4
0.4
56
0.4
0.4
ai az
57
Cooopoo
a
0.6
-
0.2
0.2
0.2
02
0.2
0.6
S'
S₁
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0 0.4 0.2 0.4
0
0
0
J
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
b
Froze
NNN
IS Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 $ 3 54 0.4 0.4 0.4 55 0.4 0.4 56 0.4 0.4 ai az 57 Cooopoo a 0.6 - 0.2 0.2 0.2 02 0.2 0.6 S ' S₁ /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Froze NNN IS Detected in frame_550.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
$3
E
$6
9, az
S4
$5
57
0.4
0.4
0.4
0.4
0.4
0.4
04
0.4
Coorpool
a
P
0.6
-
0.2
0.2
0.2
0.2
0.2
0.6
S'
S₁
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
J
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
b
Screening Doct
Screen Focu
IN IN IN N
NNNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 $ 3 E $ 6 9 , az S4 $ 5 57 0.4 0.4 0.4 0.4 0.4 0.4 04 0.4 Coorpool a P 0.6 - 0.2 0.2 0.2 0.2 0.2 0.6 S ' S₁ /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screening Doct Screen Focu IN IN IN N NNNN Detected in frame_551.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
a; az
P(S. 15)=.6
Si
$2
$3
S4
55
$6
0.4
0.4
0.4
14
04
a
Cooopool
06
-
0.4
$7
0.2
0.2
0.2
02
0.2
06
S'
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
.0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
15 Example : Mars Rover Markov Chain Transition Matrix , P a ; az P ( S . 15 ) = . 6 Si $ 2 $ 3 S4 55 $ 6 0.4 0.4 0.4 14 04 a Cooopool 06 - 0.4 $ 7 0.2 0.2 0.2 02 0.2 06 S ' 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 15 Detected in frame_552.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
$2
0.6
a
P
-
0.4
$3
0.4
0.4
9, az
54
0.4
55
56
$7
0.4
0.4
0.4
0.4
0.2
0.2
0.2
0.2
0.2
0.6
S'
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4 0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
b
S
NININ Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 $ 2 0.6 a P - 0.4 $ 3 0.4 0.4 9 , az 54 0.4 55 56 $ 7 0.4 0.4 0.4 0.4 0.2 0.2 0.2 0.2 0.2 0.6 S ' S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S NININ Detected in frame_553.jpg: Coorpool
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
&
$3
54
0.4
0.4
0.4
0.4
$5
04
04
56
0.4
a, az
ST
0.6
P
-
0.2
0.2
0.2.
0.2
0.2
0,6
'S'
0.6 0.4
0
0
0
0
S₁
0
0.4 0.2
0.4
0
0
0
0
0.4
0.2 0.4
0
0
0
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
'
15
NNNN Coorpool Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 & $ 3 54 0.4 0.4 0.4 0.4 $ 5 04 04 56 0.4 a , az ST 0.6 P - 0.2 0.2 0.2 . 0.2 0.2 0,6 ' S ' 0.6 0.4 0 0 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 ' 15 NNNN Detected in frame_554.jpg: Example: Mars Rover Markov Chain Transition Matrix. P
P(s. 15)=.6
$1
Sz
53
&
9, az
SA
55
56
$7
i
a
Coaspoo
00
04
1.2
0.2
0.2
21.2
0:2
S'
/0.6 0.4 0
0
0
0
0
S₁
0.4 0.2 0.4
0
0
0
0
0.4 0.2
0.4
0
0
0
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4
0.2 0.4 Example : Mars Rover Markov Chain Transition Matrix . P P ( s . 15 ) = . 6 $ 1 Sz 53 & 9 , az SA 55 56 $ 7 i a Coaspoo 00 04 1.2 0.2 0.2 21.2 0 : 2 S ' /0.6 0.4 0 0 0 0 0 S₁ 0.4 0.2 0.4 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Detected in frame_555.jpg: NNN..
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
0.4
$3
0.4
0.4
S4
0.4
$5
0.4
04
56
0.4
a, az
57
0.6
a
N
0.2
0.2
0.2
0.2
0.2
0.6
S'
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
b
-
Cooopool NNN .. Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 0.4 $ 3 0.4 0.4 S4 0.4 $ 5 0.4 04 56 0.4 a , az 57 0.6 a N 0.2 0.2 0.2 0.2 0.2 0.6 S ' 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b - Cooopool Detected in frame_580.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
Si
$2
0.1
a
Р
Coorpool
$3
di az
54
55
56
57
0.4
0.4
0.4
04
4
0.4
06
0.2
0.2
0.2
0.2
0.2
0.6
S'
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
S
P
INNA
N Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 Si $ 2 0.1 a Р Coorpool $ 3 di az 54 55 56 57 0.4 0.4 0.4 04 4 0.4 06 0.2 0.2 0.2 0.2 0.2 0.6 S ' 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S P INNA N Detected in frame_581.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(515)=6
Coopon
a
P
Si
$2
04
53
S4
0.4
0.4
04
0.4
55
04
04
a, az
56
0.4
0.6.
0.2
0.2
0.2.
0.2
0.2
0,6
S'
70.6 0.4 0
0
0
0
0
S₁
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
INI INN Example : Mars Rover Markov Chain Transition Matrix , P P ( 515 ) = 6 Coopon a P Si $ 2 04 53 S4 0.4 0.4 04 0.4 55 04 04 a , az 56 0.4 0.6 . 0.2 0.2 0.2 . 0.2 0.2 0,6 S ' 70.6 0.4 0 0 0 0 0 S₁ 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 INI INN Detected in frame_582.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
53
0.4
34
0.4
55
04
56
0.4
9, az
$7
0.6
P-
0.4
0.4
0.4
04
0.2
0.2
0.2
0.2
0.2
0.6
S'
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
වී.
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
S150 from
INNIS Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 53 0.4 34 0.4 55 04 56 0.4 9 , az $ 7 0.6 P- 0.4 0.4 0.4 04 0.2 0.2 0.2 0.2 0.2 0.6 S ' 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 වී . 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S150 from INNIS Detected in frame_583.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
Si
0.4
S2
ai az
$3
S4
$5
56
ST
0.4
0.4
0.4
0.4
0.4
0.4
14
04
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
INNIS Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 Si 0.4 S2 ai az $ 3 S4 $ 5 56 ST 0.4 0.4 0.4 0.4 0.4 0.4 14 04 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 INNIS Detected in frame_584.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
53
0.4
0.4
34
0.4
0.4
55
04
0.4
56
0.4
0.4
9, az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2
0.4
0
0
0
J
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
b
S
D
INNIS Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 53 0.4 0.4 34 0.4 0.4 55 04 0.4 56 0.4 0.4 9 , az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S D INNIS Detected in frame_585.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
$1
0.4
$2
0.4
$3
&
S4
55
0.4
0.4
0.4
0,4
04
0.4
0.4
56
0.4
0.4
a, az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
S₁
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0 0
0
0
0.4 0.2 0.4
b
Sing
Screenshot Facia
INNINN Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 $ 1 0.4 $ 2 0.4 $ 3 & S4 55 0.4 0.4 0.4 0,4 04 0.4 0.4 56 0.4 0.4 a , az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 S₁ /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Sing Screenshot Facia INNINN Detected in frame_586.jpg: Example Mars Rover Markov Chain Transition Matrix. P
P(s. 15)=6
3
Sz
53
SA
$5
9, az
S
06
1.Z
0.2
1.2
0:2
/0.6 0.4 0
0
0
S₁
0
0
0.4 0.2 0.4
0
0
0
0
D
0
0.4 0.2 0.4
0
0
0
P=
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0 0.4
0.2 0.4
NU SUNT
' Example Mars Rover Markov Chain Transition Matrix . P P ( s . 15 ) = 6 3 Sz 53 SA $ 5 9 , az S 06 1.Z 0.2 1.2 0 : 2 /0.6 0.4 0 0 0 S₁ 0 0 0.4 0.2 0.4 0 0 0 0 D 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NU SUNT ' Detected in frame_587.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
04
53
0.4
0.4
S4
0.4
10.4
S5
0.4
56
0.4
a, az
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Semiffos Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 04 53 0.4 0.4 S4 0.4 10.4 S5 0.4 56 0.4 a , az $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Semiffos Detected in frame_588.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
0.4
53
0.4
0.4
S4
0.4
10.4
S5
0.4
56
0.4
a, az
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
ப
C
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Semiffos
INNNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 0.4 53 0.4 0.4 S4 0.4 10.4 S5 0.4 56 0.4 a , az $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 ப C S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Semiffos INNNN Detected in frame_589.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
0.4
53
S5
S4
0.4
0.4
0.4
0.4
10.4
56
0.4
a, az
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Cloc
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Semiffos
• Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 0.4 53 S5 S4 0.4 0.4 0.4 0.4 10.4 56 0.4 a , az $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Cloc S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Semiffos • Detected in frame_590.jpg: N NJ NE
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
55
53
54
0.4
0.4
0.4
04
0.4
0.4
56
0.4
0.4
ai az
57
C10005
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2 0.4
0
0
0
P=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2
0.4
b
S N NJ NE Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 55 53 54 0.4 0.4 0.4 04 0.4 0.4 56 0.4 0.4 ai az 57 C10005 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S Detected in frame_591.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
04
$2
0,4
$3
0.4
S4
$5
56
0.4
0.4
04
04
a, az
57
0.6
0.2
0.2
0.2
02
0.2
0.6
[1000000
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
NNN
15 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 04 $ 2 0,4 $ 3 0.4 S4 $ 5 56 0.4 0.4 04 04 a , az 57 0.6 0.2 0.2 0.2 02 0.2 0.6 [ 1000000 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NNN 15 Detected in frame_592.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
S₂
53
S4
0.4
0.4
0.4
0.4
0.4
55
04
0.4
56
0.4
ai az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]
0.6 0.4 0
S₁
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0
0.4
0.2 0.4.
0
0
0
J
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0 0.4
0.2
0.4
b
S
Росси
NIN IN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 S₂ 53 S4 0.4 0.4 0.4 0.4 0.4 55 04 0.4 56 0.4 ai az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] 0.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 . 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S Росси NIN IN Detected in frame_593.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
0.4
S3
&
54
0.4
0.4
0.4
55
0.4
0.4
56
0.4
0.4
9, az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4
0.2
0.4
0
0
0
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
с
S
DONN
N Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 0.4 S3 & 54 0.4 0.4 0.4 55 0.4 0.4 56 0.4 0.4 9 , az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 с S DONN N Detected in frame_594.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
04
$2
0.4
$3
الله
0.4
04
54
$5
56
0.4
0.4
a, az
57
0.6
0.2
0.2
0.2
02
0.2
06
[1000000]
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
NNN
15 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 04 $ 2 0.4 $ 3 الله 0.4 04 54 $ 5 56 0.4 0.4 a , az 57 0.6 0.2 0.2 0.2 02 0.2 06 [ 1000000 ] 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NNN 15 Detected in frame_595.jpg: 0.6
[1000000]
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
0.4
S₂
ai az
$3
S4
$5
56
ST
0.4
0.4
0.4
0:4
0.4
0.4
0.4
0:4
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0 0
0
0.4 0.2
0.4
Son Mwen
Screenshot Fam
IN
NNNNN 0.6 [ 1000000 ] Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 0.4 S₂ ai az $ 3 S4 $ 5 56 ST 0.4 0.4 0.4 0 : 4 0.4 0.4 0.4 0 : 4 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Son Mwen Screenshot Fam IN NNNNN Detected in frame_596.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
0.4
S₂
ai az
$3
S4
55
56
57
0.4
0.4
0.4
04
0.4
0.4
0.4
0.4
0.6
[1000000]
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
0
0
0
S₁
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
J
P
=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0 0
0
0.4
0.2
0.4
b
Son Mwen
+
IN
NNNNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 0.4 S₂ ai az $ 3 S4 55 56 57 0.4 0.4 0.4 04 0.4 0.4 0.4 0.4 0.6 [ 1000000 ] 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 0 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Son Mwen + IN NNNNN Detected in frame_597.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
$2
0.4
$3
S4
$5
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
a, az
57
[1000000]
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
J
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Screen Meng
Screenshot fem
,
NNNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 $ 2 0.4 $ 3 S4 $ 5 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 a , az 57 [ 1000000 ] 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screen Meng Screenshot fem , NNNN Detected in frame_598.jpg: Example: Mars Rover Markov Chain Transition Matrix. P
P(S. 15)=6
9, az
$1
Sz
53
SA
$5
00
0.4
0.6
[10000]
04
0.2
0.2
112
0:2
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4 Example : Mars Rover Markov Chain Transition Matrix . P P ( S . 15 ) = 6 9 , az $ 1 Sz 53 SA $ 5 00 0.4 0.6 [ 10000 ] 04 0.2 0.2 112 0 : 2 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Detected in frame_599.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
Si
$2
04
$3
$5
S4
0.4
0.4
04
0.4
04
56
0.4
a az
57
0.6
0.2
0.2
0.2
0.2
0.2
0,6
[1000000]
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0.
0 0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
Semme ED
XAN
15 Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 Si $ 2 04 $ 3 $ 5 S4 0.4 0.4 04 0.4 04 56 0.4 a az 57 0.6 0.2 0.2 0.2 0.2 0.2 0,6 [ 1000000 ] 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 . 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Semme ED XAN 15 Detected in frame_600.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
04
S₂
0.4
$3
0.4
0.4
&
54
0.4
ai az
56
57
0:4
04
0.4
55
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
b
NNN
N Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 04 S₂ 0.4 $ 3 0.4 0.4 & 54 0.4 ai az 56 57 0 : 4 04 0.4 55 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b NNN N Detected in frame_601.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
Si
04
S2
04
الله
0.4
$4
$5
0.4
04
04
14
04
56
0.4
di az
ST
0.6
0.2
0.2
0.2
02
0.2
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
...
15 Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 Si 04 S2 04 الله 0.4 $ 4 $ 5 0.4 04 04 14 04 56 0.4 di az ST 0.6 0.2 0.2 0.2 02 0.2 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 ... 15 Detected in frame_602.jpg: whit
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
04
S₂
0.4
53
0.4
0.4
&
54
0.4
55
0:4
0.4
56
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0
0.4
0.2 0.4
0
0
0
J
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
b
Son M
Fona
NNNN whit Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 04 S₂ 0.4 53 0.4 0.4 & 54 0.4 55 0 : 4 0.4 56 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Son M Fona NNNN Detected in frame_603.jpg: initial
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
$3
0.4
0.4
E
54
0.4
55
04
0.4
56
0.4
0.4
9, az
ST
0.6
0.2
0.2
0.2
0.2
0.2
0.6
0.6
0.4 0
0
0
0
S₁
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P=
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
15
NINN initial Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 $ 3 0.4 0.4 E 54 0.4 55 04 0.4 56 0.4 0.4 9 , az ST 0.6 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 0 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 15 NINN Detected in frame_604.jpg: ST
Sz
Example. Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=.6
الله
0,4
9, az
SA
$7
d
06
12
Z
0.2
Dz
0:2
10.6 0.4 0
0
0
S₁
0
0
0.4 0.2 0:4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
' ST Sz Example . Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = . 6 الله 0,4 9 , az SA $ 7 d 06 12 Z 0.2 Dz 0 : 2 10.6 0.4 0 0 0 S₁ 0 0 0.4 0.2 0 : 4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 ' Detected in frame_605.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
$2
0.4
$3
54
55
0.4
0.4
04
0.4
04
56
0.4
04
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1]
initial
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
N IN N
N Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 $ 2 0.4 $ 3 54 55 0.4 0.4 04 0.4 04 56 0.4 04 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1 ] initial 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 N IN N N Detected in frame_606.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
04
S₂
0.4
55
9, az
56
ST
0.4
0.4
0.4
0.4
0.4
$3
0.4
54
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]
initial
70.6 0.4 0
S₁
0
0
0
0
Stal
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
J
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
b
SM
IS
NNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 04 S₂ 0.4 55 9 , az 56 ST 0.4 0.4 0.4 0.4 0.4 $ 3 0.4 54 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] initial 70.6 0.4 0 S₁ 0 0 0 0 Stal 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = J 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b SM IS NNN Detected in frame_607.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
24
a, az
53
54
55
56
ST
0.4
0.4
0.4
0.4
0.4
04
0.4
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]
initial
0.6 0.4 0
S₁
0
0
0
0
State
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
b
'
15
NI Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 24 a , az 53 54 55 56 ST 0.4 0.4 0.4 0.4 0.4 04 0.4 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] initial 0.6 0.4 0 S₁ 0 0 0 0 State 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b ' 15 NI Detected in frame_608.jpg: Example. Mars Rover Markov Chain Transition Matrix. P
P(S. 15)=6
ST
9, az
N
Sz
53
$4
55
56
S
LLA
06
1.2
02
ΩΣ
0:2
10.6 0.4 0
0
0
0
0
S₁
stat
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2 0.4
0
0
0
P=
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4 Example . Mars Rover Markov Chain Transition Matrix . P P ( S . 15 ) = 6 ST 9 , az N Sz 53 $ 4 55 56 S LLA 06 1.2 02 ΩΣ 0 : 2 10.6 0.4 0 0 0 0 0 S₁ stat 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Detected in frame_609.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(515)=6
Si
0.4
$2
04
S3
0.4
04
S4
0.4
04
$5
56
0:4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0,6
[1000000]
initial
0.6 0.4 0
0
0
S₁
0
0
shh
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2 0.4
0
0
0
J
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
b
S
15 Example : Mars Rover Markov Chain Transition Matrix , P P ( 515 ) = 6 Si 0.4 $ 2 04 S3 0.4 04 S4 0.4 04 $ 5 56 0 : 4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0,6 [ 1000000 ] initial 0.6 0.4 0 0 0 S₁ 0 0 shh 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S 15 Detected in frame_61.jpg: イ イ Detected in frame_610.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
53
54
55
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
9, az
0.6.
[1000000]P
initial
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
b
Stop Soren M
Do
Sorbet foo
'
NNNNNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 53 54 55 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 9 , az 0.6 . [ 1000000 ] P initial 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Stop Soren M Do Sorbet foo ' NNNNNN Detected in frame_611.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S2
04
$3
0.4
0.4
&
S4
a, az
$5
$7
0.4
0.4
0.6
[1000000]P
initial
0.2
0.2
0.2
02
0.2
0.6
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
15
NINA Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S2 04 $ 3 0.4 0.4 & S4 a , az $ 5 $ 7 0.4 0.4 0.6 [ 1000000 ] P initial 0.2 0.2 0.2 02 0.2 0.6 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 15 NINA Detected in frame_612.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
$3
ai az
54
55
$6
57
0.4
0.4
0.4
04
0.4
0.4
0.4
0.6
[1000000]P
initial
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
S₁
0
sht
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2 0.4
0
0
0
J
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
D
S
- Foca
IN IN INN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 $ 3 ai az 54 55 $ 6 57 0.4 0.4 0.4 04 0.4 0.4 0.4 0.6 [ 1000000 ] P initial 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 S₁ 0 sht 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b D S - Foca IN IN INN Detected in frame_613.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
&
$3
S4
55
0.4
0.4
0.4
04
0.4
0.4
0.4
$6
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
70.6 0.4. 0
0
S₁
0
0
0
initial
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0 0
0.4 0.2 0.4
b
Sereng
Document
Scrub Focus
INNINN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 & $ 3 S4 55 0.4 0.4 0.4 04 0.4 0.4 0.4 $ 6 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P 70.6 0.4 . 0 0 S₁ 0 0 0 initial 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Sereng Document Scrub Focus INNINN Detected in frame_614.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S2
04
$3
0.4
S4
$5
56
0.4
0.4
a, az
$7
04
0.6
0.2
0.2
0.2
02
0.2
0,6
0.6 0.4 0
0
0
0
0
initial
Stat
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
15 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S2 04 $ 3 0.4 S4 $ 5 56 0.4 0.4 a , az $ 7 04 0.6 0.2 0.2 0.2 02 0.2 0,6 0.6 0.4 0 0 0 0 0 initial Stat 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 15 Detected in frame_615.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
S1
0.4
S₂
0.4
$3
0.4
0.4
S4
0.4
0.4
55
04
56
0.4
ai az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
C1000000р
initial
70.6 0.4 0
0
S₁
0
0
0
Stat
0.4 0.2 0.4
0
0
0
0
0
M
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0 0.4
0.2 0.4
b
Focis
XXX.
IN Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 S1 0.4 S₂ 0.4 $ 3 0.4 0.4 S4 0.4 0.4 55 04 56 0.4 ai az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 C1000000р initial 70.6 0.4 0 0 S₁ 0 0 0 Stat 0.4 0.2 0.4 0 0 0 0 0 M 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Focis XXX . IN Detected in frame_616.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
53
54
0.4
0.4
0.4
0.4
55
0.4
$6
0.4
0.4
0.4
a, az
ST
0.6.
[1000000] P
initsal
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Soren Meising D
Sorshot fea
NNNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 53 54 0.4 0.4 0.4 0.4 55 0.4 $ 6 0.4 0.4 0.4 a , az ST 0.6 . [ 1000000 ] P initsal 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Soren Meising D Sorshot fea NNNN Detected in frame_617.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=.6
Si
0.4
$2
04
53
S4
56
a, az
$5
$7
0.4
0.4
0.4
0.4
04
14
04
06
[1000000]P
initial
stat
0.2
0.2
0.2
02
0.2
06
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
.0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
b
3 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = . 6 Si 0.4 $ 2 04 53 S4 56 a , az $ 5 $ 7 0.4 0.4 0.4 0.4 04 14 04 06 [ 1000000 ] P initial stat 0.2 0.2 0.2 02 0.2 06 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b 3 Detected in frame_618.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
S₂
0.4
$3
54
55
0.4
0.4
04
0.4
04
56
0.4
0.4
ai az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
70.6 0.4. 0
S₁
0
0
0
0
stat
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
S
Some from
XXX.
IN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 S₂ 0.4 $ 3 54 55 0.4 0.4 04 0.4 04 56 0.4 0.4 ai az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial 70.6 0.4 . 0 S₁ 0 0 0 0 stat 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S Some from XXX . IN Detected in frame_619.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
53
54
0.4
0.4
04
0.4
55
0.4
0.4
$6
0.4
9, az
57
0.6.
[1000000] P
initsal
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2 0.4
b
S
D
Schoo
'
NONINN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 53 54 0.4 0.4 04 0.4 55 0.4 0.4 $ 6 0.4 9 , az 57 0.6 . [ 1000000 ] P initsal 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S D Schoo ' NONINN Detected in frame_620.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
Ex
ai az
$3
54
55
56
57
04
0.4
0.4
0.4
04
14
04
0.6
[1000000]P
initial
0.2
0.2
0.2
02
0.2
06
0.6 0.4 0
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
15
- Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 Ex ai az $ 3 54 55 56 57 04 0.4 0.4 0.4 04 14 04 0.6 [ 1000000 ] P initial 0.2 0.2 0.2 02 0.2 06 0.6 0.4 0 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 15 - Detected in frame_621.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S2
0.4
53
0.4
0.4
&
54
0.4
9 az
55
56
0:4
0.4
0.4
0.4
0.6
[1000000] P
initial
sht
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4 0
0
0
0
อ
5
0
0.4 0.2 0.4.
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
b
Son M
D
NININN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S2 0.4 53 0.4 0.4 & 54 0.4 9 az 55 56 0 : 4 0.4 0.4 0.4 0.6 [ 1000000 ] P initial sht 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 5 0 0.4 0.2 0.4 . 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Son M D NININN Detected in frame_622.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
$3
&
S4
55
0.4
0.4
0.4
0.4
04
04
$6
0.4
0.4
9, az
57
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
70.6 0.4 0
0
0
0
0
State
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Screen Ming D
Scro Focus
'
NINN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 $ 3 & S4 55 0.4 0.4 0.4 0.4 04 04 $ 6 0.4 0.4 9 , az 57 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial 70.6 0.4 0 0 0 0 0 State 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screen Ming D Scro Focus ' NINN Detected in frame_623.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=.6
Si
0.4
$2
0.4
ai az
$3
S4
56
$7
0.4
0.4
0.4
04
14
04
0.4
0.6
[1000000] P
initial
0.2
0.2
0.2
02
0.2
06
(0.6 0.4 0
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Sc
15
NINA
- Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = . 6 Si 0.4 $ 2 0.4 ai az $ 3 S4 56 $ 7 0.4 0.4 0.4 04 14 04 0.4 0.6 [ 1000000 ] P initial 0.2 0.2 0.2 02 0.2 06 ( 0.6 0.4 0 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Sc 15 NINA - Detected in frame_624.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
53
0.4
0.4
0.4
&
54
0.4
0.4
55
ai az
56
0:4
0.4
0.4
0.6
[1000000]P
initial
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
Sht
0.4 0.2 0.4 0
0
0
0
0
0.4
0.2 0.4.
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Son M
D
Sort Foca
NININN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 53 0.4 0.4 0.4 & 54 0.4 0.4 55 ai az 56 0 : 4 0.4 0.4 0.6 [ 1000000 ] P initial 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 Sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 . 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Son M D Sort Foca NININN Detected in frame_625.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
0.4
S2
0.4
$3
0.4
&
S4
$5
0.4
0.4
0.4
0.4
0.4
56
0.4
9, az
57
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
stat
5
/0.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P
J
=
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Screaming
Streho
NININ N Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 0.4 S2 0.4 $ 3 0.4 & S4 $ 5 0.4 0.4 0.4 0.4 0.4 56 0.4 9 , az 57 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial stat 5 /0.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P J = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screaming Streho NININ N Detected in frame_626.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
04
S2
0:4
$3
ai az
S4
$5
$6
57
0.4
0.4
0.4
0.4
04
0.6
[1000000]P
initial
0.2
0.2
0.2
0.2
0.2
06
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
5
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
.0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
DONNA
3 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 04 S2 0 : 4 $ 3 ai az S4 $ 5 $ 6 57 0.4 0.4 0.4 0.4 04 0.6 [ 1000000 ] P initial 0.2 0.2 0.2 0.2 0.2 06 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 5 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 DONNA 3 Detected in frame_627.jpg: NNNN
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
53
S4
0.4
0.4
0.4
0.4
55
04
0.4
56
0.4
0.4
ai az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P
=
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Вос
S
initial
sht NNNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 53 S4 0.4 0.4 0.4 0.4 55 04 0.4 56 0.4 0.4 ai az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Вос S initial sht Detected in frame_628.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
$3
&
54
0.4
0.4
0.4
0.4
04
04
56
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
S₁
70.6 0.4 0
0
0
0
0
State
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0 0
0
0
0.4 0.2 0.4
b
Sing
Document
Screenshot Fo
'
NININN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 $ 3 & 54 0.4 0.4 0.4 0.4 04 04 56 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial S₁ 70.6 0.4 0 0 0 0 0 State 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Sing Document Screenshot Fo ' NININN Detected in frame_629.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
04
$2
ai az
$3
S4
$5
$6
ST
04
0.4
0.4
0.4
014
04
initial
sht
06
0.2
0.2
0.2
0.2
0.2
06
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
.0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 04 $ 2 ai az $ 3 S4 $ 5 $ 6 ST 04 0.4 0.4 0.4 014 04 initial sht 06 0.2 0.2 0.2 0.2 0.2 06 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Detected in frame_630.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
$3
0.4
0.4
34
54
0.4
55
04
0.4
56
0.4
ai az
57
0.6
[1000000] P
initial
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4. 0
S₁
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0.
0
0
0.4
0.2 0.4
b
IN
NINA Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 $ 3 0.4 0.4 34 54 0.4 55 04 0.4 56 0.4 ai az 57 0.6 [ 1000000 ] P initial 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 . 0 S₁ 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 b IN NINA Detected in frame_631.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
$3
$6
&
9, az
S4
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
S₁
70.6 0.4 0
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Scr
D
Schet Form
'
NININN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 $ 3 $ 6 & 9 , az S4 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial S₁ 70.6 0.4 0 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Scr D Schet Form ' NININN Detected in frame_632.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
Si
04
$2
0.4
الله
0.4
54
0.4
04
31.4
04
56
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
06
..
3
0.6 0.4 0
0
0
S₁
0
0
initial
shot
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4 Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 Si 04 $ 2 0.4 الله 0.4 54 0.4 04 31.4 04 56 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 06 .. 3 0.6 0.4 0 0 0 S₁ 0 0 initial shot 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Detected in frame_633.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
$3
0.4
0.4
54
0.4
04
55
0.4
56
0.4
0.4
ai az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
0.6 0.4 0
S₁
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
J
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0.
0
0
0.4 0.2
0.4
b
S
IS
NNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 $ 3 0.4 0.4 54 0.4 04 55 0.4 56 0.4 0.4 ai az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial 0.6 0.4 0 S₁ 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 b S IS NNN Detected in frame_634.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
$3
&
9, az
S4
$6
57
0.4
0.4
0.4
0.4
0.4
0.4
04
0.4
0.6.
[1.000000] P=
initial
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
9
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Soren Ming D
Schet Focza
'
NININN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 $ 3 & 9 , az S4 $ 6 57 0.4 0.4 0.4 0.4 0.4 0.4 04 0.4 0.6 . [ 1.000000 ] P = initial 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 9 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Soren Ming D Schet Focza ' NININN Detected in frame_635.jpg: XAN
0.6
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
$2
04
الله
0.4
04
Ex
S4
$5
56
0.4
0.4
di az
$7
3
-
0.6
0.2
0.2
0.2
0.2
0.2
70.6 0.4 0
0
0
0
0
initial
Sht
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P=
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
b XAN 0.6 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 $ 2 04 الله 0.4 04 Ex S4 $ 5 56 0.4 0.4 di az $ 7 3 - 0.6 0.2 0.2 0.2 0.2 0.2 70.6 0.4 0 0 0 0 0 initial Sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Detected in frame_636.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
&
9 az
$3
54
56
0.4
04
0.4
04
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
/0.6 0.4 0
0
0
0
S₁
0
stat
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
J
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
a
0
0.
0
0 0.4 0.2 0.4
b
Son
Фор
Foo
NON IN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 & 9 az $ 3 54 56 0.4 04 0.4 04 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial /0.6 0.4 0 0 0 0 S₁ 0 stat 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 a 0 0 . 0 0 0.4 0.2 0.4 b Son Фор Foo NON IN Detected in frame_637.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
0.4
$2
0.4
$3
&
S4
0.4
0.4
0.4
0.4
0.4
$6
a, az
57
0.4
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
0.6 0.4. 0
S₁
0
0
0
0
initial
sht
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
Screaming D
Schot Focus
NININN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 0.4 $ 2 0.4 $ 3 & S4 0.4 0.4 0.4 0.4 0.4 $ 6 a , az 57 0.4 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P 0.6 0.4 . 0 S₁ 0 0 0 0 initial sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Screaming D Schot Focus NININN Detected in frame_638.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
$2
04
الله
0.4
Ex
S4
0.4
$5
56
0.4
ai az
$7
04
14
04
0.6
0.2
0.2
0.2
0.2
0.2
0.6 0.4 0
0
0
0
0
initial
sht
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2 0.4
06
..
3 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 $ 2 04 الله 0.4 Ex S4 0.4 $ 5 56 0.4 ai az $ 7 04 14 04 0.6 0.2 0.2 0.2 0.2 0.2 0.6 0.4 0 0 0 0 0 initial sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 06 .. 3 Detected in frame_639.jpg: NNNN
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
&
$3
54
0.4
0.4
04
0.4
04
56
0.4
0.4
9, az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
S₁
0
0
0
0
initial
5ht
0.4 0.2 0.4
0
0
0
0
อ
0
0.4
0.2 0.4
0
0
0
J
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0.
0
0
0.4
0.2 0.4
b
Foo NNNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ & $ 3 54 0.4 0.4 04 0.4 04 56 0.4 0.4 9 , az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 S₁ 0 0 0 0 initial 5ht 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 . 0 0 0.4 0.2 0.4 b Foo Detected in frame_640.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
S₂
0.4
$3
S4
$5
0.4
0.4
0.4
0.4
0.4
04
$6
0.4
9, az
57
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
S₁
0
0
0
0
initial
sht
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Se Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 S₂ 0.4 $ 3 S4 $ 5 0.4 0.4 0.4 0.4 0.4 04 $ 6 0.4 9 , az 57 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 S₁ 0 0 0 0 initial sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Se Detected in frame_641.jpg: Example. Mars Rover Markov Chain Transition Matrix. P
P(s. 15)=6
Sz
53
SA
00
m
a, az
$5
56
4
04
0.6
[10000]P-
0.2
0.2
11.2
0.2
4/0.6 0.4 0
0
0
0
0
stat
0.4 0.2 0.4
0
0
0
0
D
0 0.4 0.2
0.4
0
0
0
P=
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4 Example . Mars Rover Markov Chain Transition Matrix . P P ( s . 15 ) = 6 Sz 53 SA 00 m a , az $ 5 56 4 04 0.6 [ 10000 ] P- 0.2 0.2 11.2 0.2 4 / 0.6 0.4 0 0 0 0 0 stat 0.4 0.2 0.4 0 0 0 0 D 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Detected in frame_642.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
w'
0.4
04
&
S4
0.4
0.4
55
04
56
a, az
57
0.4
04
0.6
CP=
initial
0.2
0.2
0.2
0.2
0.2
0.6
/0.6 0.4 0
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
b
IN IN INN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 w ' 0.4 04 & S4 0.4 0.4 55 04 56 a , az 57 0.4 04 0.6 CP = initial 0.2 0.2 0.2 0.2 0.2 0.6 /0.6 0.4 0 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b IN IN INN Detected in frame_643.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
$1
0.4
$2
53
S4
04
0.4
0.4
0.4
4
56
0.4
di az
ST
0.6
0.2
0.2
0.2
02
0.2
06
/0.6 0.4 0
0
0
0
0
initial
sht
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
.0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
15 Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 $ 1 0.4 $ 2 53 S4 04 0.4 0.4 0.4 4 56 0.4 di az ST 0.6 0.2 0.2 0.2 02 0.2 06 /0.6 0.4 0 0 0 0 0 initial sht 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b 15 Detected in frame_644.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
04
S₂
34
55
$3
S4
0.4
0.4
0.4
0.4
0.4
0.4
56
0.4
0.4
ai az
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
/0.6 0.4 0
S₁
0
0
0
0
initial
sht
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0 0
0
0
0
0.4
0.2 0.4 0
0
0
0 0
0.4 0.2 0.4
b
Доб
Socia
NNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 04 S₂ 34 55 $ 3 S4 0.4 0.4 0.4 0.4 0.4 0.4 56 0.4 0.4 ai az 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P /0.6 0.4 0 S₁ 0 0 0 0 initial sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Доб Socia NNN Detected in frame_645.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
S2
0.4
$3
S4
$6
a, az
$5
$7
0.4
0.4
0.4
0.4
0.4
0.4
0.4
[1000000
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
initial
sht
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
See 5
NININN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 S2 0.4 $ 3 S4 $ 6 a , az $ 5 $ 7 0.4 0.4 0.4 0.4 0.4 0.4 0.4 [ 1000000 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 initial sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b See 5 NININN Detected in frame_646.jpg: Example Mars Rover Markov Chain Transition Matrix. P
a, az
P(s. 15)=6
Sz
53
00
04
SA
56
0.6
0.2
1.2
0.2
11.2
0:2
/0.6 0.4 0
0
0
0
0
stat
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P=
0
0
04
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
' Example Mars Rover Markov Chain Transition Matrix . P a , az P ( s . 15 ) = 6 Sz 53 00 04 SA 56 0.6 0.2 1.2 0.2 11.2 0 : 2 /0.6 0.4 0 0 0 0 0 stat 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 04 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 ' Detected in frame_647.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
$2
0.4
$3
&
SA
0.4
0.4
04
0.4
0.4
56
0.4
α, az
ST
0.6
0.2
0.2
0.2
0.2
0.2
0.6
initial
sht
༧,༠༦༠མ།
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2
0.4
0
0
0
J
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
B
0
0
0
0
0.4
0.2 0.4
b Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 $ 2 0.4 $ 3 & SA 0.4 0.4 04 0.4 0.4 56 0.4 α , az ST 0.6 0.2 0.2 0.2 0.2 0.2 0.6 initial sht ༧,༠༦༠ མ ། 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 J P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 B 0 0 0 0 0.4 0.2 0.4 b Detected in frame_648.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
ai az
P(s. 15)=6
Si
S2
04
$3
54
55
$6
0.4
0.4
04
0.4
1.4
0.4
$7
0.6
0.2
0.2
0.2
02
0.2
06
[1000000]P
initial
0.6
0.4 0
0
0
0
0
State
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
3 Example : Mars Rover Markov Chain Transition Matrix , P ai az P ( s . 15 ) = 6 Si S2 04 $ 3 54 55 $ 6 0.4 0.4 04 0.4 1.4 0.4 $ 7 0.6 0.2 0.2 0.2 02 0.2 06 [ 1000000 ] P initial 0.6 0.4 0 0 0 0 0 State 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 Detected in frame_649.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
S₂
0.4
$3
S4
$5
0.4
0.4
0.4
0.4
0.4
0.4
56
0.4
9, az
57
0.6.
[1000000]P
initial
sht
0.2
0.2
0.2
0.2
0.2
0.6
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P
=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
S
'
NININN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 S₂ 0.4 $ 3 S4 $ 5 0.4 0.4 0.4 0.4 0.4 0.4 56 0.4 9 , az 57 0.6 . [ 1000000 ] P initial sht 0.2 0.2 0.2 0.2 0.2 0.6 S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S ' NININN Detected in frame_650.jpg: NAA..
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
S2
0.4
53
0.4
04
S4
$5
$6
0.4
0.4
α, az
$7
0.6
0.2
0.2
0.2
02
0.2
0,6
0.6 0.4 0
0
0
0
0
initial
sht
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
.0
0
0.4
0.2 0.4 0
0
0
0
0
0.4
0.2 0.4
3
' NAA .. Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si S2 0.4 53 0.4 04 S4 $ 5 $ 6 0.4 0.4 α , az $ 7 0.6 0.2 0.2 0.2 02 0.2 0,6 0.6 0.4 0 0 0 0 0 initial sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 ' Detected in frame_651.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₂
56
a, az
0.4
$3
S4
55
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
04
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
shte
/0.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0 0.4 0.2
0.4
0
0
0
P
=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Screening
Socia
' Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₂ 56 a , az 0.4 $ 3 S4 55 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 04 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial shte /0.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening Socia ' Detected in frame_652.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
S2
04
الله
α, az
S4
$5
56
ST
0.4
0.4
04
11.4
04
0.6
[10]P
initial
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
0
0
0
0
sht
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
.0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
NNNNN
3 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 S2 04 الله α , az S4 $ 5 56 ST 0.4 0.4 04 11.4 04 0.6 [ 10 ] P initial 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NNNNN 3 Detected in frame_653.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
$1
0.4
S2
$3
S4
55
0.4
0.4
0.4
0.4
0.4
0.4
0.4
56
0.4
a, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initsal
state
70.6 0.4 0
0
0
0
S₁
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P
=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
Socia Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 $ 1 0.4 S2 $ 3 S4 55 0.4 0.4 0.4 0.4 0.4 0.4 0.4 56 0.4 a , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initsal state 70.6 0.4 0 0 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Socia Detected in frame_654.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
SI
0.4
S2
04
الله
di az
$5
56
ST
0.4
0.4
04
14
04
0.4
S4
0.6
[10]P
initial
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
.0
0
0.4 0.2
0.4 0
0
0
0
0
0.4
0.2 0.4
NNNNN
3 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 SI 0.4 S2 04 الله di az $ 5 56 ST 0.4 0.4 04 14 04 0.4 S4 0.6 [ 10 ] P initial 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NNNNN 3 Detected in frame_655.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
0.4
S2
0.4
$3
S4
55
0.4
0.4
0.4
0.4
0.4
0.4
56
0.4
9, az
57
0.6.
[1000000]P
initial
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
S₁
0
0
0
0
565
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
NININ
$ Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 0.4 S2 0.4 $ 3 S4 55 0.4 0.4 0.4 0.4 0.4 0.4 56 0.4 9 , az 57 0.6 . [ 1000000 ] P initial 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 S₁ 0 0 0 0 565 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NININ $ Detected in frame_656.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
04
$2
53
54
$5
04
0.4
0.4
0.4
04
56
0.4
ai az
57
initial
stat
0.6
0.2
0.2
0.2
02
0.2
0,6
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P
=
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
..
15 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 04 $ 2 53 54 $ 5 04 0.4 0.4 0.4 04 56 0.4 ai az 57 initial stat 0.6 0.2 0.2 0.2 02 0.2 0,6 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 .. 15 Detected in frame_657.jpg: 21
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
0.4
S2
0.4
$3
E
S4
55
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
a, az
57
0.6.
[1000000]P
initial
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
S₁
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
S
Sc
NININ
$ 21 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 0.4 S2 0.4 $ 3 E S4 55 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 a , az 57 0.6 . [ 1000000 ] P initial 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 S₁ 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S Sc NININ $ Detected in frame_658.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
04
$2
$5
53
S4
04
0.4
04
104
04
56
0.4
ai az
57
GO
0.6
0.2
0.2
0.2
02
02
0,6
initial
stat
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
P
=
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
..
3 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 04 $ 2 $ 5 53 S4 04 0.4 04 104 04 56 0.4 ai az 57 GO 0.6 0.2 0.2 0.2 02 02 0,6 initial stat 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 .. 3 Detected in frame_659.jpg: 21
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
0.4
S2
0.4
$3
E
S4
55
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
9, az
57
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
532
0.6 0.4 0
0
0
0
S₁
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
S
Socia
NININ
$ 21 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 0.4 S2 0.4 $ 3 E S4 55 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 9 , az 57 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial 532 0.6 0.4 0 0 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S Socia NININ $ Detected in frame_660.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=.6
$1
0.4
S2
0.4
53
S4
04
0.4
0.4
04
$5
0.4
04
$6
ai az
$7
0.4
initial
0.6
0.2
0.2
0.2
0.2
0.2
0,6
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
=
0
0
0:4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
15 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = . 6 $ 1 0.4 S2 0.4 53 S4 04 0.4 0.4 04 $ 5 0.4 04 $ 6 ai az $ 7 0.4 initial 0.6 0.2 0.2 0.2 0.2 0.2 0,6 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0 : 4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b 15 Detected in frame_661.jpg: A
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
0.4
S₂
☑
$3
S4
$5
0.4
0.4
0.4
0.4
10.4
0.4
0.4
0.4
$6
0.4
9, az
57
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
5bb
รา
NNNN
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
==
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4 0
0
0
0
0
0.4
0.2 0.4
San Mig
S
$ A Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 0.4 S₂ ☑ $ 3 S4 $ 5 0.4 0.4 0.4 0.4 10.4 0.4 0.4 0.4 $ 6 0.4 9 , az 57 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial 5bb รา NNNN S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P == 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 San Mig S $ Detected in frame_662.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
S2
0.4
53
S4
a, az
56
$7
04
0.4
0.4
0.4
0.4
04
0.6
0.2
0.2
0.2
0.2
0.2
0,6
initial
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2 0.4
0
0
0
P
=
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
3
• Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 S2 0.4 53 S4 a , az 56 $ 7 04 0.4 0.4 0.4 0.4 04 0.6 0.2 0.2 0.2 0.2 0.2 0,6 initial 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 • Detected in frame_663.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
0.4
S₂
0.4
$3
☑
S4
0.4
0.4
0.4
0.4
0.4
0.4
04
$6
0.4
9, az
57
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
70.6 0.4 0
S₁
0
0
0
0
stat
0.4 0.2 0.4
0
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0 0.4 0.2
0.4 0
0
0
0
0 0.4
0.2 0.4
San Mig Docum
XXX N
' Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 0.4 S₂ 0.4 $ 3 ☑ S4 0.4 0.4 0.4 0.4 0.4 0.4 04 $ 6 0.4 9 , az 57 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial 70.6 0.4 0 S₁ 0 0 0 0 stat 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 San Mig Docum XXX N ' Detected in frame_664.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
S2
0.4
53
S4
04
0.4
0.4
0.4
04
56
0.4
a, az
$7
0.6
0.2
0.2
0.2
0.2
0.2
0,6
initial
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P
=
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
..
3
• Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 S2 0.4 53 S4 04 0.4 0.4 0.4 04 56 0.4 a , az $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0,6 initial 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 .. 3 • Detected in frame_665.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
0.4
S2
0.4
$3
&
S4
$5
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
9, az
57
0.6.
[1000000] P
initial
0.2
0.2
0.2
0.2
0.2
0.6
NNNN
0.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
Fo Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 0.4 S2 0.4 $ 3 & S4 $ 5 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 9 , az 57 0.6 . [ 1000000 ] P initial 0.2 0.2 0.2 0.2 0.2 0.6 NNNN 0.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Fo Detected in frame_666.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
&
$3
54
04
0.4
0.4
14
$5
0.4
04
ai az
$6
0.4
0.6
[1000000]P
initial
0.2
0.2
0.2
0.2
0.2
06
0.6 0.4 0
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
NA
3 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 & $ 3 54 04 0.4 0.4 14 $ 5 0.4 04 ai az $ 6 0.4 0.6 [ 1000000 ] P initial 0.2 0.2 0.2 0.2 0.2 06 0.6 0.4 0 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NA 3 Detected in frame_667.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
0.4
S2
&
$3
S4
$5
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
9, az
57
0.6
[1000000] P
initial
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
S₁
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
b
IN IN IN N Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 0.4 S2 & $ 3 S4 $ 5 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 9 , az 57 0.6 [ 1000000 ] P initial 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 S₁ 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b IN IN IN N Detected in frame_668.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
ai az
$3
54
56
ST
04
0.4
0.4
0.4
0.4
114
04
0.6
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
sht
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P
=
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0 0.4 0.2
0.4
15
NINA Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 ai az $ 3 54 56 ST 04 0.4 0.4 0.4 0.4 114 04 0.6 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial sht 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 15 NINA Detected in frame_669.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
0.4
S2
&
$3
S4
$5
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
9, az
57
0.6
[1000000] P
initial
sht
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P
=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
b
IN IN IN N Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 0.4 S2 & $ 3 S4 $ 5 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 9 , az 57 0.6 [ 1000000 ] P initial sht 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b IN IN IN N Detected in frame_670.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
Si
$2
04
$3
S4
0:4
0.4
0.4
11.4
55
0.4
04
56
0.4
a, az
57
0.6
0.2
0.2
02
0.2
0.2
0.6
..
0.6 0.4 0
0
0
0
0
initial
sht
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4
0.2 0.4
3
• Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 Si $ 2 04 $ 3 S4 0 : 4 0.4 0.4 11.4 55 0.4 04 56 0.4 a , az 57 0.6 0.2 0.2 02 0.2 0.2 0.6 .. 0.6 0.4 0 0 0 0 0 initial sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 • Detected in frame_671.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
0.4
S₂
0.4
$3
0.4
S4
9, az
$6
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6.
[1000000] P
initial
sht
5.
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
S₁
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
b
Sc Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 0.4 S₂ 0.4 $ 3 0.4 S4 9 , az $ 6 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 . [ 1000000 ] P initial sht 5 . 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Sc Detected in frame_672.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
S₁
0.4
$2
S4
ai az
$3
$5
56
57
04
0.4
0.4
0.4
0.4
0.4
04
0.6
0.2
0.2
0.2
0.2
0.2
0,6
70.6 0.4 0
0
0
0
0
initial
shot
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P=
0
0
0.4
0.2
0.4
0
0
0
.0
0
0.4
0.2 0.4
0
0
0
0
0 0.4
0.2 0.4
3
• Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 S₁ 0.4 $ 2 S4 ai az $ 3 $ 5 56 57 04 0.4 0.4 0.4 0.4 0.4 04 0.6 0.2 0.2 0.2 0.2 0.2 0,6 70.6 0.4 0 0 0 0 0 initial shot 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 • Detected in frame_673.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
0.4
S2
0.4
$3
S4
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
9, az
57
0.6.
[1.000000] P
initial
stat
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
S₁
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
S Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 0.4 S2 0.4 $ 3 S4 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 9 , az 57 0.6 . [ 1.000000 ] P initial stat 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 S₁ 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 S Detected in frame_674.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(S. 15)=6
S₁
0.4
$2
ai az
$3
S4
$5
56
57
04
0.4
0.4
0.4
0.4
0.4
04
0.6
0.2
0.2
0.2
0.2
0.2
0,6
[10]P
initial
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P=
0
0
0.4
0.2
0.4
0
0
0
.0
0
0.4
0.2 0.4
0
0
0
0
0 0.4
0.2 0.4
15
• Example : Mars Rover Markov Chain Transition Matrix , P P ( S . 15 ) = 6 S₁ 0.4 $ 2 ai az $ 3 S4 $ 5 56 57 04 0.4 0.4 0.4 0.4 0.4 04 0.6 0.2 0.2 0.2 0.2 0.2 0,6 [ 10 ] P initial 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 15 • Detected in frame_675.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
$2
0.4
$3
S4
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
9, az
57
0.6.
[1000000] P
initial
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
S₁
0
0
0
0
stat
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
Song
'
NINN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 $ 2 0.4 $ 3 S4 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 9 , az 57 0.6 . [ 1000000 ] P initial 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 S₁ 0 0 0 0 stat 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Song ' NINN Detected in frame_676.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
Si
0.4
$2
04
الله
0.4
0.4
54
a, az
$5
56
57
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0
0
0
0
initial
State
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
NINA
3 Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 Si 0.4 $ 2 04 الله 0.4 0.4 54 a , az $ 5 56 57 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 0 0 0 initial State 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NINA 3 Detected in frame_677.jpg: NANN
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
0.4
S2
0.4
$3
&
S4
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
9, az
57
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
0.6 0.4 0
0
0
0
0
stat
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0 0.4
0.2 0.4
Screening Di NANN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 0.4 S2 0.4 $ 3 & S4 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 9 , az 57 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial 0.6 0.4 0 0 0 0 0 stat 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Screening Di Detected in frame_678.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 1s)=.6
Si
$2
04
الله
0.4
S4
56
di az
55
57
0.4
0.4
0.4
0.4
14
04
0.6
0.2
0.2
0.2
0.2
0.2
0.6
initial
state
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P
=
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
3
- Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 1s ) = . 6 Si $ 2 04 الله 0.4 S4 56 di az 55 57 0.4 0.4 0.4 0.4 14 04 0.6 0.2 0.2 0.2 0.2 0.2 0.6 initial state 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 - Detected in frame_679.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S2
0.4
$3
&
S4
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
9, az
57
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
P
=
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
SM
Scofecia
NANN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S2 0.4 $ 3 & S4 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 9 , az 57 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial 0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 SM Scofecia NANN Detected in frame_680.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 1s)=6
Si
04
S2
ai az
$3
54
$5
56
57
04
0.4
04
0.4
0.4
14
04
0.6
[10]P
initial
0.2
0.2
0.2
02
0.2
0,6
70.6 0.4 0
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4
0.2 0.4
NNA
3 Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 1s ) = 6 Si 04 S2 ai az $ 3 54 $ 5 56 57 04 0.4 04 0.4 0.4 14 04 0.6 [ 10 ] P initial 0.2 0.2 0.2 02 0.2 0,6 70.6 0.4 0 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 NNA 3 Detected in frame_681.jpg: NNNN
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
$3
0.4
S4
$5
0.4
0.4
0.4
0.4
0.4
$6
0.4
9, az
57
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
อ
0
0.4 0.2
0.4
0
0
0
P
=
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
0
0 0.4
0.2 0.4
Dow
0.6.
[1000000]P
initial NNNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 $ 3 0.4 S4 $ 5 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 9 , az 57 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 อ 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Dow 0.6 . [ 1000000 ] P initial Detected in frame_682.jpg: XAN
Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 1s)=.6
S₁
0.4
$2
$3
S4
$5
56
04
0.4
04
14
04
D.A
di az
$7
0.6
initial
shte
0.2
0.2
0.2
02
0.2
06
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
.0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
3 XAN Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 1s ) = . 6 S₁ 0.4 $ 2 $ 3 S4 $ 5 56 04 0.4 04 14 04 D.A di az $ 7 0.6 initial shte 0.2 0.2 0.2 02 0.2 06 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 Detected in frame_683.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S₁
0.4
S₂
0.4
$3
0.4
S4
$5
0.4
0.4
0.4
0.4
0.4
$6
0.4
9, az
57
0.6.
[1000000] Ps
initsal
562
0.2
0.2
0.2
0.2
0.2
0.6
NNNN
0.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
=
0
0
0.4 0.2
0.4 0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
S
D
Sci Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S₁ 0.4 S₂ 0.4 $ 3 0.4 S4 $ 5 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 9 , az 57 0.6 . [ 1000000 ] Ps initsal 562 0.2 0.2 0.2 0.2 0.2 0.6 NNNN 0.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b S D Sci Detected in frame_684.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 1s)=.6
S₁
0.4
$2
04
الله
S4
di az
$5
56
$7
0.4
D.A
04
14
04
initial
sht
0.6
0.2
0.2
0.2
02
0.2
06
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
.0
0
0.4
0.2 0.4 0
0
0
0
0
0.4
0.2 0.4
15
• Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 1s ) = . 6 S₁ 0.4 $ 2 04 الله S4 di az $ 5 56 $ 7 0.4 D.A 04 14 04 initial sht 0.6 0.2 0.2 0.2 02 0.2 06 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 .0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 15 • Detected in frame_685.jpg: NNNN
Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=6
S1
0.4
$2
0.4
$3
0.4
S4
$5
0.4
0.4
0.4
0.4
0.4
56
0.4
9, az
$7
0.2
0.2
0.2
0.2
0.2
0.6
70.6 0.4 0
0.
0
0
S₁
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
==
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0 0
0
0
0.4 0.2 0.4
Sewing
Screenshot Fe
-0.6
[1000000]P
initial
562
M NNNN Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = 6 S1 0.4 $ 2 0.4 $ 3 0.4 S4 $ 5 0.4 0.4 0.4 0.4 0.4 56 0.4 9 , az $ 7 0.2 0.2 0.2 0.2 0.2 0.6 70.6 0.4 0 0 . 0 0 S₁ 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P == 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Sewing Screenshot Fe -0.6 [ 1000000 ] P initial 562 M Detected in frame_686.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 1s)=.6
Si
S2
04
0.6
[1000000] P
initial
state
الله
0:4
S4
$5
56
0.4
04
0.4
04
1.4
04
0.2
0.2
0.2
02
0.2
06
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
0
0
0
P
=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
Sc
3
NINA Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 1s ) = . 6 Si S2 04 0.6 [ 1000000 ] P initial state الله 0 : 4 S4 $ 5 56 0.4 04 0.4 04 1.4 04 0.2 0.2 0.2 02 0.2 06 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Sc 3 NINA Detected in frame_687.jpg: suver Markov Chain Transition Matrix, P
0
Exaniple Man
5
105 0.4 0
ST
0
а
0
04 02 04
0
0
Example Sam
0 04 02 04
0
0
0 04 02 04
0
0
0
0
04 03 04
0
0.
D
0
0.4 0.2 0.4
0
0
0 suver Markov Chain Transition Matrix , P 0 Exaniple Man 5 105 0.4 0 ST 0 а 0 04 02 04 0 0 Example Sam 0 04 02 04 0 0 0 04 02 04 0 0 0 0 04 03 04 0 0 . D 0 0.4 0.2 0.4 0 0 0 Detected in frame_688.jpg: Example Mars Rover Marko Glam Episuds
151
"
Example Samolic sprandes stuming from 54 Example Mars Rover Marko Glam Episuds 151 " Example Samolic sprandes stuming from 54 Detected in frame_689.jpg: Example: Mars Rover Markov Chain Episodes
S₁
06
52
$3
54
44
0.4
$5
LA
0.4
0.2
ILZ
102
02
126
Example: Sample episodes starting from $4
54, 55, 56, 57, 57.57.
S4, 54. 55. 54, 55, 56...
54.53 $2.5
3 Example : Mars Rover Markov Chain Episodes S₁ 06 52 $ 3 54 44 0.4 $ 5 LA 0.4 0.2 ILZ 102 02 126 Example : Sample episodes starting from $ 4 54 , 55 , 56 , 57 , 57.57 . S4 , 54. 55. 54 , 55 , 56 ... 54.53 $ 2.5 3 Detected in frame_690.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
Sz
04
$3
S4
$5
$6
0.4
0.4
04
0.4
0.4
0.41
0.4
0.4
10.4
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
• 54, 54, 55, 54, 55, 56....
S4, 53, 52, 51.....
S
Screening
Screenshot
A Focus Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 Sz 04 $ 3 S4 $ 5 $ 6 0.4 0.4 04 0.4 0.4 0.41 0.4 0.4 10.4 $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... • 54 , 54 , 55 , 54 , 55 , 56 .... S4 , 53 , 52 , 51 ..... S Screening Screenshot A Focus Detected in frame_691.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
Sz
04
$3
S4
$5
$6
57
0.4
0.4
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
S4, S4, S5, S4, S5, S6,
S4, 53, 52, 51....
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 Sz 04 $ 3 S4 $ 5 $ 6 57 0.4 0.4 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , S4 , 53 , 52 , 51 .... Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_692.jpg: Example: Mars Rover Markov Chain Episodes
S₁
Sz
53
1.4
0,4
SA
$5
$6
0.4
0.4
A
DA
0.4
0:4
04
ST
06
0.2
0.2
02
0.2
0.2
006
Example: Sample episodes starting from $4
S4, S5, S6, 57, 57, 57,...
54, 54, 55, 54, 55, 56,
54, 53, 52, 51....
3
. Example : Mars Rover Markov Chain Episodes S₁ Sz 53 1.4 0,4 SA $ 5 $ 6 0.4 0.4 A DA 0.4 0 : 4 04 ST 06 0.2 0.2 02 0.2 0.2 006 Example : Sample episodes starting from $ 4 S4 , S5 , S6 , 57 , 57 , 57 , ... 54 , 54 , 55 , 54 , 55 , 56 , 54 , 53 , 52 , 51 .... 3 . Detected in frame_693.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
$5
56
$7
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, 55, 56, 57, 57, 57,...
S4, 54, 55, 54, 55, 56....
54, 53, 52, 51....
SM
D
100 Рос
IN Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 $ 5 56 $ 7 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , 55 , 56 , 57 , 57 , 57 , ... S4 , 54 , 55 , 54 , 55 , 56 .... 54 , 53 , 52 , 51 .... SM D 100 Рос IN Detected in frame_694.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
$2
$3
S4
$5
56
$7
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
10.4
0.4
0.6
0.2
02
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, S5, S6, ...
S4, 53, 52, 51....
S
D
EXT From Example : Mars Rover Markov Chain Episodes $ 1 0.4 $ 2 $ 3 S4 $ 5 56 $ 7 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 10.4 0.4 0.6 0.2 02 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , ... S4 , 53 , 52 , 51 .... S D EXT From Detected in frame_695.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
$2
04
0.4
$3
0.4
S4
$5
56
$7
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
S4, S4, S5, S4, S5, S6,
S4, 53, 52, 51....
Stop
Screen Mirroring Document
Screenshot
focus
"
I. IN IN N Example : Mars Rover Markov Chain Episodes $ 1 0.4 $ 2 04 0.4 $ 3 0.4 S4 $ 5 56 $ 7 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , S4 , 53 , 52 , 51 .... Stop Screen Mirroring Document Screenshot focus " I. IN IN N Detected in frame_696.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
$3
S4
$5
$6
04
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.4
0.4
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
⚫S4, 55, 56, 57, 57, 57,...
• 54, 54, 55, 54, 55, 56....
S4, 53, 52, 51.....
Screen Meg D
Screenshot
Роси
NNNN Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 $ 3 S4 $ 5 $ 6 04 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.4 0.4 $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 ⚫S4 , 55 , 56 , 57 , 57 , 57 , ... • 54 , 54 , 55 , 54 , 55 , 56 .... S4 , 53 , 52 , 51 ..... Screen Meg D Screenshot Роси NNNN Detected in frame_697.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
$5
$6
$7
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from $4
S4, S5, 56, 57, 57, 57,...
S4, 54, 55, 54, 55, 56,
S4, 53, 52, 51....
■Stop
Screen Ming Document
Screenshot
NNNN Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 $ 5 $ 6 $ 7 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from $ 4 S4 , S5 , 56 , 57 , 57 , 57 , ... S4 , 54 , 55 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... ■ Stop Screen Ming Document Screenshot NNNN Detected in frame_698.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
$3
S4
$5
56
$7
04
0.4
0.4
0.4
0.4
3.4
0.4
04
0.4
0.6:
02
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
S4, 54, 55, 54, 55, 56,
S4, 53, 52, 51....
S
Docam
Роси
N
4 Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 $ 3 S4 $ 5 56 $ 7 04 0.4 0.4 0.4 0.4 3.4 0.4 04 0.4 0.6 : 02 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... S4 , 54 , 55 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... S Docam Роси N 4 Detected in frame_699.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
$5
0.4
04
0.4
0.4
0.4
0.4
10.4
0.4
0.4
0.4
$6
0.4
0.4
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
⚫S4, 54, 55, 54, 55, 56...
S4, 53, 52, 51.....
Screen M
D
Screenshot
Focus
NNNN Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 $ 5 0.4 04 0.4 0.4 0.4 0.4 10.4 0.4 0.4 0.4 $ 6 0.4 0.4 $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... ⚫S4 , 54 , 55 , 54 , 55 , 56 ... S4 , 53 , 52 , 51 ..... Screen M D Screenshot Focus NNNN Detected in frame_700.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
$5
$6
57
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
02
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
⚫ 54, 55, 56, 57, 57.57....
• S4, 54, 55, 54, 55, 56.....
S4, 53, 52.51.....
Screening D
IN IN INN Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 $ 5 $ 6 57 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 02 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 ⚫ 54 , 55 , 56 , 57 , 57.57 .... • S4 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52.51 ..... Screening D IN IN INN Detected in frame_701.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
S5
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
10.4
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
• 54, 55, 56, 57, 57, 57,...
•S4, 54, 55, 54, 55, 56...
S4, 53, 52, 51....
Screen Ming Document
Screenshot
Focus
NNNN
" Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 S5 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 10.4 $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 • 54 , 55 , 56 , 57 , 57 , 57 , ... • S4 , 54 , 55 , 54 , 55 , 56 ... S4 , 53 , 52 , 51 .... Screen Ming Document Screenshot Focus NNNN " Detected in frame_702.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
Sz
$3
S4
$5
$6
57
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
⚫S4, S5, S6, S7, 57, 57,...
S4, S4, 55, 54, 55, 56,
S4, 53, 52, 51....
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN
" Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 Sz $ 3 S4 $ 5 $ 6 57 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 ⚫S4 , S5 , S6 , S7 , 57 , 57 , ... S4 , S4 , 55 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... Stop Screen Mirroring Document Screenshot Focus NNNN " Detected in frame_703.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
$5
$6
$7
0.4
04
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57....
• 54, 54, 55, 54, 55, 56....
S4, 53, 52, 51....
Screening D
IN IN INN
+ Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 $ 5 $ 6 $ 7 0.4 04 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 .... • 54 , 54 , 55 , 54 , 55 , 56 .... S4 , 53 , 52 , 51 .... Screening D IN IN INN + Detected in frame_704.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
$5
$6
57
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57....
⚫S4, 54, 55, 54, 55, 56.....
S4, 53, 52, 51....
Screening D
IS
NNNN Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 $ 5 $ 6 57 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 .... ⚫S4 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52 , 51 .... Screening D IS NNNN Detected in frame_705.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
Sz
04
$3
S4
$5
$6
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, 57, 57, 57,...
S4, S4, 55, 54, 55, 56....
S4, 53, 52, 51....
Stop
Screen Mirroring Docum
Screenshot
A Focus
ANNON
" Example : Mars Rover Markov Chain Episodes $ 1 0.4 Sz 04 $ 3 S4 $ 5 $ 6 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , 57 , 57 , 57 , ... S4 , S4 , 55 , 54 , 55 , 56 .... S4 , 53 , 52 , 51 .... Stop Screen Mirroring Docum Screenshot A Focus ANNON " Detected in frame_706.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
Sz
$3
S4
$5
$6
S7
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, S5, S6,...
S4, 53, 52, 51....
Stop Screen Mirroring
Screenshot
focu
"
IN INN Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 Sz $ 3 S4 $ 5 $ 6 S7 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , ... S4 , 53 , 52 , 51 .... Stop Screen Mirroring Screenshot focu " IN INN Detected in frame_707.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
$3
S4
$5
$6
57
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57,...
•S4, 54, 55, 54, 55, 56.....
S4, 53, 52, 51.....
Screen
Р
' Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 $ 3 S4 $ 5 $ 6 57 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 , ... • S4 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52 , 51 ..... Screen Р ' Detected in frame_708.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
S5
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
10.4
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from $4
S4, S5, S6, S7, 57, 57,...
• 54, 54, 55, 54, 55, 56.....
S4, 53, 52, 51....
Stop
Screen Mirroring Дос
Screenshot
A Focus
NSU SEN
" Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 S5 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 10.4 $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from $ 4 S4 , S5 , S6 , S7 , 57 , 57 , ... • 54 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52 , 51 .... Stop Screen Mirroring Дос Screenshot A Focus NSU SEN " Detected in frame_709.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
Sz
$3
S4
$5
$6
S7
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, 55, 56,..
S4, 53, 52, 51.....
Stop Screen Mirroring Docum
Screenshot
Focus
"
IN INN Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 Sz $ 3 S4 $ 5 $ 6 S7 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , 55 , 56 , .. S4 , 53 , 52 , 51 ..... Stop Screen Mirroring Docum Screenshot Focus " IN INN Detected in frame_710.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
$5
$6
57
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
⚫S4, 55, 56, 57, 57, 57....
S4, S4, S5, S4, S5, S6,
S4, 53, 52, 51....
Screening
Foca
NNNN Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 $ 5 $ 6 57 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 ⚫S4 , 55 , 56 , 57 , 57 , 57 .... S4 , S4 , S5 , S4 , S5 , S6 , S4 , 53 , 52 , 51 .... Screening Foca NNNN Detected in frame_711.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
S5
$6
$7
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
S4, S4, S5, S4, S5, S6,
S4, 53, 52, 51....
Screen Ming Document
Screenshot
WAT Focus
NNNN
" Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 S5 $ 6 $ 7 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , S4 , 53 , 52 , 51 .... Screen Ming Document Screenshot WAT Focus NNNN " Detected in frame_712.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
S2
04
$3
S4
$5
$6
$7
0.4
0.4
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
⚫S4, 55, 56, 57, 57, 57....
S4, 54, 55, 54, 55, 56,
S4, 53, 52, 51.....
Stop
Screen Mirroring Document
Screenshot
50 Гос
NNNN
' Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 S2 04 $ 3 S4 $ 5 $ 6 $ 7 0.4 0.4 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 ⚫S4 , 55 , 56 , 57 , 57 , 57 .... S4 , 54 , 55 , 54 , 55 , 56 , S4 , 53 , 52 , 51 ..... Stop Screen Mirroring Document Screenshot 50 Гос NNNN ' Detected in frame_713.jpg: Example: Mars Rover Markov Chain Episodes
S₁
$2
$3
0.4
D.4
0,4
0.4
3.4
04
SA
0.4
$5
0.4
0.4
$6
0.4
ST
06
0.2
0.Z
02
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57.57
S4, S4, S5, 54, 55, 56, ...
54. 53; 52: 51....
3 Example : Mars Rover Markov Chain Episodes S₁ $ 2 $ 3 0.4 D.4 0,4 0.4 3.4 04 SA 0.4 $ 5 0.4 0.4 $ 6 0.4 ST 06 0.2 0.Z 02 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57.57 S4 , S4 , S5 , 54 , 55 , 56 , ... 54. 53 ; 52 : 51 .... 3 Detected in frame_714.jpg: ་་
D
10 00 10
0
n
Π
0
w3 0
it a
ぶ
2 ་ ་ D 10 00 10 0 n Π 0 w3 0 it a ぶ 2 Detected in frame_715.jpg: 57
Example Mars Rover Markov Chain Transition Matrix, P
P/s | 5) = 6
ST
53
S
ய
a
NNNN
112
VE
0.2
BZ
02
/0.6 0.4 0
0
0.
0
0.
0.4 0.2 0.4
Q
0
0
0
0
0.4 0.2
0.4
0
D
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2
0.4 0
0
D
0
0
0.4
0.2 0.4 57 Example Mars Rover Markov Chain Transition Matrix , P P / s | 5 ) = 6 ST 53 S ய a NNNN 112 VE 0.2 BZ 02 /0.6 0.4 0 0 0 . 0 0 . 0.4 0.2 0.4 Q 0 0 0 0 0.4 0.2 0.4 0 D 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 D 0 0 0.4 0.2 0.4 Detected in frame_716.jpg: Example Mars Rover Markov Chain Transition Matrix, P
P(s, 1s)=.6
Sz
N
M
53
55
56
$7
06
[100000]P
initial
sht
0.4
$4
04
11.4
0.2
0.2
02
0.4
0:2
06
/0.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
P =
0
0 0.4
0.2
0.4 0
0
0
0
0
0.4 0.2 0.4 0
B
0
0
0
0
0.4 0.2 0.4 Example Mars Rover Markov Chain Transition Matrix , P P ( s , 1s ) = . 6 Sz N M 53 55 56 $ 7 06 [ 100000 ] P initial sht 0.4 $ 4 04 11.4 0.2 0.2 02 0.4 0 : 2 06 /0.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 B 0 0 0 0 0.4 0.2 0.4 Detected in frame_717.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s. 15)=.6
Si
0.4
$2
0.4
$3
&
54
0.4
0.4
0.4
104
$5
04
04
56
0.4
a, az
ST
0.6
0.2
0.2
0.2
0.2
0.2
0.6
initial
sht
S₁
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2
0.4 0
0
0
0
0
0.4 0.2 0.4
D
Sims it foca
NNN N Example : Mars Rover Markov Chain Transition Matrix , P P ( s . 15 ) = . 6 Si 0.4 $ 2 0.4 $ 3 & 54 0.4 0.4 0.4 104 $ 5 04 04 56 0.4 a , az ST 0.6 0.2 0.2 0.2 0.2 0.2 0.6 initial sht S₁ 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 D Sims it foca NNN N Detected in frame_718.jpg: NIN
Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 15)=.6
$1
0.4
$2
0.4
53
0.4
0.4
&
54
0.4
104
55
0.4
0.4
56
0.4
0.4
ai az
57
N
0.6
0.2
0.2
0.2
0.2
0.2
0.6
00
0.6 0.4 0
0
0
0
0
S₁
initial
shh
0.4 0.2 0.4
0
0
0
0
0 0.4 0.2 0.4
0
0
0
P =
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
b NIN Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 15 ) = . 6 $ 1 0.4 $ 2 0.4 53 0.4 0.4 & 54 0.4 104 55 0.4 0.4 56 0.4 0.4 ai az 57 N 0.6 0.2 0.2 0.2 0.2 0.2 0.6 00 0.6 0.4 0 0 0 0 0 S₁ initial shh 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Detected in frame_719.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 1s)=.6
Si
0.4
S2
04
الله
0.4
54
56
a, az
$5
57
0.4
0.0
0.4
0.4
4
04
0.6
0.2
0.2
0.2
02
0.2
0,6
initial
State
70.6 0.4 0
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
=
0
0
0.4 0.2
0.4
0
0
0
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4 0.2 0.4
b
3
4 Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 1s ) = . 6 Si 0.4 S2 04 الله 0.4 54 56 a , az $ 5 57 0.4 0.0 0.4 0.4 4 04 0.6 0.2 0.2 0.2 02 0.2 0,6 initial State 70.6 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b 3 4 Detected in frame_720.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 15)=6
$1
0.4
S₂
0.4
$6
a, az
57
0.4
0.4
0.4
0.4
$3
0.4
S4
0.4
$5
0.4
0.6
[1000000]P
initial
0.2
0.2
0.2
0.2
0.2
0.6
0.6 0.4 0
S₁
0
0
0
0
sht
0.4 0.2 0.4
0
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
b
Son M
D
Stoc
NININN Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 15 ) = 6 $ 1 0.4 S₂ 0.4 $ 6 a , az 57 0.4 0.4 0.4 0.4 $ 3 0.4 S4 0.4 $ 5 0.4 0.6 [ 1000000 ] P initial 0.2 0.2 0.2 0.2 0.2 0.6 0.6 0.4 0 S₁ 0 0 0 0 sht 0.4 0.2 0.4 0 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Son M D Stoc NININN Detected in frame_721.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 1s)=.6
Si
04
S2
04
$3
0:4
0.4
S4
0.4
di az
$5
56
$7
0.4
14
04
3
0.6
0.2
0.2
0.2
02
0.2
DUG
/0.6 0.4 0
0
0
0
S₁
0
initial
sht
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4 Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 1s ) = . 6 Si 04 S2 04 $ 3 0 : 4 0.4 S4 0.4 di az $ 5 56 $ 7 0.4 14 04 3 0.6 0.2 0.2 0.2 02 0.2 DUG /0.6 0.4 0 0 0 0 S₁ 0 initial sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 Detected in frame_722.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 1s)=6
9, az
0.4
S₂
0.4
$3
S4
$5
$6
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
sht
0.6 0.4 0
S₁
0
0
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
b
Sen Mening Doc
Sca
INN INN Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 1s ) = 6 9 , az 0.4 S₂ 0.4 $ 3 S4 $ 5 $ 6 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial sht 0.6 0.4 0 S₁ 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Sen Mening Doc Sca INN INN Detected in frame_723.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 1s)=.6
Si
$2
04
$3
S4
56
di az
$5
$7
0.4
0.4
0.0
0.4
0.4
14
04
0.6
0.2
0.2
0.2
0.2
0.2
06
70.6 0.4 0
0
0
0
0
initial
sht
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P =
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4
0.2 0.4
0
0
0
0
0
0.4 0.2 0.4
3 Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 1s ) = . 6 Si $ 2 04 $ 3 S4 56 di az $ 5 $ 7 0.4 0.4 0.0 0.4 0.4 14 04 0.6 0.2 0.2 0.2 0.2 0.2 06 70.6 0.4 0 0 0 0 0 initial sht 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 3 Detected in frame_724.jpg: Example: Mars Rover Markov Chain Transition Matrix, P
P(s, 1s)=6
0.4
S₂
0.4
$3
S4
$5
0.4
0.4
0.4
0.4
0.4
0.4
0.4
56
0.4
9, az
57
0.6.
0.2
0.2
0.2
0.2
0.2
0.6
[1000000]P
initial
sht
0.6 0.4 0
0
0
S₁
0
0
0.4 0.2 0.4
0
0
0
0
0
0.4 0.2
0.4
0
0
0
P
=
0
0
0.4
0.2
0.4
0
0
0
0
0
0.4
0.2 0.4 0
0
0
0
0
0.4 0.2 0.4
b
Sen Mening Doc
Sca
INN INN Example : Mars Rover Markov Chain Transition Matrix , P P ( s , 1s ) = 6 0.4 S₂ 0.4 $ 3 S4 $ 5 0.4 0.4 0.4 0.4 0.4 0.4 0.4 56 0.4 9 , az 57 0.6 . 0.2 0.2 0.2 0.2 0.2 0.6 [ 1000000 ] P initial sht 0.6 0.4 0 0 0 S₁ 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 P = 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 0 0 0 0 0 0.4 0.2 0.4 b Sen Mening Doc Sca INN INN Detected in frame_725.jpg: ars Rover Markov Chain Transition Matrix, P
-.6
02
ма
Z
5-
L
04
70.6 0.4 0
D
0
0
0
0.4 0.2 0.4
D
1)
0
0
0
0
0.4 0.2
0.4
0
'0
0
P =
0
0
0:4 02
0.4 0
0
0
0
°
0.4
0.2 0.4
0
k
0
0
0
0
0.4 0.2
0.4
b
0
0
M
Example
INTERN IN
Example
54 $5.5
54, 53, 52 ars Rover Markov Chain Transition Matrix , P -.6 02 ма Z 5- L 04 70.6 0.4 0 D 0 0 0 0.4 0.2 0.4 D 1 ) 0 0 0 0 0.4 0.2 0.4 0 ' 0 0 P = 0 0 0 : 4 02 0.4 0 0 0 0 ° 0.4 0.2 0.4 0 k 0 0 0 0 0.4 0.2 0.4 b 0 0 M Example INTERN IN Example 54 $ 5.5 54 , 53 , 52 Detected in frame_726.jpg: Example Mars Rover Markov Chain Episodes
12
Example Sample episodes starting from 54
lia
E
01
55
Ma
ут Example Mars Rover Markov Chain Episodes 12 Example Sample episodes starting from 54 lia E 01 55 Ma ут Detected in frame_727.jpg: Example: Mars Rover Markov Chain Episodes
S₁
0
52
0.4
06
12
$3
S4
A
0.4
A
$5
D4
$6
ம
57
2
1X2
02
0.6
Example: Sample episodes starting from $4
54.55, 56, 57, 57.57.
S4, 54, 55, 54, 55, 56....
54. 53, 52, 5....
NI Example : Mars Rover Markov Chain Episodes S₁ 0 52 0.4 06 12 $ 3 S4 A 0.4 A $ 5 D4 $ 6 ம 57 2 1X2 02 0.6 Example : Sample episodes starting from $ 4 54.55 , 56 , 57 , 57.57 . S4 , 54 , 55 , 54 , 55 , 56 .... 54. 53 , 52 , 5 .... NI Detected in frame_728.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
$2
$3
S4
S5
56
$7
04
0.4
0.4
0,4
0.4
0.4
0.4
0.4
0.6:
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, 55, 56, 57, 57, 57,...
54, 54, 55, 54, 55, 56....
S4, 53. 52. 51....
Sen M
IN Example : Mars Rover Markov Chain Episodes $ 1 0.4 $ 2 $ 3 S4 S5 56 $ 7 04 0.4 0.4 0,4 0.4 0.4 0.4 0.4 0.6 : 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , 55 , 56 , 57 , 57 , 57 , ... 54 , 54 , 55 , 54 , 55 , 56 .... S4 , 53. 52. 51 .... Sen M IN Detected in frame_729.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
S5
56
$7
0.4
04
0.4
0.4
0,4
0.4
0.4
0.4
0.4
0.4
0.6
02
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, 55, 56, 57, 57, 57,...
54, 54, 55, 54, 55, 56....
S4, 53. 52. 51....
Sen M
855..
N
' Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 S5 56 $ 7 0.4 04 0.4 0.4 0,4 0.4 0.4 0.4 0.4 0.4 0.6 02 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , 55 , 56 , 57 , 57 , 57 , ... 54 , 54 , 55 , 54 , 55 , 56 .... S4 , 53. 52. 51 .... Sen M 855 .. N ' Detected in frame_730.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
Sz
$3
S4
$5
$6
$7
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
⚫S4, S5, S6, S7, 57, 57,...
• 54, 54, 55, 54, 55, 56....
S4, 53, 52, 51....
Stop Screen Mirroring Document
Screenshot
Focus
NNNN
+ Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 Sz $ 3 S4 $ 5 $ 6 $ 7 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 ⚫S4 , S5 , S6 , S7 , 57 , 57 , ... • 54 , 54 , 55 , 54 , 55 , 56 .... S4 , 53 , 52 , 51 .... Stop Screen Mirroring Document Screenshot Focus NNNN + Detected in frame_731.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
55
56
$7
0.4
04
0.4
0.4
0.4
0.4
0.4
34
0.4
0.4
0.4
0.6
02
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57,...
S4, S4, 55, 54, 55, 56,
54, 53, 52.51....
SM
D
50 России
IN
NINN
+ Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 55 56 $ 7 0.4 04 0.4 0.4 0.4 0.4 0.4 34 0.4 0.4 0.4 0.6 02 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , 55 , 54 , 55 , 56 , 54 , 53 , 52.51 .... SM D 50 России IN NINN + Detected in frame_732.jpg: Example: Mars Rover Markov Chain Episodes
$1
04
0.4
Sz
$3
S4
$5
S6
$7
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
⚫S4, 54, 55, 54, 55, 56.....
S4, 53, 52, 51.....
Screen Mog D
Screenshot
Foc
NNNN
" Example : Mars Rover Markov Chain Episodes $ 1 04 0.4 Sz $ 3 S4 $ 5 S6 $ 7 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... ⚫S4 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52 , 51 ..... Screen Mog D Screenshot Foc NNNN " Detected in frame_733.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
$5
$6
$7
0.4
0.4
0.4
0.4
04
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
S4, S4, S5, S4, S5, S6,
S4, 53, 52, 51....
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN
' Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 $ 5 $ 6 $ 7 0.4 0.4 0.4 0.4 04 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , S4 , 53 , 52 , 51 .... Stop Screen Mirroring Document Screenshot Focus NNNN ' Detected in frame_734.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
04
$3
S4
$5
$6
$7
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from $4
54, 55, 56, 57, 57, 57,...
S4, 54, 55, 54, 55, 56,
S4, 53, 52, 51....
Screen M
D
Scho
Рос
NININN
. Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 04 $ 3 S4 $ 5 $ 6 $ 7 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from $ 4 54 , 55 , 56 , 57 , 57 , 57 , ... S4 , 54 , 55 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... Screen M D Scho Рос NININN . Detected in frame_735.jpg: Example: Mars Rover Markov Chain Episodes
$1
2
Sz
$3
SA
$5
$6
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, S5, S6,
S4, 53, 52, 51....
Screen Ming
Screenshot
Focus
NNNN
" Example : Mars Rover Markov Chain Episodes $ 1 2 Sz $ 3 SA $ 5 $ 6 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , S4 , 53 , 52 , 51 .... Screen Ming Screenshot Focus NNNN " Detected in frame_736.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
$3
S4
$5
$6
S7
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, 57, 57, 57,...
S4, 54, 55, 54, 55, 56,
S4, 53, 52, 51....
Stop
Screen Mirroring Document
Screenshot
focus
NNNN
' Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 $ 3 S4 $ 5 $ 6 S7 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , 57 , 57 , 57 , ... S4 , 54 , 55 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... Stop Screen Mirroring Document Screenshot focus NNNN ' Detected in frame_737.jpg: Example: Mars Rover Markov Chain Episodes
S₁
0.4
0.4
$2
06
0.2
53
SA
D4
DA
0.4
$5
$6
57
04
0.4
A
10.4
0.4
0.2
0.2
0.2
0.6
Example: Sample episodes starting from $4
54. $5, 56, 57, 57.57....
S4, S4, S5. 54, 55, 56,
54. 53, 52.51....
A A
15 Example : Mars Rover Markov Chain Episodes S₁ 0.4 0.4 $ 2 06 0.2 53 SA D4 DA 0.4 $ 5 $ 6 57 04 0.4 A 10.4 0.4 0.2 0.2 0.2 0.6 Example : Sample episodes starting from $ 4 54. $ 5 , 56 , 57 , 57.57 .... S4 , S4 , S5 . 54 , 55 , 56 , 54. 53 , 52.51 .... A A 15 Detected in frame_738.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
$3
S4
$5
56
$7
04
0.4
DA
0.4
0.4
3.4
0.4
0.4
0.4
0.6
0.2
02
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, 56, 57, 57, 57,...
S4, 54, 55, 54, 55, 56....
S4, 53, 52, 51.....
Shop
Son M
20 Рос Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 $ 3 S4 $ 5 56 $ 7 04 0.4 DA 0.4 0.4 3.4 0.4 0.4 0.4 0.6 0.2 02 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , 56 , 57 , 57 , 57 , ... S4 , 54 , 55 , 54 , 55 , 56 .... S4 , 53 , 52 , 51 ..... Shop Son M 20 Рос Detected in frame_739.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
Sz
$3
S4
$5
$6
$7
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, 57, 57, 57,...
S4, S4, S5, S4, S5, S6,...
S4, 53, 52, 51.....
Screening Ос
18 Росси
+ Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 Sz $ 3 S4 $ 5 $ 6 $ 7 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , ... S4 , 53 , 52 , 51 ..... Screening Ос 18 Росси + Detected in frame_740.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
$5
$6
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
• S4, 54, 55, 54, 55, 56,...
S4, 53, 52, 51.....
Screen Mirroring Document
Screenshot
Focus
"
IN IN INN Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 $ 5 $ 6 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... • S4 , 54 , 55 , 54 , 55 , 56 , ... S4 , 53 , 52 , 51 ..... Screen Mirroring Document Screenshot Focus " IN IN INN Detected in frame_741.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
$5
$6
57
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
• 54, 55, 56, 57, 57, 57,...
S4, 54, 55, 54, 55, 56,
S4, 53, 52, 51....
Screening D
Scho
Foc
'
NININN Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 $ 5 $ 6 57 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 • 54 , 55 , 56 , 57 , 57 , 57 , ... S4 , 54 , 55 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... Screening D Scho Foc ' NININN Detected in frame_742.jpg: lecture
Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
Sz
$3
S4
$5
$6
$7
04
0.4
0.4
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
⚫54, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, S5, S6,
S4, 53, 52, 51....
Stop
Screen Mirroring Document
Screenshot
Focus
'
INN INN lecture Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 Sz $ 3 S4 $ 5 $ 6 $ 7 04 0.4 0.4 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 ⚫54 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , S4 , 53 , 52 , 51 .... Stop Screen Mirroring Document Screenshot Focus ' INN INN Detected in frame_743.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
$4
S5
56
$7
0.4
04
0.4
DA
0.4
0.4
0.4
3.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
S4, 54, 55, 54, 55, 56,
S4, 53, 52.51....
F
'
IN
NNN Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 $ 4 S5 56 $ 7 0.4 04 0.4 DA 0.4 0.4 0.4 3.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... S4 , 54 , 55 , 54 , 55 , 56 , S4 , 53 , 52.51 .... F ' IN NNN Detected in frame_744.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
0.4
$3
S4
$5
S6
$7
0.4
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, S5, S6,..
S4, 53, 52, 51....
Screen M
Свещени
Screenshot
Гос
NANN
' Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 0.4 $ 3 S4 $ 5 S6 $ 7 0.4 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , .. S4 , 53 , 52 , 51 .... Screen M Свещени Screenshot Гос NANN ' Detected in frame_745.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
$3
S4
$5
$6
57
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from $4
54, 55, 56, 57, 57, 57,...
⚫S4, 54, 55, 54, 55, 56.....
S4, 53, 52, 51....
Shop Screening Со
Scho
Foc
'
RININN Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 $ 3 S4 $ 5 $ 6 57 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from $ 4 54 , 55 , 56 , 57 , 57 , 57 , ... ⚫S4 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52 , 51 .... Shop Screening Со Scho Foc ' RININN Detected in frame_746.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
S5
$6
$7
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from $4
S4, S5, S6, S7, 57, 57,...
• S4, 54, 55, 54, 55, 56....
S4, 53, 52, 51.....
Screening D
Screenshot
NNNN
" Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 S5 $ 6 $ 7 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from $ 4 S4 , S5 , S6 , S7 , 57 , 57 , ... • S4 , 54 , 55 , 54 , 55 , 56 .... S4 , 53 , 52 , 51 ..... Screening D Screenshot NNNN " Detected in frame_747.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
$5
$6
57
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, S5, S6,
S4, 53, 52, 51....
Stop
Screen Mirroring Document
Screenshot
Focus
NN N
" Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 $ 5 $ 6 57 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , S4 , 53 , 52 , 51 .... Stop Screen Mirroring Document Screenshot Focus NN N " Detected in frame_748.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
$3
S4
$5
$6
57
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
10.4
0.4
0.6
0.2
0,2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from $4
• 54, 55, 56, 57, 57, 57....
S4, 54, 55, 54, 55, 56.....
S4, 53, 52, 51....
Seven Mong Doct
Focu
'
RININN Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 $ 3 S4 $ 5 $ 6 57 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 10.4 0.4 0.6 0.2 0,2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from $ 4 • 54 , 55 , 56 , 57 , 57 , 57 .... S4 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52 , 51 .... Seven Mong Doct Focu ' RININN Detected in frame_749.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
S5
$6
$7
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from $4
S4, S5, S6, S7, 57, 57,...
⚫S4, 54, 55, 54, 55, 56...
S4, 53, 52, 51.....
Screen Moving Document
Screenshot
A Focus
NNNN
" Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 S5 $ 6 $ 7 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from $ 4 S4 , S5 , S6 , S7 , 57 , 57 , ... ⚫S4 , 54 , 55 , 54 , 55 , 56 ... S4 , 53 , 52 , 51 ..... Screen Moving Document Screenshot A Focus NNNN " Detected in frame_750.jpg: Example: Mars Rover Markov Chain Episodes
$1
S2
$3
S4
$5
$6
$7
0.4
04
0.4
0.4
04
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
⚫ 54, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, S5, S6, ..
S4, 53, 52, 51....
Stop
Screen Mirroring Document
Screenshot
Focus
IN IN INN Example : Mars Rover Markov Chain Episodes $ 1 S2 $ 3 S4 $ 5 $ 6 $ 7 0.4 04 0.4 0.4 04 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 ⚫ 54 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , .. S4 , 53 , 52 , 51 .... Stop Screen Mirroring Document Screenshot Focus IN IN INN Detected in frame_751.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
Sz
$3
S4
$5
$6
57
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57,...
• S4, 54, 55, 54, 55, 56.....
S4, 53, 52, 51....
Screen Ming D
Form Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 Sz $ 3 S4 $ 5 $ 6 57 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 , ... • S4 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52 , 51 .... Screen Ming D Form Detected in frame_752.jpg: D
Example: Mars Rover Markov Chain Episodes
$1
0.4
S2
$3
S4
$5
$6
$7
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
10.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
⚫54, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, S5, S6, ..
S4, 53, 52, 51....
Screen Ming D
Screenshot
FOCUS
"
INNINN D Example : Mars Rover Markov Chain Episodes $ 1 0.4 S2 $ 3 S4 $ 5 $ 6 $ 7 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 10.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 ⚫54 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , .. S4 , 53 , 52 , 51 .... Screen Ming D Screenshot FOCUS " INNINN Detected in frame_753.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
S2
$3
S4
$5
$6
$7
04
0.4
0.4
04
0.4
0.4
10.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, S5, S6, ..
S4, 53, 52, 51....
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN Example : Mars Rover Markov Chain Episodes $ 1 0.4 S2 $ 3 S4 $ 5 $ 6 $ 7 04 0.4 0.4 04 0.4 0.4 10.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , .. S4 , 53 , 52 , 51 .... Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_754.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
$5
$6
57
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
10.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57,...
• S4, 54, 55, 54, 55, 56.....
S4, 53, 52, 51....
Screen M
D
Form
'
NOININ Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 $ 5 $ 6 57 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 10.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 , ... • S4 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52 , 51 .... Screen M D Form ' NOININ Detected in frame_755.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
Sz
K
$3
S4
$6
$7
0.4
0,4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
10.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
S4, S4, S5, S4, S5, S6,
S4, 53, 52, 51....
Screen Mon
Саб
Screenshot
Focus
NNNN
" Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 Sz K $ 3 S4 $ 6 $ 7 0.4 0,4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 10.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , S4 , 53 , 52 , 51 .... Screen Mon Саб Screenshot Focus NNNN " Detected in frame_756.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
$2
04
$3
S4
$5
$6
$7
0.4
0.4
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, 55, 56, 57, 57, 57,...
S4, 54, 55, 54, 55, 56,
S4, 53, 52, 51....
Stop
Screen Mirroring Document
Screenshot
focu
NNNN
+ Example : Mars Rover Markov Chain Episodes $ 1 0.4 $ 2 04 $ 3 S4 $ 5 $ 6 $ 7 0.4 0.4 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , 55 , 56 , 57 , 57 , 57 , ... S4 , 54 , 55 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... Stop Screen Mirroring Document Screenshot focu NNNN + Detected in frame_757.jpg: Example: Mars Rover Markov Chain Episodes
S₁
0.4
0:4
$2
DU
0.4
53
$4.
$5
$6
ST
0,4
0.4
04
04
0.4
0.4
04
.06
0.2
0.Z
02
0.2
0:2
0.6
Example: Sample episodes starting from $4
S4, S5, 56, 57, 57, 57-
54, S4, 55, 54, 55, 56,
54.53, 52, 51...
..
15 Example : Mars Rover Markov Chain Episodes S₁ 0.4 0 : 4 $ 2 DU 0.4 53 $ 4 . $ 5 $ 6 ST 0,4 0.4 04 04 0.4 0.4 04 .06 0.2 0.Z 02 0.2 0 : 2 0.6 Example : Sample episodes starting from $ 4 S4 , S5 , 56 , 57 , 57 , 57- 54 , S4 , 55 , 54 , 55 , 56 , 54.53 , 52 , 51 ... .. 15 Detected in frame_758.jpg: T
Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
04
$3
S4
$5
56
$7
0.4
0.4
0.4
0.4
0.4
3.4
0.4
0.4
0.4
0.6
0.2
0.2
0.21
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57,...
S4, S4, S5, 54, 55, 56,
S4, 53, 52, 51....
S T Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 04 $ 3 S4 $ 5 56 $ 7 0.4 0.4 0.4 0.4 0.4 3.4 0.4 0.4 0.4 0.6 0.2 0.2 0.21 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... S Detected in frame_759.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
S2
$3
S4
$5
$6
$7
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, 57, 57, 57,...
S4, S4, 55, 54, 55, 56,
S4, 53, 52, 51....
Screening D
Screenshot
Focus
"
INNINN Example : Mars Rover Markov Chain Episodes $ 1 0.4 S2 $ 3 S4 $ 5 $ 6 $ 7 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , 57 , 57 , 57 , ... S4 , S4 , 55 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... Screening D Screenshot Focus " INNINN Detected in frame_760.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
S2
$3
S4
$5
$6
57
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
S4, S4, S5, 54, 55, 56,
S4, 53, 52, 51....
Screen Mirroring Document
Screenshot
IN IN INN
> Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 S2 $ 3 S4 $ 5 $ 6 57 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... S4 , S4 , S5 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... Screen Mirroring Document Screenshot IN IN INN > Detected in frame_761.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
$3
S4
$5
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
$6
0.4
0.4
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
54, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, S5, S6,
S4, 53, 52, 51....
Screen Mon
Document
Fec
+
IN
ININ N Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 $ 3 S4 $ 5 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 $ 6 0.4 0.4 $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 54 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , S4 , 53 , 52 , 51 .... Screen Mon Document Fec + IN ININ N Detected in frame_762.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
S5
$6
$7
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
10.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
• 54, 54, 55, 54, 55, 56.....
S4, 53, 52, 51.....
Stop S
D
Screenshot
Focus
NNNN
" Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 S5 $ 6 $ 7 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 10.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... • 54 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52 , 51 ..... Stop S D Screenshot Focus NNNN " Detected in frame_763.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
$5
$6
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, 57, 57, 57,...
S4, S4, S5, 54, 55, 56...
S4, 53, 52, 51....
Screen Mirroring Document
Screenshot
Focus
I. IN INN Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 $ 5 $ 6 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , 57 , 57 , 57 , ... S4 , S4 , S5 , 54 , 55 , 56 ... S4 , 53 , 52 , 51 .... Screen Mirroring Document Screenshot Focus I. IN INN Detected in frame_764.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
$5
$6
$7
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from $4
S4, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, S5, S6,
S4, 53, 52, 51....
Screen Mon
Document
Focu
IN
+ Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 $ 5 $ 6 $ 7 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from $ 4 S4 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , S4 , 53 , 52 , 51 .... Screen Mon Document Focu IN + Detected in frame_765.jpg: 2-1
Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
$3
S4
$5
$6
$7
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
• 54, 54, 55, 54, 55, 56.....
S4, 53, 52, 51....
Screening Document
Screenshot
Focus
NNNN
" 2-1 Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 $ 3 S4 $ 5 $ 6 $ 7 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... • 54 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52 , 51 .... Screening Document Screenshot Focus NNNN " Detected in frame_766.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
$5
$6
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
S4, S4, S5, S4, S5, S6,
S4, 53, 52, 51....
Screen Mirroring Document
Screenshot
A Focus
NNNN
" Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 $ 5 $ 6 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , S4 , 53 , 52 , 51 .... Screen Mirroring Document Screenshot A Focus NNNN " Detected in frame_767.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
$5
$6
57
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
⚫54, 55, 56, 57, 57, 57,...
• 54, 54, 55, 54, 55, 56....
S4, 53, 52, 51....
S
Screen M
Document
Screenshot
A Focus Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 $ 5 $ 6 57 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 ⚫54 , 55 , 56 , 57 , 57 , 57 , ... • 54 , 54 , 55 , 54 , 55 , 56 .... S4 , 53 , 52 , 51 .... S Screen M Document Screenshot A Focus Detected in frame_768.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
S5
$6
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
10.4
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
• 54, 54, 55, 54, 55, 56.....
S4, 53, 52, 51....
Screen M
Document
Screenshot
NNNN Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 S5 $ 6 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 10.4 $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... • 54 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52 , 51 .... Screen M Document Screenshot NNNN Detected in frame_769.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
$5
$6
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, S7, 57, 57,...
S4, S4, S5, S4, S5, S6, .
S4, 53, 52, 51.....
Stop
Screen Mirroring Document
Screenshot
Focus
NNNN
" Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 $ 5 $ 6 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , S7 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , . S4 , 53 , 52 , 51 ..... Stop Screen Mirroring Document Screenshot Focus NNNN " Detected in frame_770.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
$5
$6
$7
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
10.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
• 54, 55, 56, 57, 57, 57....
•S4, 54, 55, 54, 55, 56,
S4, 53, 52, 51....
Seven Mong Ос
'
IN Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 $ 5 $ 6 $ 7 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 10.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 • 54 , 55 , 56 , 57 , 57 , 57 .... • S4 , 54 , 55 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... Seven Mong Ос ' IN Detected in frame_771.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
Sz
$3
S4
S5
$6
$7
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
⚫54, 55, 56, 57, 57, 57,...
• S4, 54, 55, 54, 55, 56.....
S4, 53, 52, 51.....
Screening Document
Screenshot
Focus
NNNN
" Example : Mars Rover Markov Chain Episodes $ 1 0.4 Sz $ 3 S4 S5 $ 6 $ 7 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 ⚫54 , 55 , 56 , 57 , 57 , 57 , ... • S4 , 54 , 55 , 54 , 55 , 56 ..... S4 , 53 , 52 , 51 ..... Screening Document Screenshot Focus NNNN " Detected in frame_772.jpg: Рос
Example: Mars Rover Markov Chain Episodes
$1
2-2
S₂
$3
S4
$5
$6
$7
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
• 54, 55, 56, 57, 57, 57,...
S4, S4, S5, S4, S5, S6,
S4, S3, 52, 51....
Stop
Screen Mirroring Document
Screenshot
focus
NNNN
' Рос Example : Mars Rover Markov Chain Episodes $ 1 2-2 S₂ $ 3 S4 $ 5 $ 6 $ 7 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 • 54 , 55 , 56 , 57 , 57 , 57 , ... S4 , S4 , S5 , S4 , S5 , S6 , S4 , S3 , 52 , 51 .... Stop Screen Mirroring Document Screenshot focus NNNN ' Detected in frame_773.jpg: Example: Mars Rover Markov Chain Episodes
S₁
0.4
$2
DA
53
0.4
A
SA
0.4
06
02
$5
S6
ST
04
0.4
04
0.4
0.2
0.2
0.2
0.6
Example: Sample episodes starting from $4
54. $5, 56, 57, 57.57....
54, 54, 55, 54, 55, 56,.....
54. 53, 52, 51....
TY IN N
3 Example : Mars Rover Markov Chain Episodes S₁ 0.4 $ 2 DA 53 0.4 A SA 0.4 06 02 $ 5 S6 ST 04 0.4 04 0.4 0.2 0.2 0.2 0.6 Example : Sample episodes starting from $ 4 54. $ 5 , 56 , 57 , 57.57 .... 54 , 54 , 55 , 54 , 55 , 56 , ..... 54. 53 , 52 , 51 .... TY IN N 3 Detected in frame_774.jpg: Example: Mars Rover Markov Chain Episodes
$1
$2
$3
S4
$5
56
$7
0.4
04
0.4
0.4
0.4
0.4
0.4
3:4
0.4
0.4
0.4
0.6
02
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, S6, 57, 57, 57,...
S4, S4, 55, 54, 55, 56,
S4, 53, 52, 51....
SM
'
IS Example : Mars Rover Markov Chain Episodes $ 1 $ 2 $ 3 S4 $ 5 56 $ 7 0.4 04 0.4 0.4 0.4 0.4 0.4 3 : 4 0.4 0.4 0.4 0.6 02 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , S6 , 57 , 57 , 57 , ... S4 , S4 , 55 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... SM ' IS Detected in frame_775.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
$2
04
0.4
$3
S4
$5
56
$7
0.4
0.4
0.4
0.4
3.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, S5, 56, 57, 57, 57,...
S4, S4, 55, 54, 55, 56,
S4, 53, 52, 51....
S
EX Focu
NININN
+ Example : Mars Rover Markov Chain Episodes $ 1 0.4 $ 2 04 0.4 $ 3 S4 $ 5 56 $ 7 0.4 0.4 0.4 0.4 3.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , S5 , 56 , 57 , 57 , 57 , ... S4 , S4 , 55 , 54 , 55 , 56 , S4 , 53 , 52 , 51 .... S EX Focu NININN + Detected in frame_776.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
$5
$6
57
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0,2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, 55, 56, 57, 57, 57,...
• S4, 54, 55, 54, 55, 56....
S4, 53, 52, 51.....
S
Screening D
Foc
'
NININ Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 $ 5 $ 6 57 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0,2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , 55 , 56 , 57 , 57 , 57 , ... • S4 , 54 , 55 , 54 , 55 , 56 .... S4 , 53 , 52 , 51 ..... S Screening D Foc ' NININ Detected in frame_777.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
$5
$6
$7
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
10.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
• S4, 55, 56, 57, 57, 57,...
• S4, 54, 55, 54, 55, 56....
S4, 53, 52, 51.....
Stop Screen Mirroring Document
Screenshot
Focu
N. IN INN
+ Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 $ 5 $ 6 $ 7 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 10.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 • S4 , 55 , 56 , 57 , 57 , 57 , ... • S4 , 54 , 55 , 54 , 55 , 56 .... S4 , 53 , 52 , 51 ..... Stop Screen Mirroring Document Screenshot Focu N. IN INN + Detected in frame_778.jpg: Example: Mars Rover Markov Chain Episodes
$1
Sz
$3
S4
$5
S6
57
0.4
04
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from S4
S4, 55, 56, 57, 57, 57,...
S4, 54, 55, 54, 55, 56....
S4, 53, 52, 51....
S
Screening D
Fem
'
IN IN Example : Mars Rover Markov Chain Episodes $ 1 Sz $ 3 S4 $ 5 S6 57 0.4 04 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from S4 S4 , 55 , 56 , 57 , 57 , 57 , ... S4 , 54 , 55 , 54 , 55 , 56 .... S4 , 53 , 52 , 51 .... S Screening D Fem ' IN IN Detected in frame_779.jpg: Example: Mars Rover Markov Chain Episodes
$1
0.4
0.4
$2
$3
S4
S5
$6
04
0.4
0.4
04
0.4
0.4
0.4
0.4
0.4
10.4
$7
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Example: Sample episodes starting from $4
S4, S5, S6, S7, 57, 57,...
• S4, 54, 55, 54, 55, 56...
S4, 53, 52, 51....
Screen Mirroring Document
Screenshot
Focus
" Example : Mars Rover Markov Chain Episodes $ 1 0.4 0.4 $ 2 $ 3 S4 S5 $ 6 04 0.4 0.4 04 0.4 0.4 0.4 0.4 0.4 10.4 $ 7 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Example : Sample episodes starting from $ 4 S4 , S5 , S6 , S7 , 57 , 57 , ... • S4 , 54 , 55 , 54 , 55 , 56 ... S4 , 53 , 52 , 51 .... Screen Mirroring Document Screenshot Focus " Detected in frame_785.jpg: Jens May P Jens May P Detected in frame_786.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
Definition of Markov Reward Process: (MRP)
* 5. is a (finite) set of states (s = 5)
P is dynamics/transition model that specifices P(51-51s = 5)
⚫is a reward function R(ss) = |||
Discount factor=[0,1]
Note no actions
If finite number (N) of states, can express R as a vector
الك Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards Definition of Markov Reward Process : ( MRP ) * 5. is a ( finite ) set of states ( s = 5 ) P is dynamics / transition model that specifices P ( 51-51s = 5 ) ⚫is a reward function R ( ss ) = ||| Discount factor = [ 0,1 ] Note no actions If finite number ( N ) of states , can express R as a vector الك Detected in frame_787.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
⚫P is dynamics/transition model that specifices P(5+1 55, = 5)
R is a reward function R(ss) =E[rts, = 5]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
S Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = ⚫P is dynamics / transition model that specifices P ( 5 + 1 55 , = 5 ) R is a reward function R ( ss ) = E [ rts , = 5 ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector S Detected in frame_788.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫ S is a (finite) set of states (s = 5)
==
⚫P is dynamics/transition model that specifices P(St+1 =s' st = s)
R is a reward function R(sts) =E[rt|st = s]
Discount factory [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screen Mig D
Screenshot
From Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫ S is a ( finite ) set of states ( s = 5 ) == ⚫P is dynamics / transition model that specifices P ( St + 1 = s ' st = s ) R is a reward function R ( sts ) = E [ rt | st = s ] Discount factory [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screen Mig D Screenshot From Detected in frame_789.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫ S is a (finite) set of states (s = 5)
==
⚫P is dynamics/transition model that specifices P(St+1 =s' st = s)
R is a reward function R(sts) =E[rt|st = s]
Discount factory [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
San Mig D
Screenshot
From Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫ S is a ( finite ) set of states ( s = 5 ) == ⚫P is dynamics / transition model that specifices P ( St + 1 = s ' st = s ) R is a reward function R ( sts ) = E [ rt | st = s ] Discount factory [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector San Mig D Screenshot From Detected in frame_790.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
•P is dynamics/transition model that specifices P(St+1 = 5' 5 = 5)
R is a reward function R(ss) =E[rt|st = 5]
Discount factor [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screen Microring
Screenshot
A focus
'
INIS IN Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) • P is dynamics / transition model that specifices P ( St + 1 = 5 ' 5 = 5 ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screen Microring Screenshot A focus ' INIS IN Detected in frame_791.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
•P is dynamics/transition model that specifices P(5+1 5' 5, = 5)
⚫R is a reward function R(ss) =E[rist = 5]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
IN
NNNN Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = • P is dynamics / transition model that specifices P ( 5 + 1 5 ' 5 , = 5 ) ⚫R is a reward function R ( ss ) = E [ rist = 5 ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector IN NNNN Detected in frame_792.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
⚫P is dynamics/transition model that specifices P(S+1 ss = s)
R is a reward function R(ss) =E[rt|st = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Scre
D
Scree
Рос Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = ⚫P is dynamics / transition model that specifices P ( S + 1 ss = s ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Scre D Scree Рос Detected in frame_793.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
=
•P is dynamics/transition model that specifices P(St+1 s'|5, = 5)
R is a reward function R(ss) =E[rt|st = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Screen Mirroring Document
Screenshot
+] Focus
NININN Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) = • P is dynamics / transition model that specifices P ( St + 1 s ' | 5 , = 5 ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Screen Mirroring Document Screenshot + ] Focus NININN Detected in frame_794.jpg: Markov Reward Process (MRP)
NNNN
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
=
•P is dynamics/transition model that specifices P(St+1 s'|5, = 5)
R is a reward function R(ss) =E[rt|st = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Screen Mirroring Document
Screenshot
+] Focus Markov Reward Process ( MRP ) NNNN • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) = • P is dynamics / transition model that specifices P ( St + 1 s ' | 5 , = 5 ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Screen Mirroring Document Screenshot + ] Focus Detected in frame_795.jpg: Markov Reward Process (MRP)
黑
NNN..
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
⚫P is dynamics/transition model that specifices P(St+1 =s' st = s)
R is a reward function R(ss) =E[rt|st = s]
Discount factor [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Sto Soren Mo
Осення
Рос Markov Reward Process ( MRP ) 黑 NNN .. • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) ⚫P is dynamics / transition model that specifices P ( St + 1 = s ' st = s ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Sto Soren Mo Осення Рос Detected in frame_796.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
=
•P is dynamics/transition model that specifices P(St+1 s'|5, = 5)
R is a reward function R(s, s) =E[rt|st = 5]
Discount factor [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screen M
Dece
Screenshot
A Focus
NNNN
' Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) = • P is dynamics / transition model that specifices P ( St + 1 s ' | 5 , = 5 ) R is a reward function R ( s , s ) = E [ rt | st = 5 ] Discount factor [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screen M Dece Screenshot A Focus NNNN ' Detected in frame_797.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
•P is dynamics/transition model that specifices P(5+1 5' 5, = 5)
R is a reward function R(ss) =E[rt|st = 5]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Song
Р
IN
●ININN
" Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = • P is dynamics / transition model that specifices P ( 5 + 1 5 ' 5 , = 5 ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Song Р IN ● ININN " Detected in frame_798.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
• P is dynamics/transition model that specifices P(5+1 5' 5, = 5)
R is a reward function R(s; s) =E[rt|st = 5]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Song
Р
IN
NNN Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = • P is dynamics / transition model that specifices P ( 5 + 1 5 ' 5 , = 5 ) R is a reward function R ( s ; s ) = E [ rt | st = 5 ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Song Р IN NNN Detected in frame_799.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
• P is dynamics/transition model that specifices P(5+1 5' 5, = 5)
R is a reward function R(s; s) =E[rt|st = 5]
Discount factor [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
S
Р
IN
●NINN
" Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = • P is dynamics / transition model that specifices P ( 5 + 1 5 ' 5 , = 5 ) R is a reward function R ( s ; s ) = E [ rt | st = 5 ] Discount factor [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector S Р IN ● NINN " Detected in frame_800.jpg: Markov Reward Process (MRP)
NAN
N
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫ S is a (finite) set of states (s = S)
=
⚫P is dynamics/transition model that specifices P(St+1 s'|5, = 5)
R is a reward function R(s; s) =E[rt|st = 5]
Discount factor [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
S
D
Sor
Foca Markov Reward Process ( MRP ) NAN N • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫ S is a ( finite ) set of states ( s = S ) = ⚫P is dynamics / transition model that specifices P ( St + 1 s ' | 5 , = 5 ) R is a reward function R ( s ; s ) = E [ rt | st = 5 ] Discount factor [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector S D Sor Foca Detected in frame_801.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
• P is dynamics/transition model that specifices P(5+1 = 5' 5 = 5)
R is a reward function R(ss) =E[rt|st = 5]
Discount factor [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
NINN Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) • P is dynamics / transition model that specifices P ( 5 + 1 = 5 ' 5 = 5 ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector NINN Detected in frame_802.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
⚫P is dynamics/transition model that specifices P(S+1 s'|5, = 5)
R is a reward function R(ss) =E[rt|st = s]
Discount factory [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screen Meming D
Fec
ININ N Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = ⚫P is dynamics / transition model that specifices P ( S + 1 s ' | 5 , = 5 ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factory [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screen Meming D Fec ININ N Detected in frame_803.jpg: Markov Reward Process (MRP)
NNNN
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
⚫P is dynamics/transition model that specifices P(St+1 =s' st = s)
R is a reward function R(ss) =E[rt|st = s]
Discount factory [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screen Mirroring D
Screenshot
Focus Markov Reward Process ( MRP ) NNNN • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) ⚫P is dynamics / transition model that specifices P ( St + 1 = s ' st = s ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factory [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screen Mirroring D Screenshot Focus Detected in frame_804.jpg: Markov Reward Process (MRP)
NNNN
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
⚫P is dynamics/transition model that specifices P(St+1 =s' st = s)
R is a reward function R(ss) =E[rt|st = s]
Discount factory [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screen Mirroring D
Screenshot
Focus Markov Reward Process ( MRP ) NNNN • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) ⚫P is dynamics / transition model that specifices P ( St + 1 = s ' st = s ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factory [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screen Mirroring D Screenshot Focus Detected in frame_805.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
•Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
=
•P is dynamics/transition model that specifices P(St+1 s' st = s)
R is a reward function R(sts) =E[rt|st = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Screen Mirroring Document
Screenshot
Focus
NONNINN Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) = • P is dynamics / transition model that specifices P ( St + 1 s ' st = s ) R is a reward function R ( sts ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Screen Mirroring Document Screenshot Focus NONNINN Detected in frame_806.jpg: Markov Reward Process (MRP)
*
NNN
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
⚫P is dynamics/transition model that specifices P(St+1 s'|5, = s)
R is a reward function R(ss) =E[rt|st = s]
Discount factor [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screening D Markov Reward Process ( MRP ) * NNN • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = ⚫P is dynamics / transition model that specifices P ( St + 1 s ' | 5 , = s ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screening D Detected in frame_807.jpg: Markov Reward Process (MRP)
NNUN..
• Markov Reward Process is a Markov Chain + rewards
•Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
=
•P is dynamics/transition model that specifices P(St+1 s'|5, = 5)
R is a reward function R(ss) =E[rt|st = 5]
Discount factor
⚫ Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Screen Moving Document
Screenshot
A Focus Markov Reward Process ( MRP ) NNUN .. • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) = • P is dynamics / transition model that specifices P ( St + 1 s ' | 5 , = 5 ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor ⚫ Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Screen Moving Document Screenshot A Focus Detected in frame_808.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
•P is dynamics/transition model that specifices P(5+1 5'5, = 5)
R is a reward function R(ss) =E[rt|st = 5]
Discount factor [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
S
NNN Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = • P is dynamics / transition model that specifices P ( 5 + 1 5'5 , = 5 ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector S NNN Detected in frame_809.jpg: Markov Reward Process (MRP)
*
NNN
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
P is dynamics/transition model that specifices P(St+1 s'|5, = s)
R is a reward function R(ss) =E[rt|st = s]
Discount factory [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screening D
Роси Markov Reward Process ( MRP ) * NNN • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = P is dynamics / transition model that specifices P ( St + 1 s ' | 5 , = s ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factory [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screening D Роси Detected in frame_810.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫ S is a (finite) set of states (s = 5)
⚫P is dynamics/transition model that specifices P(St+1 = 5' 5 = 5)
R is a reward function R(sts) =E[rt|st = s]
Discount factor [0,1]
• Note: no actions
?
If finite number (N) of states, can express R as a vector
Screen M
Росси
'
NININN Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫ S is a ( finite ) set of states ( s = 5 ) ⚫P is dynamics / transition model that specifices P ( St + 1 = 5 ' 5 = 5 ) R is a reward function R ( sts ) = E [ rt | st = s ] Discount factor [ 0,1 ] • Note : no actions ? If finite number ( N ) of states , can express R as a vector Screen M Росси ' NININN Detected in frame_811.jpg: Markov Reward Process (MRP)
NNNN
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
=
⚫P is dynamics/transition model that specifices P(St+1 ss = s)
R is a reward function R(ss) =E[rt|st = 5]
Discount factor [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screening D
Screenshot
A Focus Markov Reward Process ( MRP ) NNNN • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) = ⚫P is dynamics / transition model that specifices P ( St + 1 ss = s ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screening D Screenshot A Focus Detected in frame_812.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
⚫P is dynamics/transition model that specifices P(5+1 5' 5, = 5)
⚫R is a reward function R(ss) =E[rist = s]
Discount factor€ [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
S
Foca
NNNN Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = ⚫P is dynamics / transition model that specifices P ( 5 + 1 5 ' 5 , = 5 ) ⚫R is a reward function R ( ss ) = E [ rist = s ] Discount factor € [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector S Foca NNNN Detected in frame_813.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s€ S)
=
⚫P is dynamics/transition model that specifices P(St+1 5'5, = 5)
R is a reward function R(s; s) =E[rt|st = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Россия
NNNN Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) = ⚫P is dynamics / transition model that specifices P ( St + 1 5'5 , = 5 ) R is a reward function R ( s ; s ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Россия NNNN Detected in frame_814.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫ S is a (finite) set of states (s = 5)
=
⚫P is dynamics/transition model that specifices P(St+1 5' 5 = 5)
R is a reward function R(s; s) =E[rt|st = 5]
Discount factor [0.1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screen M
D
Fem
'
ININ N Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫ S is a ( finite ) set of states ( s = 5 ) = ⚫P is dynamics / transition model that specifices P ( St + 1 5 ' 5 = 5 ) R is a reward function R ( s ; s ) = E [ rt | st = 5 ] Discount factor [ 0.1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screen M D Fem ' ININ N Detected in frame_815.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
• P is dynamics/transition model that specifices P(S+1 = 5' 5 = 5)
R is a reward function R(ss) =E[rt|st = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Some wit
Fec
NNNN Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) • P is dynamics / transition model that specifices P ( S + 1 = 5 ' 5 = 5 ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Some wit Fec NNNN Detected in frame_816.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
⚫P is dynamics/transition model that specifices P(St+1 s'|5, = 5)
R is a reward function R(sts) =E[rt|st = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Screen M
Спе
15 Русия
ININ N Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = ⚫P is dynamics / transition model that specifices P ( St + 1 s ' | 5 , = 5 ) R is a reward function R ( sts ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Screen M Спе 15 Русия ININ N Detected in frame_817.jpg: Markov Reward Process (MRP)
2-2
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = S)
⚫P is dynamics/transition model that specifices P(St+1 =s' st = s)
R is a reward function R(sts) =E[rt|st = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Stop Screen Mirroring Document
Screenshot
K] Focus
NININN Markov Reward Process ( MRP ) 2-2 • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = S ) ⚫P is dynamics / transition model that specifices P ( St + 1 = s ' st = s ) R is a reward function R ( sts ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Stop Screen Mirroring Document Screenshot K ] Focus NININN Detected in frame_818.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫ S is a (finite) set of states (s = 5)
=
•P is dynamics/transition model that specifices P(5+1 5' 5, = 5)
R is a reward function R(ss) =E[rt|st = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Some M
●ININN Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫ S is a ( finite ) set of states ( s = 5 ) = • P is dynamics / transition model that specifices P ( 5 + 1 5 ' 5 , = 5 ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Some M ● ININN Detected in frame_819.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
⚫P is dynamics/transition model that specifices P(S+1 ss = s)
R is a reward function R(sts) =E[rt|st = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Screen M
Се
15 Русия
ININ N Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = ⚫P is dynamics / transition model that specifices P ( S + 1 ss = s ) R is a reward function R ( sts ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Screen M Се 15 Русия ININ N Detected in frame_820.jpg: Markov Reward Process (MRP)
2-2
x
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
•P is dynamics/transition model that specifices P(S++1 = s' st = s)
R is a reward function R(ss) =E[rt|st = 5]
Discount factor€ [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screen Mirroring Document
Screenshot
Focus
NININN Markov Reward Process ( MRP ) 2-2 x • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) • P is dynamics / transition model that specifices P ( S ++ 1 = s ' st = s ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor € [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screen Mirroring Document Screenshot Focus NININN Detected in frame_821.jpg: Markov Reward Process (MRP)
-
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
•P is dynamics/transition model that specifices P(5+1 5' 5, = 5)
R is a reward function R(ss) =E[rt|st = 5]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Doc
IS
NNNN Markov Reward Process ( MRP ) - • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = • P is dynamics / transition model that specifices P ( 5 + 1 5 ' 5 , = 5 ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Doc IS NNNN Detected in frame_822.jpg: Markov Reward Process (MRP)
мека
x
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
⚫P is dynamics/transition model that specifices P(St+1 s'|5, = 5)
R is a reward function R(ss) =E[rist = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Song Document
Scho
Рос
ININ N Markov Reward Process ( MRP ) мека x • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = ⚫P is dynamics / transition model that specifices P ( St + 1 s ' | 5 , = 5 ) R is a reward function R ( ss ) = E [ rist = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Song Document Scho Рос ININ N Detected in frame_823.jpg: ст
Markov Reward Process (MRP)
2-1
NANN
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
⚫P is dynamics/transition model that specifices P(St+1 =s' st = s)
R is a reward function R(ss) =E[rt|st = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Screening
Screenshot
1.1 Роси ст Markov Reward Process ( MRP ) 2-1 NANN • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) ⚫P is dynamics / transition model that specifices P ( St + 1 = s ' st = s ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Screening Screenshot 1.1 Роси Detected in frame_824.jpg: Markov Reward Process (MRP)
мека
x
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
⚫P is dynamics/transition model that specifices P(St+1 s'|5, = 5)
R is a reward function R(sts) =E[rt|st = s]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
Song Document
Рос
ININ N Markov Reward Process ( MRP ) мека x • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = ⚫P is dynamics / transition model that specifices P ( St + 1 s ' | 5 , = 5 ) R is a reward function R ( sts ) = E [ rt | st = s ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector Song Document Рос ININ N Detected in frame_825.jpg: Markov Reward Process (MRP)
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
=
⚫P is dynamics/transition model that specifices P(S+1 ss = s)
R is a reward function R(s;
Discount factor [0,1]
• Note: no actions
s)=E[rt|st = 5]
If finite number (N) of states, can express R as a vector
Stop Screen Mo
Document
Screenshot
K] Focus
NIS
N Markov Reward Process ( MRP ) • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) = ⚫P is dynamics / transition model that specifices P ( S + 1 ss = s ) R is a reward function R ( s ; Discount factor [ 0,1 ] • Note : no actions s ) = E [ rt | st = 5 ] If finite number ( N ) of states , can express R as a vector Stop Screen Mo Document Screenshot K ] Focus NIS N Detected in frame_826.jpg: Markov Reward Process (MRP)
WCL-
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
•P is dynamics/transition model that specifices P(5+1 5' 5, = 5)
R is a reward function R(ss) =E[rts, = 5]
Discount factor
• Note: no actions
[0,1]
If finite number (N) of states, can express R as a vector
S
160 России Markov Reward Process ( MRP ) WCL- • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = • P is dynamics / transition model that specifices P ( 5 + 1 5 ' 5 , = 5 ) R is a reward function R ( ss ) = E [ rts , = 5 ] Discount factor • Note : no actions [ 0,1 ] If finite number ( N ) of states , can express R as a vector S 160 России Detected in frame_827.jpg: Markov Reward Process (MRP)
NNN..
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
⚫P is dynamics/transition model that specifices P(St+1 ss = s)
R is a reward function R(ss) =E[rt|st = s]
Discount factory [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screen Добиени
Focus Markov Reward Process ( MRP ) NNN .. • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = ⚫P is dynamics / transition model that specifices P ( St + 1 ss = s ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factory [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screen Добиени Focus Detected in frame_828.jpg: Markov Reward Process (MRP)
NNN..
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s = 5)
=
⚫P is dynamics/transition model that specifices P(S+1 ss = s)
R is a reward function R(ss) =E[rt|st = s]
Discount factory [0,1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screen M
D
Focus Markov Reward Process ( MRP ) NNN .. • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s = 5 ) = ⚫P is dynamics / transition model that specifices P ( S + 1 ss = s ) R is a reward function R ( ss ) = E [ rt | st = s ] Discount factory [ 0,1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screen M D Focus Detected in frame_829.jpg: Markov Reward Process (MRP)
2-1
• Markov Reward Process is a Markov Chain + rewards
• Definition of Markov Reward Process (MRP)
⚫S is a (finite) set of states (s € S)
=
•P is dynamics/transition model that specifices P(St+1 s'|5, = s)
R is a reward function R(ss) =E[rt|st = 5]
Discount factor [0.1]
• Note: no actions
If finite number (N) of states, can express R as a vector
Screening Docum
Screenshot
Роси
NNN.
IS Markov Reward Process ( MRP ) 2-1 • Markov Reward Process is a Markov Chain + rewards • Definition of Markov Reward Process ( MRP ) ⚫S is a ( finite ) set of states ( s € S ) = • P is dynamics / transition model that specifices P ( St + 1 s ' | 5 , = s ) R is a reward function R ( ss ) = E [ rt | st = 5 ] Discount factor [ 0.1 ] • Note : no actions If finite number ( N ) of states , can express R as a vector Screening Docum Screenshot Роси NNN . IS Detected in frame_830.jpg: ד
Example Mars Rover MRP
F
$1
■Rawan
in si +1 in in all iter stater ד Example Mars Rover MRP F $ 1 ■ Rawan in si +1 in in all iter stater Detected in frame_831.jpg: Example. Mars Rover MRP
Sij
$2
53
04
Sa
04
$5
04
56
57
06
02
UZ
UZ
017
02
• Reward. +1 in s +10 in sy 0 in all other states Example . Mars Rover MRP Sij $ 2 53 04 Sa 04 $ 5 04 56 57 06 02 UZ UZ 017 02 • Reward . +1 in s +10 in sy 0 in all other states Detected in frame_832.jpg: Example: Mars Rover MRP
$1
0,4
04
$2
04
$3
S4
$5.
56
ST
0.4
0.4
0.4
0.4
04
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Reward: +1 in s₁, +10 in s7, 0 in all other states
S
D
NNN
IS Example : Mars Rover MRP $ 1 0,4 04 $ 2 04 $ 3 S4 $ 5 . 56 ST 0.4 0.4 0.4 0.4 04 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Reward : +1 in s₁ , +10 in s7 , 0 in all other states S D NNN IS Detected in frame_833.jpg: Example: Mars Rover MRP
2-1
$1
0.4
0.4
$2
0.4
$3
S4
$5
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
• Reward: +1 in s₁, +10 in s7, 0 in all other states
Screen Mirroring Document
Screenshot
Focus
NNNN Example : Mars Rover MRP 2-1 $ 1 0.4 0.4 $ 2 0.4 $ 3 S4 $ 5 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 • Reward : +1 in s₁ , +10 in s7 , 0 in all other states Screen Mirroring Document Screenshot Focus NNNN Detected in frame_834.jpg: Example: Mars Rover MRP
lecture2-2
$1
0.4
0.4
$2
0.4
S3
S4
$5
56
ST
0.4
0.4
0.4
0.4
0.4
0.4
0.4.
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
• Reward: +1 in s₁, +10 in s7, 0 in all other states
Stop Screen Mirroring Document
Screenshot
Focus
NNNN Example : Mars Rover MRP lecture2-2 $ 1 0.4 0.4 $ 2 0.4 S3 S4 $ 5 56 ST 0.4 0.4 0.4 0.4 0.4 0.4 0.4 . 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 • Reward : +1 in s₁ , +10 in s7 , 0 in all other states Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_835.jpg: Example: Mars Rover MRP
$1
0.4
0.4
$2
0.4
##
0.4
$3
.
S4
$5
S6
57
0.4
0.4
0.4
0.4
0.4
04
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
• Reward: +1 in s₁, +10 in s7, 0 in all other states
Screen Mirroring Document
Screenshot
A focus
NNN.
IS
" Example : Mars Rover MRP $ 1 0.4 0.4 $ 2 0.4 ## 0.4 $ 3 . S4 $ 5 S6 57 0.4 0.4 0.4 0.4 0.4 04 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 • Reward : +1 in s₁ , +10 in s7 , 0 in all other states Screen Mirroring Document Screenshot A focus NNN . IS " Detected in frame_836.jpg: Example: Mars Rover MRP
$1
0.4
0.4
$2
0.4
11
0.41
$3
ecture2-2
K
S4
$5
56
57
0.4
0.4
0.4
0.4
0.4
0.4
104
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Reward: +1 in s₁, +10 in s7, 0 in all other states
Screen Mirroring Document
Screenshot
Focus
NNUN..
IN
" Example : Mars Rover MRP $ 1 0.4 0.4 $ 2 0.4 11 0.41 $ 3 ecture2-2 K S4 $ 5 56 57 0.4 0.4 0.4 0.4 0.4 0.4 104 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Reward : +1 in s₁ , +10 in s7 , 0 in all other states Screen Mirroring Document Screenshot Focus NNUN .. IN " Detected in frame_837.jpg: Example: Mars Rover MRP
T
$1
0.4
0.4
$2
0.4
##
0.4
$3
0.4
2-1
lecture2-2
.
S4
$5
S6
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
• Reward: +1 in s₁, +10 in s7, 0 in all other states
Screen Mirroring Document
Screenshot
A focus
●ININN
N Example : Mars Rover MRP T $ 1 0.4 0.4 $ 2 0.4 ## 0.4 $ 3 0.4 2-1 lecture2-2 . S4 $ 5 S6 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 • Reward : +1 in s₁ , +10 in s7 , 0 in all other states Screen Mirroring Document Screenshot A focus ● ININN N Detected in frame_838.jpg: Documents
Example: Mars Rover MRP
Rls,
$1
0.4
0.4
$2
0.4
S3
lecture2-2
S4
$5
56
ST
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Reward: +1 in s₁, +10 in s7, 0 in all other states
Screen Mirroring Document
Screenshot
A Focus
NNN.
IN Documents Example : Mars Rover MRP Rls , $ 1 0.4 0.4 $ 2 0.4 S3 lecture2-2 S4 $ 5 56 ST 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Reward : +1 in s₁ , +10 in s7 , 0 in all other states Screen Mirroring Document Screenshot A Focus NNN . IN Detected in frame_839.jpg: Example: Mars Rover MRP
R(S)=
$1
0.4
04.
$2
$3
SA
$5
56
ST
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Reward: +1 in s₁, +10 in s7, 0 in all other states
San M
"
IN
NISS Example : Mars Rover MRP R ( S ) = $ 1 0.4 04 . $ 2 $ 3 SA $ 5 56 ST 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Reward : +1 in s₁ , +10 in s7 , 0 in all other states San M " IN NISS Detected in frame_840.jpg: Example: Mars Rover MRP
R(s,)= |
$1
0.4
0.4
$2
0.4
$3
S4
$5
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Reward: +1 in s₁, +10 in s7, 0 in all other states
Screen Mirroring Document
Screenshot
Focus
N N N N
' Example : Mars Rover MRP R ( s , ) = | $ 1 0.4 0.4 $ 2 0.4 $ 3 S4 $ 5 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Reward : +1 in s₁ , +10 in s7 , 0 in all other states Screen Mirroring Document Screenshot Focus N N N N ' Detected in frame_841.jpg: lecture
Example: Mars Rover MRP
R(s,)= |
$1
0.4
0.4
$2
0.4
cture-1
S3
SA
$5
56
0.4
0.4
0.4
0.4
0.4
0.4
0.4
2-2
0.4
R
0.4
ST
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Reward: +1 in s₁. +10 in s7, 0 in all other states
Stop Screen Mirroring Document
Screenshot
Focus
NNNN lecture Example : Mars Rover MRP R ( s , ) = | $ 1 0.4 0.4 $ 2 0.4 cture - 1 S3 SA $ 5 56 0.4 0.4 0.4 0.4 0.4 0.4 0.4 2-2 0.4 R 0.4 ST 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Reward : +1 in s₁ . +10 in s7 , 0 in all other states Stop Screen Mirroring Document Screenshot Focus NNNN Detected in frame_842.jpg: x
Example: Mars Rover MRP
R(s,)= |
$1
0.4
0.4
$2
53
REST
SA
$5
56
$7
0.4
0.4
0.4
0.4
0.4
04
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
06
• Reward: +1 in s1, +10 in s7, 0 in all other states
Screen M
Document
Рос
NNN.
N
' x Example : Mars Rover MRP R ( s , ) = | $ 1 0.4 0.4 $ 2 53 REST SA $ 5 56 $ 7 0.4 0.4 0.4 0.4 0.4 04 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 06 • Reward : +1 in s1 , +10 in s7 , 0 in all other states Screen M Document Рос NNN . N ' Detected in frame_843.jpg: Example: Mars Rover MRP
R(s,)= |
$1
0.4
0.4
$2
0.4
##
04
$3
0.4
lecture2-2
R(577)=
SA
$5
S6
57
0.4
0.4
0.4
0.4
04
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
•Reward: +1 in s₁, +10 in s7, 0 in all other states
Screen Mirroring Document
Screenshot
A Focus
NNNN Example : Mars Rover MRP R ( s , ) = | $ 1 0.4 0.4 $ 2 0.4 ## 04 $ 3 0.4 lecture2-2 R ( 577 ) = SA $ 5 S6 57 0.4 0.4 0.4 0.4 04 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 • Reward : +1 in s₁ , +10 in s7 , 0 in all other states Screen Mirroring Document Screenshot A Focus NNNN Detected in frame_844.jpg: x
Example: Mars Rover MRP
R(s,)= |
$1
0.4
0.4
$2
53
R(57)=10
SA
$5
56
$7
0.4
0.4
0.4
0.4
0.4
04
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
06
• Reward: +1 in s1, +10 in s7, 0 in all other states
Screen M
Document
Рос
NEN O
' x Example : Mars Rover MRP R ( s , ) = | $ 1 0.4 0.4 $ 2 53 R ( 57 ) = 10 SA $ 5 56 $ 7 0.4 0.4 0.4 0.4 0.4 04 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 06 • Reward : +1 in s1 , +10 in s7 , 0 in all other states Screen M Document Рос NEN O ' Detected in frame_845.jpg: Example: Mars Rover MRP
R(s,)= |
$1
0.4
0.4
$2
0.4
0.4
$3
x
K
R(57)=10
S4
$5
56
57
0.4
0.4
0.4
0.4
0.41
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
•Reward: +1 in s₁, +10 in s7, 0 in all other states
Screen Mirroring Document
Screenshot
A focus
N IN IN
" Example : Mars Rover MRP R ( s , ) = | $ 1 0.4 0.4 $ 2 0.4 0.4 $ 3 x K R ( 57 ) = 10 S4 $ 5 56 57 0.4 0.4 0.4 0.4 0.41 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 • Reward : +1 in s₁ , +10 in s7 , 0 in all other states Screen Mirroring Document Screenshot A focus N IN IN " Detected in frame_846.jpg: Росте
Example: Mars Rover MRP
R(s,)= |
$1
0.4
0.4
$2
0.4
11
0.4
S3
lecture2-1
R(517)=10
SA
$5
56
ST
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
• Reward: +1 in s₁, +10 in $7, 0 in all other states
Screen Mirroring Document
Screenshot
Focus
NNNN
" Росте Example : Mars Rover MRP R ( s , ) = | $ 1 0.4 0.4 $ 2 0.4 11 0.4 S3 lecture2-1 R ( 517 ) = 10 SA $ 5 56 ST 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 • Reward : +1 in s₁ , +10 in $ 7 , 0 in all other states Screen Mirroring Document Screenshot Focus NNNN " Detected in frame_847.jpg: Example: Mars Rover MRP
Rls,)= |
$1
0.4
04
$2
04
53
-
R(57)=10
S4
$5
$6
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
• Reward: +1 in St. +10 in s7, 0 in all other states
Screen M
Focus
NE NE Example : Mars Rover MRP Rls , ) = | $ 1 0.4 04 $ 2 04 53 - R ( 57 ) = 10 S4 $ 5 $ 6 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 • Reward : +1 in St. +10 in s7 , 0 in all other states Screen M Focus NE NE Detected in frame_848.jpg: Example: Mars Rover MRP
R(s,)= |
$1
0.4
0.4
$2
0.4
##
0.4
$3
-
R(57)=10
S4
$5
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
• Reward: +1 in s₁, +10 in s7, 0 in all other states
Screen Mirroring Document
Screenshot
A focus
'
IN IN IN IN Example : Mars Rover MRP R ( s , ) = | $ 1 0.4 0.4 $ 2 0.4 ## 0.4 $ 3 - R ( 57 ) = 10 S4 $ 5 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 • Reward : +1 in s₁ , +10 in s7 , 0 in all other states Screen Mirroring Document Screenshot A focus ' IN IN IN IN Detected in frame_849.jpg: Example: Mars Rover MRP
R(si)=1
$1
0.4
04
$2
0.4
$3
CTC-
R(57)=10
S4
$5
56
ST
0.4
0.4
0.4
0.4
04
0.4
0.4
10.4
0.4
06
0.2
0.2
0.2
0.2
02
0.6
Reward: +1 in s₁. +10 in 57, 0 in all other states
Son M
D
IS
NINA Example : Mars Rover MRP R ( si ) = 1 $ 1 0.4 04 $ 2 0.4 $ 3 CTC- R ( 57 ) = 10 S4 $ 5 56 ST 0.4 0.4 0.4 0.4 04 0.4 0.4 10.4 0.4 06 0.2 0.2 0.2 0.2 02 0.6 Reward : +1 in s₁ . +10 in 57 , 0 in all other states Son M D IS NINA Detected in frame_850.jpg: 2-1
x
2-2
Example: Mars Rover MRP
R(s,)= |
$1
0.4
0.4
$2
S3
R(57)=10
S4
$5
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
• Reward: +1 in s₁, +10 in s7, 0 in all other states
Screen M
Document
Screenshot
A focus
" 2-1 x 2-2 Example : Mars Rover MRP R ( s , ) = | $ 1 0.4 0.4 $ 2 S3 R ( 57 ) = 10 S4 $ 5 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 • Reward : +1 in s₁ , +10 in s7 , 0 in all other states Screen M Document Screenshot A focus " Detected in frame_851.jpg: 2-1
×
2-2
Example: Mars Rover MRP
R(s,)= |
$1
0.4
0.4
$2
0.4
R(577)=10
$3
S4
$5
56
57
0.4
0.4
0.4
0.4
0.4
0.4
0.4
04
0.4
0.6
0.2
0.2
0.2
0.2
0.2
0.6
Reward: +1 in s₁, +10 in s7, 0 in all other states
Screen Mirroring Document
Screenshot
Focus
'
N IN IN 2-1 × 2-2 Example : Mars Rover MRP R ( s , ) = | $ 1 0.4 0.4 $ 2 0.4 R ( 577 ) = 10 $ 3 S4 $ 5 56 57 0.4 0.4 0.4 0.4 0.4 0.4 0.4 04 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.6 Reward : +1 in s₁ , +10 in s7 , 0 in all other states Screen Mirroring Document Screenshot Focus ' N IN IN Detected in frame_852.jpg: Jecture-1
R(577)=10
57
Example: Mars Rover MRP
R(s,)= |
$1
0.4
0.4
$2
S3
S4
$5
56
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0:4
0.4
0.6
0.2
0.2
0.2
0.2
0.2
06
Reward: +1 in s₁, +10 in s7, 0 in all other states
Screen Mirroring Document
Screenshot
Focus
NNN Jecture - 1 R ( 577 ) = 10 57 Example : Mars Rover MRP R ( s , ) = | $ 1 0.4 0.4 $ 2 S3 S4 $ 5 56 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0 : 4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 06 Reward : +1 in s₁ , +10 in s7 , 0 in all other states Screen Mirroring Document Screenshot Focus NNN Detected in frame_877.jpg: Return & Value Function
Definition of Horizon
Number of une steps in each episode
■Can infinit
Uthers called finite Markor reward process
Definition of Resun, G (or a MRP)
Discounted sum of rewards from Ume stea† to orizon
G=1171
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in siste s Return & Value Function Definition of Horizon Number of une steps in each episode ■ Can infinit Uthers called finite Markor reward process Definition of Resun , G ( or a MRP ) Discounted sum of rewards from Ume stea † to orizon G = 1171 Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in siste s Detected in frame_878.jpg: Return & Value Function
ה
Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. G (for a MRP)
Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
K
Q.
|55| Return & Value Function ה Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . G ( for a MRP ) Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s K Q. | 55 | Detected in frame_879.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
1++3
N
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
V(s) =E[G₁s=s]=E[+++²+2+3+3 +5=5] Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon 1 ++ 3 N Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s V ( s ) = E [ G₁s = s ] = E [ +++ ² + 2 + 3 + 3 + 5 = 5 ] Detected in frame_880.jpg: Return & Value Function
T
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
+3
NNN •
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[Gs, s]=E[++++++7³+3+ ·· | St=5] Return & Value Function T • Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon +3 NNN • Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ Gs , s ] = E [ ++++++ 7³ + 3 + ·· | St = 5 ] Detected in frame_881.jpg: Return & Value Function
T
NNN •
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
G₁ =+++++2+7³ ++3+...
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[Gs, s]=E[++++++7³+3+ ·· | St=5] Return & Value Function T NNN • • Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon G₁ = +++++ 2 + 7³ ++ 3 + ... Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ Gs , s ] = E [ ++++++ 7³ + 3 + ·· | St = 5 ] Detected in frame_882.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
G₁ = ++++++³+3+...
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G5=s]=E[+1+1+1+2+3+3+5=5]
10 Рос
IN IN INN Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon G₁ = ++++++ ³ + 3 + ... Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G5 = s ] = E [ + 1 + 1 + 1 + 2 + 3 + 3 + 5 = 5 ] 10 Рос IN IN INN Detected in frame_883.jpg: Return & Value Function
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
3 Return & Value Function • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s 3 Detected in frame_884.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G|s=s]=E[r₁ + +1 +²+2+7³r+3 + · · · |S₁ = 5]
S
NNINN
+ Return & Value Function • Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G | s = s ] = E [ r₁ + +1 + ² + 2 + 7³r + 3 + · · · | S₁ = 5 ] S NNINN + Detected in frame_885.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
. Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function. V(s) (for a MRP)
• Expected return from starting in state s
V(5) EGs s] | +++++
=
NNN
IN Return & Value Function • Definition of Horizon Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function . V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) EGs s ] | +++++ = NNN IN Detected in frame_886.jpg: Return & Value Function
ה
⚫ Definition of Horizon
Number of time steps in each episode
. Can be infinite
Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) EIG❘s s] [6 + 7/7+1+9²+z+9³fr+3 + · · · |5; = $]. Return & Value Function ה ⚫ Definition of Horizon Number of time steps in each episode . Can be infinite Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) EIG❘s s ] [ 6 + 7 / 7 + 1 + 9² + z + 9³fr + 3 + · · · | 5 ; = $ ] . Detected in frame_887.jpg: Return & Value Function
Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G, (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
•Expected return from starting in state s
V(5)=E|G|s=s] = [+1+1+1+2+3+3 +
15
NINN Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G , ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) = E | G | s = s ] = [ + 1 + 1 + 1 + 2 + 3 + 3 + 15 NINN Detected in frame_888.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[Gs=s]=E[++++++³/+3 +-
N Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ Gs = s ] = E [ ++++++ ³ / + 3 + - N Detected in frame_889.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5)=E[G₁s=s]=E[+1+1+1+2+3+3+....
N Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) = E [ G₁s = s ] = E [ + 1 + 1 + 1 + 2 + 3 + 3 + .... N Detected in frame_890.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G₁s=s]=E[+V+1+7²+2+7³r+3 + · - · |S₂ = 5]
San Mig D
58 Рос
IN Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G₁s = s ] = E [ + V + 1 + 7² + 2 + 7³r + 3 + · - · | S₂ = 5 ] San Mig D 58 Рос IN Detected in frame_891.jpg: Return & Value Function
ww
NAJN
Definition of Horizon
Number of time steps in each episode
• Can be infinite
. Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
G₁ =+++++³+3+**
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G, st =s]=E[re + re+1 +²re+2+3+3 + ·· | S₁ = 5]
Scre
Foc Return & Value Function ww NAJN Definition of Horizon Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon G₁ = +++++ ³ + 3 + ** Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , st = s ] = E [ re + re + 1 + ²re + 2 + 3 + 3 + ·· | S₁ = 5 ] Scre Foc Detected in frame_892.jpg: Return & Value Function
x
K
Definition of Horizon
• Number of time steps in each episode
• Can be infinite
. Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5)=E[Gist=s]=E[+++²+2+3+3 +
NNNN Return & Value Function x K Definition of Horizon • Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) = E [ Gist = s ] = E [ +++ ² + 2 + 3 + 3 + NNNN Detected in frame_893.jpg: Return & Value Function
⚫ Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
3 Return & Value Function ⚫ Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s 3 Detected in frame_894.jpg: Return & Value Function
1
NAN
N
Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
G=+7+1+2+2 +7³ +3 +---
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G, 5=s]=E[r₁ + +1 +7²+2+3+3 + · · · |S₁ = 5]
S
D Return & Value Function 1 NAN N Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon G = + 7 + 1 + 2 + 2 + 7³ +3 + --- Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , 5 = s ] = E [ r₁ + +1 + 7² + 2 + 3 + 3 + · · · | S₁ = 5 ] S D Detected in frame_895.jpg: Return & Value Function
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) EGs s] [+++++++
5,=
sj Return & Value Function • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) EGs s ] [ +++++++ 5 , = sj Detected in frame_896.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
■ Can be infinite
•Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function. V(s) (for a MRP)
• Expected return from starting in state s
V(s) EGs, s] | +++++³1143+ 5 = $ Return & Value Function • Definition of Horizon Number of time steps in each episode ■ Can be infinite • Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function . V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) EGs , s ] | +++++ ³1143 + 5 = $ Detected in frame_897.jpg: Return & Value Function
⚫ Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5)=E|G|s=s]=E[+1+1+1+2+3+3 +
15 Return & Value Function ⚫ Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) = E | G | s = s ] = E [ + 1 + 1 + 1 + 2 + 3 + 3 + 15 Detected in frame_898.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
1++3
NNN
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G₁s=s]=E[+1+1+1+2+3+3 + ·· |5;=5] Return & Value Function • Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon 1 ++ 3 NNN Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G₁s = s ] = E [ + 1 + 1 + 1 + 2 + 3 + 3 + ·· | 5 ; = 5 ] Detected in frame_899.jpg: Return & Value Function
Definition of Horizon
• Number of time steps in each episode
• Can be infinite
• Otherwise called finite Markov reward process
Definition of Return, G, (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G,|s=s] =E[n + +1 +²re+2+7³ +3 + · - · |St = 5] Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite • Otherwise called finite Markov reward process Definition of Return , G , ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , | s = s ] = E [ n + +1 + ²re + 2 + 7³ +3 + · - · | St = 5 ] Detected in frame_900.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
2
= Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s 2 = Detected in frame_901.jpg: Return & Value Function
ה
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return. G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
= Return & Value Function ה • Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return . G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s = Detected in frame_902.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G, (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G, 5=s]=E[+++++³+3+ |S₁ =5]
S
NNN Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G , ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , 5 = s ] = E [ +++++ ³ + 3 + | S₁ = 5 ] S NNN Detected in frame_903.jpg: Return & Value Function
2
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
Discounted sum of rewards from time step t to horizon
3
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|sts]=n+1+1+1+2+3+3 + |S=5] Return & Value Function 2 • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) Discounted sum of rewards from time step t to horizon 3 Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | sts ] = n + 1 + 1 + 1 + 2 + 3 + 3 + | S = 5 ] Detected in frame_904.jpg: Return & Value Function
Definition of Horizon
• Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
-
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G, st = s]=E[ + 7+1+2+2+7³r+3 + · · · |S₁ = 5]
18 Русия Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon - Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , st = s ] = E [ + 7 + 1 + 2 + 2 + 7³r + 3 + · · · | S₁ = 5 ] 18 Русия Detected in frame_905.jpg: Return & Value Function
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
1++3
Definition of State Value Function, V(s) (for a MRP)
•Expected return from starting in state s
V(5) EGs s] | +++++³ 3+
NNN..
N Return & Value Function • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon 1 ++ 3 Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) EGs s ] | +++++ ³ 3+ NNN .. N Detected in frame_906.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
■ Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
11-3
= Return & Value Function • Definition of Horizon Number of time steps in each episode ■ Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s 11-3 = Detected in frame_907.jpg: Return & Value Function
FF
Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G, (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
V(s) =E|G|s=s] = [+1+1+1+2+3+3 +
15 Return & Value Function FF Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G , ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s V ( s ) = E | G | s = s ] = [ + 1 + 1 + 1 + 2 + 3 + 3 + 15 Detected in frame_908.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|s=s]=E[+1+1+2+2+3+3+..
= Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | s = s ] = E [ + 1 + 1 + 2 + 2 + 3 + 3 + .. = Detected in frame_909.jpg: Return & Value Function
C
Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G, (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G5=s]=E[+1+1+2+2+³+3+ |St=5] Return & Value Function C Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G , ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G5 = s ] = E [ + 1 + 1 + 2 + 2 + ³ + 3 + | St = 5 ] Detected in frame_910.jpg: Return & Value Function
2-1
Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
V(5) EGs, s] [+++++++
=
.
ININ N Return & Value Function 2-1 Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s V ( 5 ) EGs , s ] [ +++++++ = . ININ N Detected in frame_911.jpg: Return & Value Function
• Definition of Horizon
• Number of time steps in each episode
■ Can be infinite
⚫ Otherwise called finite Markov reward process.
Definition of Return. Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
G₁ = + + + + e +...
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
A Return & Value Function • Definition of Horizon • Number of time steps in each episode ■ Can be infinite ⚫ Otherwise called finite Markov reward process . Definition of Return . Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon G₁ = + + + + e + ... Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s A Detected in frame_912.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G, (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
+3
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s)=E[G₁ss]=E[+V+1 + 7²+2+3+3 + · · · |S₁ = 5]
N
' Return & Value Function • Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G , ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon +3 Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G₁ss ] = E [ + V + 1 + 7² + 2 + 3 + 3 + · · · | S₁ = 5 ] N ' Detected in frame_913.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G, (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s)=E[G|s=s]=E[+V+1+9²+2+3+3 + ·-· |St=5]
18 Рос
IN
NNN Return & Value Function • Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G , ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G | s = s ] = E [ + V + 1 + 9² + 2 + 3 + 3 + · - · | St = 5 ] 18 Рос IN NNN Detected in frame_914.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G, (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
NININ N Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G , ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s NININ N Detected in frame_915.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
■ Can be infinite
•Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
11+3
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s Return & Value Function • Definition of Horizon Number of time steps in each episode ■ Can be infinite • Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon 11 + 3 Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s Detected in frame_916.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G, (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
+3
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s)=E[G|sts]=E[+V+1 + √²+2+7³ +3 + · · · |S₁ = 5]
' Return & Value Function • Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G , ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon +3 Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G | sts ] = E [ + V + 1 + √² + 2 + 7³ +3 + · · · | S₁ = 5 ] ' Detected in frame_917.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
A
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s)=E[G|5=s]=E[+1+1+7
+²+2+3+3+ | St=5]
S
Fea
NINN
N Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon A Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G | 5 = s ] = E [ + 1 + 1 + 7 + ² + 2 + 3 + 3 + | St = 5 ] S Fea NINN N Detected in frame_918.jpg: Return & Value Function
x
Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
G₁ =++++1+2+7³/+3+...
Definition of State Value Function, V(s) (for a MRP)
•Expected return from starting in state s
V(5) EG₁s, 5]=+++++++ |5, = :
IN
NNNNN Return & Value Function x Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon G₁ = ++++ 1 + 2 + 7³ / + 3 + ... Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) EG₁s , 5 ] = +++++++ | 5 , = : IN NNNNN Detected in frame_919.jpg: Return & Value Function
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process.
Definition of Return. Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
NAAL.
.
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|5=s] = [+1+1+2+2+3+3+ Return & Value Function • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process . Definition of Return . Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon NAAL . . Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | 5 = s ] = [ + 1 + 1 + 2 + 2 + 3 + 3 + Detected in frame_920.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
• Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
G₁ =+7+1+2+2 +93 +3 +
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G, 5=s]=E[r₁ + 7+1+2+2+3+3 + ··-· |S₁ = 5]
S
D Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite • Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon G₁ = + 7 + 1 + 2 + 2 +93 +3 + Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , 5 = s ] = E [ r₁ + 7 + 1 + 2 + 2 + 3 + 3 + ·· - · | S₁ = 5 ] S D Detected in frame_921.jpg: Return & Value Function
2-1
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
V(s) EGs s] E16+ 7+1+9²+2+9³3+
55)
فضا 3
NNN..
IN Return & Value Function 2-1 • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s V ( s ) EGs s ] E16 + 7 + 1 + 9² + 2 + 9³3 + 55 ) فضا 3 NNN .. IN Detected in frame_922.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
■ Can be infinite
Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function. V(s) (for a MRP)
• Expected return from starting in state s Return & Value Function • Definition of Horizon Number of time steps in each episode ■ Can be infinite Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function . V ( s ) ( for a MRP ) • Expected return from starting in state s Detected in frame_923.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5)=E[GS=s]=[+1+1+1+2+3+3 + |S;=5] Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) = E [ GS = s ] = [ + 1 + 1 + 1 + 2 + 3 + 3 + | S ; = 5 ] Detected in frame_924.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5)=E[GS=s]=E[+1+1+1+2+3+3 + |S;=5]
150 Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) = E [ GS = s ] = E [ + 1 + 1 + 1 + 2 + 3 + 3 + | S ; = 5 ] 150 Detected in frame_925.jpg: Return & Value Function
Definition of Horizon
• Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
V(s) =E[G,❘s, = s]=E[+V+1+7²+2+3+3 + · · · |S₁ = S]
Semen M
From
NNNNN Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s V ( s ) = E [ G , ❘s , = s ] = E [ + V + 1 + 7² + 2 + 3 + 3 + · · · | S₁ = S ] Semen M From NNNNN Detected in frame_926.jpg: Return & Value Function
Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G, st = s]=E[re + re+1+7²re+2+3³ +3 + ·· | S₁ = 5]
Screen M
NININ N Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , st = s ] = E [ re + re + 1 + 7²re + 2 + 3³ +3 + ·· | S₁ = 5 ] Screen M NININ N Detected in frame_927.jpg: D
Return & Value Function
-
NNNN
Definition of Horizon
• Number of time steps in each episode
• Can be infinite
• Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
G₁ =+7+1+2+2+3+3 +...
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
V(s)=E[GS=s]=E[++++2+3+3 +5=5]
Son Mo D Return & Value Function - NNNN Definition of Horizon • Number of time steps in each episode • Can be infinite • Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon G₁ = + 7 + 1 + 2 + 2 + 3 + 3 + ... Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s V ( s ) = E [ GS = s ] = E [ ++++ 2 + 3 + 3 + 5 = 5 ] Son Mo Detected in frame_928.jpg: Return & Value Function
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process.
Definition of Return, Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
3
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) E|G|s=s]=+++++³+3+5=5]
+2 Return & Value Function • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process . Definition of Return , Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon 3 Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) E | G | s = s ] = +++++ ³ + 3 + 5 = 5 ] +2 Detected in frame_929.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
NNNN
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G₁|st = s]=E[re + re+1 +²re+2+3+3 + ·· | S₁ = 5]
Screening
Screenshot
Focus Return & Value Function • Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon NNNN Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G₁ | st = s ] = E [ re + re + 1 + ²re + 2 + 3 + 3 + ·· | S₁ = 5 ] Screening Screenshot Focus Detected in frame_930.jpg: Return & Value Function
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process.
Definition of Return, Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
3
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) E|G|s, =s]=E+++++³+3+5=5] Return & Value Function • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process . Definition of Return , Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon 3 Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) E | G | s , = s ] = E +++++ ³ + 3 + 5 = 5 ] Detected in frame_931.jpg: Return & Value Function
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
. Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
K
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G|s=s] = [++++++³ +3 + ··|
Som
From
=
NOIS INN
$ Return & Value Function • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon K Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G | s = s ] = [ ++++++ ³ +3 + ·· | Som From = NOIS INN $ Detected in frame_932.jpg: Return & Value Function
⚫ Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. G (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|st=s]=E[+1+1+2+2+3+3+
NS Return & Value Function ⚫ Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . G ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | st = s ] = E [ + 1 + 1 + 2 + 2 + 3 + 3 + NS Detected in frame_933.jpg: Return & Value Function
Definition of Horizon
• Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G, st =s]=E[+V+1+2+2+3+3 + · · · |S₁ = 5]
SM Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , st = s ] = E [ + V + 1 + 2 + 2 + 3 + 3 + · · · | S₁ = 5 ] SM Detected in frame_934.jpg: Return & Value Function
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
. Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
1++3
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
فض لفظ
NNUN.. Return & Value Function • Definition of Horizon • Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon 1 ++ 3 Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s فض لفظ NNUN .. Detected in frame_935.jpg: Return & Value Function
-
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s Return & Value Function - • Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s Detected in frame_936.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function. V(s) (for a MRP)
• Expected return from starting in state s
V(5) E[Gs, s]=E[+++++++³+3+ |S₁ = 5] Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function . V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E [ Gs , s ] = E [ +++++++ ³ + 3 + | S₁ = 5 ] Detected in frame_937.jpg: Return & Value Function
Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[Gs=s]=E[++++++³++3+ | St=5]
130 Return & Value Function Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ Gs = s ] = E [ ++++++ ³ ++ 3 + | St = 5 ] 130 Detected in frame_938.jpg: Return & Value Function
2-1
Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
NNNN
S Return & Value Function 2-1 Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s NNNN S Detected in frame_939.jpg: Return & Value Function
-
• Definition of Horizon
Number of time steps in each episode
■ Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
A Return & Value Function - • Definition of Horizon Number of time steps in each episode ■ Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s A Detected in frame_940.jpg: Return & Value Function
711-
A
Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s)=E[G₁sts]=E[ +++ √²+2 + √³½+3 + ·· Return & Value Function 711- A Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G₁sts ] = E [ +++ √² + 2 + √³½ + 3 + ·· Detected in frame_941.jpg: Return & Value Function
Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
-
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s)=E[G|s=s]=E[+Y+1 +²+2+7³ +3 + - · | St=5]
NINN Return & Value Function Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon - Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G | s = s ] = E [ + Y + 1 + ² + 2 + 7³ +3 + - · | St = 5 ] NINN Detected in frame_942.jpg: Return & Value Function
NNN
Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G, (for a MRP)
• Discounted sum of rewards from time step t to horizon
G=
G₁ = 1+ + ++ +² 10+2 +7³/+3+...
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G, 5=s] = Er + +1 +²+2+7³ +3 + - - · |S; = 5]
A Return & Value Function NNN Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G , ( for a MRP ) • Discounted sum of rewards from time step t to horizon G = G₁ = 1+ + ++ + ² 10 + 2 + 7³ / + 3 + ... Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , 5 = s ] = Er + +1 + ² + 2 + 7³ +3 + - - · | S ; = 5 ] A Detected in frame_943.jpg: Return & Value Function
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
NNNN
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G|st=s]=E[++++++√³+3+ ·· | St=5]
Screen Mining Return & Value Function • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon NNNN Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G | st = s ] = E [ ++++++ √³ + 3 + ·· | St = 5 ] Screen Mining Detected in frame_944.jpg: Return & Value Function
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process.
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
3
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
V(5)=E|G|s=s]=n+1+1+1+2+3+3 +5=5] Return & Value Function • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process . Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon 3 Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s V ( 5 ) = E | G | s = s ] = n + 1 + 1 + 1 + 2 + 3 + 3 + 5 = 5 ] Detected in frame_945.jpg: Return & Value Function
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
NNNN
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G₁st=s]=E[re + re+1+7²re+2+³ +3 + - S₁ =5]
Screen Ming D
From Return & Value Function • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon NNNN Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G₁st = s ] = E [ re + re + 1 + 7²re + 2 + ³ +3 + - S₁ = 5 ] Screen Ming D From Detected in frame_946.jpg: Return & Value Function
A AM ---
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process.
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
3
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
V(5) E|G|5=s]=E++++2+3+3 +5=5] Return & Value Function A AM --- • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process . Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon 3 Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s V ( 5 ) E | G | 5 = s ] = E ++++ 2 + 3 + 3 + 5 = 5 ] Detected in frame_947.jpg: Return & Value Function
NNNN
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
(G₁ = 1₁ + re+1 + y² re+2 +7³/+3+...
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G|st=s]=E[+1+1+2+2+3+3 + ·· | St=5]
Screening Return & Value Function NNNN • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ( G₁ = 1₁ + re + 1 + y² re + 2 + 7³ / + 3 + ... Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G | st = s ] = E [ + 1 + 1 + 2 + 2 + 3 + 3 + ·· | St = 5 ] Screening Detected in frame_948.jpg: Return & Value Function
⚫ Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process.
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
يل
3
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|5=s]=E[+1+1+2+2+3+3 +15=5] Return & Value Function ⚫ Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process . Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon يل 3 Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | 5 = s ] = E [ + 1 + 1 + 2 + 2 + 3 + 3 + 15 = 5 ] Detected in frame_949.jpg: Return & Value Function
NNNN
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G❘st=s] =E[r₁ + 7+1+2+2+3+3 + - - · | S₁ = 5]
Screening D Return & Value Function NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G❘st = s ] = E [ r₁ + 7 + 1 + 2 + 2 + 3 + 3 + - - · | S₁ = 5 ] Screening D Detected in frame_950.jpg: Return & Value Function
3
⚫ Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
(G₁ = 1 + + + 7²+2+3+3 +
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G₁|st=s]=E+++++³+3+5=5] Return & Value Function 3 ⚫ Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon ( G₁ = 1 + + + 7² + 2 + 3 + 3 + Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G₁ | st = s ] = E +++++ ³ + 3 + 5 = 5 ] Detected in frame_951.jpg: Return & Value Function
NNNN
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
{
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G,|st=s] =E[r₁ + 7+1+7²+2+3+3 + · - · | S₁ = 5]
Screening D
Foc Return & Value Function NNNN • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ { Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , | st = s ] = E [ r₁ + 7 + 1 + 7² + 2 + 3 + 3 + · - · | S₁ = 5 ] Screening D Foc Detected in frame_952.jpg: Return & Value Function
Th
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process.
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
يال
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) E|G|5=s]=E[+1+1+1+2+3+3 +5=5]
5 Return & Value Function Th • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process . Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon يال ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) E | G | 5 = s ] = E [ + 1 + 1 + 1 + 2 + 3 + 3 + 5 = 5 ] 5 Detected in frame_953.jpg: Return & Value Function
cture-1
NNNN
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
یا
།,
(G₁ = r₁ + re+1 + y²+2+3+3 +...
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G|s=s]=E[+++++++7³+3+ |S=5]
Screen M
D Return & Value Function cture - 1 NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon یا ། , ( G₁ = r₁ + re + 1 + y² + 2 + 3 + 3 + ... Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G | s = s ] = E [ +++++++ 7³ + 3 + | S = 5 ] Screen M D Detected in frame_954.jpg: Return & Value Function
Th
AAM-
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process.
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
يل
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) E|G|5=s]=E[+1+1+1+2+3+3 +5=5]
5 Return & Value Function Th AAM- • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process . Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon يل ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) E | G | 5 = s ] = E [ + 1 + 1 + 1 + 2 + 3 + 3 + 5 = 5 ] 5 Detected in frame_955.jpg: Return & Value Function
NNNN
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
یا
།,
(G₁ = re+ye+1 +²+2+3+3+...
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G, st = s]=E[+++²+2+³½ +3 + · · · |S; = 5]
Screen M
D Return & Value Function NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon یا ། , ( G₁ = re + ye + 1 + ² + 2 + 3 + 3 + ... Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , st = s ] = E [ +++ ² + 2 + ³½ +3 + · · · | S ; = 5 ] Screen M D Detected in frame_956.jpg: Return & Value Function
Th
AAM-
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) E|G|5=s]=E[+1+1+1+2+3+3 +5=5]
5 Return & Value Function Th AAM- • Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) E | G | 5 = s ] = E [ + 1 + 1 + 1 + 2 + 3 + 3 + 5 = 5 ] 5 Detected in frame_957.jpg: Return & Value Function
NNNN
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
{,
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G❘st=s] =E[ +Yre+1 +²+2+7³½+3 + · ·
Screen Mirroring D Return & Value Function NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ { , Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G❘st = s ] = E [ + Yre + 1 + ² + 2 + 7³½ + 3 + · · Screen Mirroring D Detected in frame_958.jpg: Return & Value Function
A
3
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
Discounted sum of rewards from time step t to horizon
يد
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|5=s] = [+1+1+1+2+3+3 +5=5] Return & Value Function A 3 • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) Discounted sum of rewards from time step t to horizon يد ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | 5 = s ] = [ + 1 + 1 + 1 + 2 + 3 + 3 + 5 = 5 ] Detected in frame_959.jpg: Return & Value Function
NNNN
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G❘st=s] =E[r + re+1+7²+2+3+3 + · · · |S; = 5]
Screening
Schot Return & Value Function NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G❘st = s ] = E [ r + re + 1 + 7² + 2 + 3 + 3 + · · · | S ; = 5 ] Screening Schot Detected in frame_960.jpg: Return & Value. Function
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
Discounted sum of rewards from time step t to horizon
یا
↓
A
3
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|5=5]=E+++++³+3+5=5] Return & Value . Function • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) Discounted sum of rewards from time step t to horizon یا ↓ A 3 Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | 5 = 5 ] = E +++++ ³ + 3 + 5 = 5 ] Detected in frame_961.jpg: Return & Value Function
NNNN
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
{,
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G❘st=s] =E[r + re+1+7²+2+3+3 + · · · |S; = 5]
Screening
Schot Return & Value Function NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ { , Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G❘st = s ] = E [ r + re + 1 + 7² + 2 + 3 + 3 + · · · | S ; = 5 ] Screening Schot Detected in frame_962.jpg: Return & Value Function
[え]]]]
Definition of Horizon
• Number of time steps in each episode
• Can be infinite
■ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
يد
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|s=s]=E+++++³½+3 + ·-·|
15 Return & Value Function [ え ] ] ] ] Definition of Horizon • Number of time steps in each episode • Can be infinite ■ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ يد ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | s = s ] = E +++++ ³½ + 3 + · - · | 15 Detected in frame_963.jpg: Return & Value Function
NNNN
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
. Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G,|st=s]=E[r + We+1+7²+2+3+3 + · - · |S₁ = 5]
Sorena Mwing Сос
58. Россия Return & Value Function NNNN • Definition of Horizon Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , | st = s ] = E [ r + We + 1 + 7² + 2 + 3 + 3 + · - · | S₁ = 5 ] Sorena Mwing Сос 58. Россия Detected in frame_964.jpg: Return & Value Function
Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
ايل
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|s=s]=E[+1+1+2+2+3+3+ Return & Value Function Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon ايل ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | s = s ] = E [ + 1 + 1 + 2 + 2 + 3 + 3 + Detected in frame_965.jpg: Return & Value Function
NNNN
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
. Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[Gs=s]=E[+++++++³+3+ |St=5]
Soren
Сос Return & Value Function NNNN • Definition of Horizon Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ Gs = s ] = E [ +++++++ ³ + 3 + | St = 5 ] Soren Сос Detected in frame_966.jpg: Return & Value Function
[え]]]]
Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
ايل
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|sts] = En+++++³/+3+ Return & Value Function [ え ] ] ] ] Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon ايل ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | sts ] = En +++++ ³ / + 3 + Detected in frame_967.jpg: Return & Value Function
-
NNNN
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
• Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
།,
(G₁ = + + re+1 +² 1+2+7³/+3+...
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G, st = s]=E[+1+1+7²+2+3 +3 + · - · |S₁ = 5]
Screen Mon Овошения Return & Value Function - NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite • Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ ། , ( G₁ = + + re + 1 + ² 1 + 2 + 7³ / + 3 + ... Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , st = s ] = E [ + 1 + 1 + 7² + 2 + 3 +3 + · - · | S₁ = 5 ] Screen Mon Овошения Detected in frame_968.jpg: Return & Value Function
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
⚫ Discounted sum of rewards from time step t to horizon
يد
↓
AAM-
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|s=s]=E[+1+1+1+2+3+3 +5=5] Return & Value Function • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) ⚫ Discounted sum of rewards from time step t to horizon يد ↓ AAM- Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | s = s ] = E [ + 1 + 1 + 1 + 2 + 3 + 3 + 5 = 5 ] Detected in frame_969.jpg: Return & Value Function
cture-1
NNNN
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
. Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
།
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G|s=s]=E[+1+1+2+2+3+3 + |S;=5]
Screen Mirroring С Return & Value Function cture - 1 NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ ། Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G | s = s ] = E [ + 1 + 1 + 2 + 2 + 3 + 3 + | S ; = 5 ] Screen Mirroring С Detected in frame_970.jpg: Return & Value Function
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
يد
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|s=s]=E[+1+1+1+2+3+3 +5=5] Return & Value Function • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon يد ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | s = s ] = E [ + 1 + 1 + 1 + 2 + 3 + 3 + 5 = 5 ] Detected in frame_971.jpg: Return & Value Function
cture-1
NNNN
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
. Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
།
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G|s=s]=E[+1+1+2+2 +7³½+3+ |S;=5]
Screen Mirroring С Return & Value Function cture - 1 NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ ། Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G | s = s ] = E [ + 1 + 1 + 2 + 2 + 7³½ + 3 + | S ; = 5 ] Screen Mirroring С Detected in frame_972.jpg: Return & Value Function
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
يل
↓
A A
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
V(5) E|G|5, =s] = [+++++³+3+5=5] Return & Value Function • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon يل ↓ A A Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s V ( 5 ) E | G | 5 , = s ] = [ +++++ ³ + 3 + 5 = 5 ] Detected in frame_973.jpg: Return & Value Function
2-1
NANON
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G₁st=s]=E[r + re+1+√²+2+3+3 + - · |S₁ = 5]
Screen Mirroring Document Return & Value Function 2-1 NANON • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G₁st = s ] = E [ r + re + 1 + √² + 2 + 3 + 3 + - · | S₁ = 5 ] Screen Mirroring Document Detected in frame_974.jpg: Return & Value Function
A A
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
يال
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G, 5, 5] El+1+1+2+2+3+3 +
3 Return & Value Function A A • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon يال ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G , 5 , 5 ] El + 1 + 1 + 2 + 2 + 3 + 3 + 3 Detected in frame_975.jpg: Return & Value Function
NANON
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G₁st=s]=E[r + re+1+7²+2+3+3 + - - · |St = 5]
Stop Screening Document Return & Value Function NANON • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G₁st = s ] = E [ r + re + 1 + 7² + 2 + 3 + 3 + - - · | St = 5 ] Stop Screening Document Detected in frame_976.jpg: Return & Value Function
• Definition of Horizon
⚫ Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
يال
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G|5, s]=E[+1+1+1+2+3+3++
3 Return & Value Function • Definition of Horizon ⚫ Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon يال ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G | 5 , s ] = E [ + 1 + 1 + 1 + 2 + 3 + 3 ++ 3 Detected in frame_977.jpg: Return & Value Function
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
{,
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G❘st=s] =E[+V+1+7²+2+3+3 + · - · |S₁ = 5]
Soren M
D Return & Value Function • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ { , Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G❘st = s ] = E [ + V + 1 + 7² + 2 + 3 + 3 + · - · | S₁ = 5 ] Soren M D Detected in frame_978.jpg: Return & Value Function
ד
⚫ Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process.
Definition of Return. G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
V(s) E|G|5=s]=n+1+1+2+2+3+31 Return & Value Function ד ⚫ Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process . Definition of Return . G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s V ( s ) E | G | 5 = s ] = n + 1 + 1 + 2 + 2 + 3 + 31 Detected in frame_979.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
• Otherwise called finite Markov reward process
Definition of Return, Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G₁|5=s]=E[+1+1+2+2+3+3 + ·| St=5]
NNN Return & Value Function • Definition of Horizon Number of time steps in each episode • Can be infinite • Otherwise called finite Markov reward process Definition of Return , Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G₁ | 5 = s ] = E [ + 1 + 1 + 2 + 2 + 3 + 3 + · | St = 5 ] NNN Detected in frame_980.jpg: Return & Value Function
Definition of Horizon
Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
نا
A A
-
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G5=s] = [+++++³+3+ ··|5=5]
S
NINN Return & Value Function Definition of Horizon Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon نا A A - Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G5 = s ] = [ +++++ ³ + 3 + ·· | 5 = 5 ] S NINN Detected in frame_981.jpg: Return & Value Function
ד
⚫ Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process.
Definition of Return. G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
Definition of State Value Function, V(s) (for a MRP)
Expected return from starting in state s
V(s) E|G|5=5]=B++++2+3+31
3 Return & Value Function ד ⚫ Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process . Definition of Return . G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ Definition of State Value Function , V ( s ) ( for a MRP ) Expected return from starting in state s V ( s ) E | G | 5 = 5 ] = B ++++ 2 + 3 + 31 3 Detected in frame_982.jpg: Return & Value Function
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
NNNN
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G,|s=s]=E[+++++³½+3+ ··
Screen D
Foam Return & Value Function • Definition of Horizon • Number of time steps in each episode • Can be infinite Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon NNNN Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , | s = s ] = E [ +++++ ³½ + 3 + ·· Screen D Foam Detected in frame_983.jpg: Return & Value Function
AAM
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
يا
,,
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
3 Return & Value Function AAM • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon يا ,, Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s 3 Detected in frame_984.jpg: Return & Value Function
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
. Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G, st = s]=E[+++²+2+7³½+3+ ··
B
D
NNNN Return & Value Function • Definition of Horizon • Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , st = s ] = E [ +++ ² + 2 + 7³½ + 3 + ·· B D NNNN Detected in frame_985.jpg: Return & Value Function
2
A
3
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
يا
,,
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(5) E|G₁|5=s] = El++++2+3+3 +5=5] Return & Value Function 2 A 3 • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon يا ,, Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( 5 ) E | G₁ | 5 = s ] = El ++++ 2 + 3 + 3 + 5 = 5 ] Detected in frame_986.jpg: Return & Value Function
исти2-1
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
. Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G, st = s]=E[+++²+2+7³½+3+ ··
B
D
NNNN Return & Value Function исти2-1 • Definition of Horizon • Number of time steps in each episode • Can be infinite . Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G , st = s ] = E [ +++ ² + 2 + 7³½ + 3 + ·· B D NNNN Detected in frame_987.jpg: Return & Value Function
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. Ge (for a MRP)
• Discounted sum of rewards from time step t to horizon
لا
يل
3
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s)=E|G|5=s] = [+++++³+3+ ··|5=5] Return & Value Function • Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . Ge ( for a MRP ) • Discounted sum of rewards from time step t to horizon لا يل 3 Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E | G | 5 = s ] = [ +++++ ³ + 3 + ·· | 5 = 5 ] Detected in frame_988.jpg: Return & Value Function
NNNN
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
{
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G❘s=s]=E[ +7+1+²+2+3+3 + ·· · | S₁ = 5]
Screening
Screenshot
F Return & Value Function NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ { Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G❘s = s ] = E [ + 7 + 1 + ² + 2 + 3 + 3 + ·· · | S₁ = 5 ] Screening Screenshot F Detected in frame_989.jpg: Return & Value Function
3
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. G (for a MRP)
• Discounted sum of rewards from time step t to horizon
يال
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s)=E|G|5=s] = [+1+1+2+2+3+3 + ··|S=5]
L Return & Value Function 3 • Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . G ( for a MRP ) • Discounted sum of rewards from time step t to horizon يال ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E | G | 5 = s ] = [ + 1 + 1 + 2 + 2 + 3 + 3 + ·· | S = 5 ] L Detected in frame_990.jpg: Return & Value Function
NNNN
• Definition of Horizon
• Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return, G (for a MRP)
• Discounted sum of rewards from time step t to horizon
↓
{
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s) =E[G❘st=s] =E[r + 7+1+7²+2+3+3 +- · | St=5]
Screening
Screenshot Return & Value Function NNNN • Definition of Horizon • Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return , G ( for a MRP ) • Discounted sum of rewards from time step t to horizon ↓ { Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E [ G❘st = s ] = E [ r + 7 + 1 + 7² + 2 + 3 + 3 + - · | St = 5 ] Screening Screenshot Detected in frame_991.jpg: Return & Value Function
3
• Definition of Horizon
Number of time steps in each episode
• Can be infinite
⚫ Otherwise called finite Markov reward process
Definition of Return. G (for a MRP)
• Discounted sum of rewards from time step t to horizon
يال
↓
Definition of State Value Function, V(s) (for a MRP)
• Expected return from starting in state s
V(s)=E|G|5=s] = [+1+1+2+2+3+3 + ··|S=5]
L Return & Value Function 3 • Definition of Horizon Number of time steps in each episode • Can be infinite ⚫ Otherwise called finite Markov reward process Definition of Return . G ( for a MRP ) • Discounted sum of rewards from time step t to horizon يال ↓ Definition of State Value Function , V ( s ) ( for a MRP ) • Expected return from starting in state s V ( s ) = E | G | 5 = s ] = [ + 1 + 1 + 2 + 2 + 3 + 3 + ·· | S = 5 ] L Detected in frame_992.jpg: DISCOUNT TIDE
Mathematical conviemant avoid infitte reliime mid vallies!
• Hillmans miten act as if him's a dimant farm
- Drily
- Fum
abomimmediate maand
reward is as linealiquid in rowund
■If episode length are always all 1 DISCOUNT TIDE Mathematical conviemant avoid infitte reliime mid vallies ! • Hillmans miten act as if him's a dimant farm - Drily - Fum abomimmediate maand reward is as linealiquid in rowund ■ If episode length are always all 1 Detected in frame_993.jpg: Discount Factor
ה
T
.
• Mathematically convenient (avoid infinite returns and values)
• Humans often act as if there's a discount factor < 1
70: Only care about immediate reward
7 = 1. Future reward is as beneficial as immediate reward
If episode lengths are always finite, can use 7 = 1 Discount Factor ה T . • Mathematically convenient ( avoid infinite returns and values ) • Humans often act as if there's a discount factor < 1 70 : Only care about immediate reward 7 = 1. Future reward is as beneficial as immediate reward If episode lengths are always finite , can use 7 = 1 Detected in frame_994.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor <1
0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
⚫ If episode lengths are always finite, can use y = 1
S
Diane
Foca
NNNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward ⚫ If episode lengths are always finite , can use y = 1 S Diane Foca NNNNN Detected in frame_995.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screening D
NINN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screening D NINN Detected in frame_996.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screening Document
Screenshot WA] Focus Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screening Document Screenshot WA ] Focus Detected in frame_997.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor <1
0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use = 1
Doct
100 Рос
N Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use = 1 Doct 100 Рос N Detected in frame_998.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
⚫ Humans often act as if there's a discount factor < 1
y= 0: Only care about immediate reward
= 1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen Mirroring Document
Screenshot
Focus
NNNN Discount Factor • Mathematically convenient ( avoid infinite returns and values ) ⚫ Humans often act as if there's a discount factor < 1 y = 0 : Only care about immediate reward = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen Mirroring Document Screenshot Focus NNNN Detected in frame_999.jpg: Discount Factor
• Mathematically convenient (avoid infinite returns and values)
Humans often act as if there's a discount factor < 1
0: Only care about immediate reward
y=1: Future reward is as beneficial as immediate reward
• If episode lengths are always finite, can use y = 1
Screen M
D
Scho
Form
Si Discount Factor • Mathematically convenient ( avoid infinite returns and values ) Humans often act as if there's a discount factor < 1 0 : Only care about immediate reward y = 1 : Future reward is as beneficial as immediate reward • If episode lengths are always finite , can use y = 1 Screen M D Scho Form Si