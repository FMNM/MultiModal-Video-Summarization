{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import torch\n",
    "import whisper\n",
    "from transformers import pipeline\n",
    "import yt_dlp as youtube_dl\n",
    "from keybert import KeyBERT\n",
    "from bertopic import BERTopic\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the audio using yt-dlp\n",
    "def download_audio(video_link, output_audio_path):\n",
    "    ydl_opts = {\n",
    "        \"format\": \"bestaudio/best\",\n",
    "        \"postprocessors\": [{\"key\": \"FFmpegExtractAudio\", \"preferredcodec\": \"wav\", \"preferredquality\": \"192\"}],\n",
    "        \"postprocessor_args\": [\"-ar\", \"16000\"],\n",
    "        \"prefer_ffmpeg\": True,\n",
    "        \"keepvideo\": False,\n",
    "        \"outtmpl\": output_audio_path + \".%(ext)s\",\n",
    "    }\n",
    "    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([video_link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transcribe audio and extract segments using Whisper\n",
    "def transcribe_audio_with_timestamps(audio_path, model_name=\"base\"):\n",
    "    model = whisper.load_model(model_name)\n",
    "    print(\"Transcribing audio...\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    transcription = result[\"text\"]\n",
    "    segments = result[\"segments\"]  # Contains timestamps per segment\n",
    "    return transcription, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization using transformer-based summarization (e.g., BART)\n",
    "def summarize_text_in_chunks(text, chunk_size=1024):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "    # Split text into chunks\n",
    "    text_chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0][\"summary_text\"]\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API Key for ChatGPT\n",
    "openai.api_key = \"your_openai_api_key_here\"\n",
    "\n",
    "\n",
    "# Function to ask questions using RAG-like structure with ChatGPT API\n",
    "def query_chatgpt(context, question):\n",
    "    response = openai.Completion.create(engine=\"gpt-4\", prompt=f\"{context}\\n\\nQ: {question}\\nA:\", max_tokens=150, temperature=0.5)\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to dynamically segment the transcription into topics using BERTopic\n",
    "def topic_segmentation(transcription, segments):\n",
    "    texts = [segment[\"text\"] for segment in segments]\n",
    "    topic_model = BERTopic()  # Initialize BERTopic model\n",
    "\n",
    "    print(\"Segmenting topics...\")\n",
    "    # Use tqdm to show progress while segmenting the topics\n",
    "    topics, _ = topic_model.fit_transform(tqdm(texts, desc=\"Topic segmentation\"))\n",
    "\n",
    "    # Group segments by topics with timeframes\n",
    "    topic_segments = {}\n",
    "    for idx, topic in enumerate(topics):\n",
    "        if topic not in topic_segments:\n",
    "            topic_segments[topic] = {\"text\": [], \"start_time\": segments[idx][\"start\"], \"end_time\": segments[idx][\"end\"]}\n",
    "        topic_segments[topic][\"text\"].append(segments[idx][\"text\"])\n",
    "        topic_segments[topic][\"end_time\"] = segments[idx][\"end\"]  # Update the end time\n",
    "\n",
    "    return topic_segments, topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries for each segmented topic\n",
    "def summarize_topics(topic_segments):\n",
    "    topic_summaries = {}\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "    print(\"Summarizing topics...\")\n",
    "    for topic, data in tqdm(topic_segments.items(), desc=\"Summarizing each topic\"):\n",
    "        full_text = \" \".join(data[\"text\"])  # Combine all texts for this topic\n",
    "        summary = summarizer(full_text, max_length=150, min_length=40, do_sample=False)[0][\"summary_text\"]\n",
    "        topic_summaries[topic] = {\"summary\": summary, \"start_time\": data[\"start_time\"], \"end_time\": data[\"end_time\"]}\n",
    "\n",
    "    return topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to process video, transcribe, segment by topic, summarize, and enable Q&A with ChatGPT\n",
    "def process_lecture_video(video_link, audio_output_path):\n",
    "    # Step 1: Download the audio from the video\n",
    "    print(\"STEP 1:....\")\n",
    "    download_audio(video_link, audio_output_path)\n",
    "\n",
    "    # Step 2: Transcribe the audio and get timestamps for each segment\n",
    "    print(\"\\nSTEP 2:....\")\n",
    "    transcription, segments = transcribe_audio_with_timestamps(audio_output_path + \".wav\")\n",
    "\n",
    "    # Step 3: Generate overall summary of the lecture\n",
    "    print(\"\\nSTEP 3:....\")\n",
    "    overall_summary = summarize_text_in_chunks(transcription, chunk_size=1024)\n",
    "\n",
    "    # Step 4: Segment the transcription by topics dynamically\n",
    "    print(\"\\nSTEP 4:....\")\n",
    "    topic_segments, topic_model = topic_segmentation(transcription, segments)\n",
    "\n",
    "    # Step 5: Summarize each segmented topic\n",
    "    print(\"\\nSTEP 5:....\")\n",
    "    topic_summaries = summarize_topics(topic_segments)\n",
    "\n",
    "    return overall_summary, topic_summaries, topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle queries, providing both answer and relevant timeframe\n",
    "def query_with_timeframe(question, transcript, segments, topic_summaries):\n",
    "    # Answer the question using ChatGPT API\n",
    "    answer = query_chatgpt(transcript, question)\n",
    "\n",
    "    # Find relevant topic timeframe from answer\n",
    "    relevant_timeframe = None\n",
    "    for topic, summary in topic_summaries.items():\n",
    "        if answer.lower() in summary[\"summary\"].lower():\n",
    "            relevant_timeframe = (summary[\"start_time\"], summary[\"end_time\"])\n",
    "            break\n",
    "\n",
    "    return answer, relevant_timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "video_link = \"https://www.youtube.com/watch?v=AhyznRSDjw8\"\n",
    "audio_output_path = \"downloaded_audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process the lecture video to get summaries and topics\n",
    "# overall_summary, topic_summaries, topic_model = process_lecture_video(video_link, audio_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the audio from the video\n",
    "print(\"STEP 1:....\")\n",
    "download_audio(video_link, audio_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Transcribe the audio and get timestamps for each segment\n",
    "print(\"STEP 2:....\")\n",
    "transcription, segments = transcribe_audio_with_timestamps(audio_output_path + \".wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate overall summary of the lecture\n",
    "print(\"STEP 3:....\")\n",
    "overall_summary = summarize_text_in_chunks(transcription, chunk_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Segment the transcription by topics dynamically\n",
    "print(\"STEP 4:....\")\n",
    "topic_segments, topic_model = topic_segmentation(transcription, segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Summarize each segmented topic\n",
    "print(\"STEP 5:....\")\n",
    "topic_summaries = summarize_topics(topic_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the overall summary and topic summaries\n",
    "print(\"Overall Summary of the lecture:\", overall_summary)\n",
    "for topic, summary in topic_summaries.items():\n",
    "    print(f\"Topic {topic}: {summary['summary']} (Time: {summary['start_time']} to {summary['end_time']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ask a query and get answer with timeframe\n",
    "question = \"What is the main topic discussed?\"\n",
    "answer, timeframe = query_with_timeframe(question, overall_summary, topic_summaries)\n",
    "print(f\"Answer: {answer}, Relevant Timeframe: {timeframe}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
