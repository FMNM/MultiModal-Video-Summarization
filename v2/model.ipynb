{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import whisper\n",
    "from transformers import pipeline\n",
    "import yt_dlp as youtube_dl\n",
    "from bertopic import BERTopic\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the audio using yt-dlp\n",
    "def download_audio(video_link, output_audio_path):\n",
    "    ydl_opts = {\n",
    "        \"format\": \"bestaudio/best\",\n",
    "        \"postprocessors\": [{\"key\": \"FFmpegExtractAudio\", \"preferredcodec\": \"wav\", \"preferredquality\": \"192\"}],\n",
    "        \"postprocessor_args\": [\"-ar\", \"16000\"],\n",
    "        \"prefer_ffmpeg\": True,\n",
    "        \"keepvideo\": False,\n",
    "        \"outtmpl\": output_audio_path + \".%(ext)s\",\n",
    "    }\n",
    "    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([video_link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transcribe audio and extract segments using Whisper\n",
    "def transcribe_audio_with_timestamps(audio_path, model_name=\"base\"):\n",
    "    model = whisper.load_model(model_name)\n",
    "    print(\"Transcribing audio...\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    transcription = result[\"text\"]\n",
    "    segments = result[\"segments\"]  # Contains timestamps per segment\n",
    "    return transcription, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization using transformer-based summarization (e.g., BART)\n",
    "def summarize_text_in_chunks(text, chunk_size=1024):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
    "\n",
    "    # Split text into chunks\n",
    "    text_chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0][\"summary_text\"]\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API Key for ChatGPT\n",
    "openai.api_key = \"your_openai_api_key_here\"\n",
    "\n",
    "\n",
    "# Function to ask questions using RAG-like structure with ChatGPT API\n",
    "def query_chatgpt(context, question):\n",
    "    response = openai.Completion.create(engine=\"gpt-4\", prompt=f\"{context}\\n\\nQ: {question}\\nA:\", max_tokens=150, temperature=0.5)\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to dynamically segment the transcription into topics using BERTopic\n",
    "def topic_segmentation(segments):\n",
    "    texts = [segment[\"text\"] for segment in segments]\n",
    "    topic_model = BERTopic()  # Initialize BERTopic model\n",
    "\n",
    "    print(\"Segmenting topics...\")\n",
    "    # Use tqdm to show progress while segmenting the topics\n",
    "    topics, _ = topic_model.fit_transform(tqdm(texts, desc=\"Topic segmentation\"))\n",
    "\n",
    "    # Group segments by topics with timeframes\n",
    "    topic_segments = {}\n",
    "    for idx, topic in enumerate(topics):\n",
    "        if topic not in topic_segments:\n",
    "            topic_segments[topic] = {\"text\": [], \"start_time\": segments[idx][\"start\"], \"end_time\": segments[idx][\"end\"]}\n",
    "        topic_segments[topic][\"text\"].append(segments[idx][\"text\"])\n",
    "        topic_segments[topic][\"end_time\"] = segments[idx][\"end\"]  # Update the end time\n",
    "\n",
    "    return topic_segments, topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries for each segmented topic\n",
    "def summarize_topics(topic_segments, chunk_size=1024):\n",
    "    topic_summaries = {}\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)  # Use CPU by setting device=-1\n",
    "\n",
    "    print(\"Summarizing topics...\")\n",
    "    for topic, data in tqdm(topic_segments.items(), desc=\"Summarizing each topic\"):\n",
    "        full_text = \" \".join(data[\"text\"])  # Combine all texts for this topic\n",
    "\n",
    "        # Split the full text into chunks to handle large inputs\n",
    "        text_chunks = [full_text[i : i + chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
    "        summaries = []\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0][\"summary_text\"]\n",
    "            summaries.append(summary)\n",
    "\n",
    "        # Combine chunk summaries and store them\n",
    "        topic_summaries[topic] = {\"summary\": \" \".join(summaries), \"start_time\": data[\"start_time\"], \"end_time\": data[\"end_time\"]}\n",
    "\n",
    "    return topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to process video, transcribe, segment by topic, summarize, and enable Q&A with ChatGPT\n",
    "def process_lecture_video(video_link, audio_output_path):\n",
    "    # Step 1: Download the audio from the video\n",
    "    print(\"STEP 1:....\")\n",
    "    download_audio(video_link, audio_output_path)\n",
    "\n",
    "    # Step 2: Transcribe the audio and get timestamps for each segment\n",
    "    print(\"\\nSTEP 2:....\")\n",
    "    transcription, segments = transcribe_audio_with_timestamps(audio_output_path + \".wav\")\n",
    "\n",
    "    # Step 3: Generate overall summary of the lecture\n",
    "    print(\"\\nSTEP 3:....\")\n",
    "    overall_summary = summarize_text_in_chunks(transcription, chunk_size=1024)\n",
    "\n",
    "    # Step 4: Segment the transcription by topics dynamically\n",
    "    print(\"\\nSTEP 4:....\")\n",
    "    topic_segments, topic_model = topic_segmentation(transcription, segments)\n",
    "\n",
    "    # Step 5: Summarize each segmented topic\n",
    "    print(\"\\nSTEP 5:....\")\n",
    "    topic_summaries = summarize_topics(topic_segments)\n",
    "\n",
    "    return overall_summary, topic_summaries, topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle queries, providing both answer and relevant timeframe\n",
    "def query_with_timeframe(question, transcript, segments, topic_summaries):\n",
    "    # Answer the question using ChatGPT API\n",
    "    answer = query_chatgpt(transcript, question)\n",
    "\n",
    "    # Find relevant topic timeframe from answer\n",
    "    relevant_timeframe = None\n",
    "    for topic, summary in topic_summaries.items():\n",
    "        if answer.lower() in summary[\"summary\"].lower():\n",
    "            relevant_timeframe = (summary[\"start_time\"], summary[\"end_time\"])\n",
    "            break\n",
    "\n",
    "    return answer, relevant_timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "video_link = \"https://www.youtube.com/watch?v=AhyznRSDjw8\"\n",
    "audio_output_path = \"downloaded_audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process the lecture video to get summaries and topics\n",
    "# overall_summary, topic_summaries, topic_model = process_lecture_video(video_link, audio_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1:....\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=AhyznRSDjw8\n",
      "[youtube] AhyznRSDjw8: Downloading webpage\n",
      "[youtube] AhyznRSDjw8: Downloading ios player API JSON\n",
      "[youtube] AhyznRSDjw8: Downloading web creator player API JSON\n",
      "[youtube] AhyznRSDjw8: Downloading m3u8 information\n",
      "[info] AhyznRSDjw8: Downloading 1 format(s): 251\n",
      "[download] Destination: downloaded_audio.webm\n",
      "[download] 100% of   45.54MiB in 00:00:03 at 12.82MiB/s    \n",
      "[ExtractAudio] Destination: downloaded_audio.wav\n",
      "Deleting original file downloaded_audio.webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download the audio from the video\n",
    "print(\"STEP 1:....\")\n",
    "download_audio(video_link, audio_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2:....\n",
      "Transcribing audio...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Transcribe the audio and get timestamps for each segment\n",
    "print(\"STEP 2:....\")\n",
    "transcription, segments = transcribe_audio_with_timestamps(audio_output_path + \".wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3:....\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate overall summary of the lecture\n",
    "print(\"STEP 3:....\")\n",
    "overall_summary = summarize_text_in_chunks(transcription, chunk_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4:....\n",
      "Segmenting topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Topic segmentation: 100%|██████████| 750/750 [00:00<00:00, 748270.22it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0393a4ef17104c3f920818b339c6900a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cddd741c1fd24fddace1e4061dac22db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880b25db71f14ef598ddfa5ee8989058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ae2a5d6b7b4fe2b0410b9dda2272b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a5382c22174bf08df0a2b9174fd8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c6f4d052754e7aaec6b151dba6d76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b51d99e9ed4a76a7a79f3358aa58e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ed0615ae8b4c94a0712dcd4868f18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837e5017ce954fed888900007eb1f40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cb33e6f91642ac9e6597f5945c1ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e074cd0985f4e03b163153cd0be6480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4: Segment the transcription by topics dynamically\n",
    "print(\"STEP 4:....\")\n",
    "topic_segments, topic_model = topic_segmentation(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5:....\n",
      "Summarizing topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing each topic:   5%|▌         | 1/20 [00:05<01:45,  5.54s/it]Your max_length is set to 150, but your input_length is only 112. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=56)\n",
      "Summarizing each topic:  10%|█         | 2/20 [00:23<03:48, 12.67s/it]Your max_length is set to 150, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Summarizing each topic:  15%|█▌        | 3/20 [00:38<03:52, 13.67s/it]Your max_length is set to 150, but your input_length is only 72. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\n",
      "Summarizing each topic:  20%|██        | 4/20 [00:49<03:23, 12.75s/it]Your max_length is set to 150, but your input_length is only 35. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Summarizing each topic:  25%|██▌       | 5/20 [01:46<07:09, 28.64s/it]Your max_length is set to 150, but your input_length is only 94. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      "Summarizing each topic:  30%|███       | 6/20 [01:50<04:43, 20.26s/it]Your max_length is set to 150, but your input_length is only 15. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
      "Summarizing each topic:  35%|███▌      | 7/20 [02:07<04:12, 19.44s/it]Your max_length is set to 150, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Summarizing each topic:  40%|████      | 8/20 [02:14<03:05, 15.45s/it]Your max_length is set to 150, but your input_length is only 62. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n",
      "Summarizing each topic:  60%|██████    | 12/20 [02:57<01:26, 10.81s/it]Your max_length is set to 150, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Summarizing each topic:  65%|██████▌   | 13/20 [03:21<01:44, 14.86s/it]Your max_length is set to 150, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Summarizing each topic:  70%|███████   | 14/20 [03:29<01:17, 12.86s/it]Your max_length is set to 150, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Summarizing each topic:  75%|███████▌  | 15/20 [03:37<00:57, 11.42s/it]Your max_length is set to 150, but your input_length is only 140. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n",
      "Summarizing each topic:  85%|████████▌ | 17/20 [04:23<00:54, 18.01s/it]Your max_length is set to 150, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Summarizing each topic:  90%|█████████ | 18/20 [04:40<00:35, 17.79s/it]Your max_length is set to 150, but your input_length is only 117. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n",
      "Summarizing each topic:  95%|█████████▌| 19/20 [04:51<00:15, 15.50s/it]Your max_length is set to 150, but your input_length is only 81. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Summarizing each topic: 100%|██████████| 20/20 [05:00<00:00, 15.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Summarize each segmented topic\n",
    "print(\"STEP 5:....\")\n",
    "topic_summaries = summarize_topics(topic_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Summary of the lecture: Two lectures today are really exciting because they start to move beyond a lot of what we've talked about in the class so far, which is focusing a lot on really static data sets. In today, in this lecture right now, I'm going to start to talk about how we can learn about this very long-standing field of reinforcement learning. In the real world, you have your deep learning model actually deployed together with the data. This is the key motivation of reinforcement learning. You're going to try and learn through reinforcement, making mistakes in your world, and then collecting data on those mistakes to learn how to improve. \"I'm hoping for a 5-0 not to lose any games, but I think the realistic goal would be 4-1 in my favour\" \"I wasn't expecting that good. Everything that he did was proper. It was calculated and it was done well\" We've really covered two different types of learning in this course to date. Supervised learning is in this domain where we're given data in the form of x's, our inputs, and our labels y. Our goal here is to learn a function or a neural network that can learn to predict why. Unsupervised learning is a type of learning that has no notion of labels. In reinforcement learning, we're going to be only given data in the form of black and white sketches. In this lecture, we'll talk about yet another type oflearning algorithms. The goal of reinforcement learning is to build an agent that can learn how to maximize what are called rewards. This is the third component that is specific to reinforcement learning. You want to maximize all of those rewards over many, many time steps in the future. We're going to be focusing exclusively on this third type of learning paradigm. An agent is a being, basically, that can take actions. For example, you can think of an agent as a machine that is, let's say, an autonomous drone. The environment is simply the world where that agent lives and where it operates. An agent can send commands to that environment in the form of what are called actions. It can take actions in that environment and let's say the possible set of all actions that it could take is a set of capital A. This is essentially how the environment responds back to the agent. Reinforcement learning is a bit unique because it has one more component here in addition to these other components, which is called the reward. Now, the reward is a feedback by which we measure or we can try to measure the success of a particular agent in its environment. When we look at the total reward that an agent accumulates over the course of its lifetime, we can simply sum up all of the rewards that an agents gets after a certain time t. Its capital R of t is the sum of all rewards from that point on into the future into infinity. The algorithm uses a discounting factor to make rewards worth less than what they might be in the future. This factor is like multiple, like I said, multiplied it every single future reward exponentially. It's important to understand that also typically this discounting factors is, you know, between zero and one. The Q function takes as input two different things. The first is the state that you're currently in. The second is a possible action that you could execute in this particular state. The Q function of these two pieces is going to denote or. t today. The Q function tells us for any possible action, right? What is the expected reward for that action to be taken? So if we wanted to take a specific action given a specific state, ultimately we ultimately we n The way we do that from a Q function is simply to pick the action that will maximize our future reward. And we can simply pry out number one. If we have a continuous action space, maybe we do something a bit more intelligent. There are two classes of reinforcement learning algorithms that we're going to briefly touch on as part of today's lecture. The first class is what's going to be called value learning. And that's exactly this process that we've just talked about. The second class of algorithms is kind of a different framing of the same approach. First optimizing the Q function and finding the Q value and then using t. We'll build up that intuition and that knowledge onto the second part of policy learning. So maybe let's start by just digging a bit deeper into the Q function specifically, just to start to understand how we could estimate this in the beginning. The objective of the game is to basically eliminate all of those rainbow pixels, so you want to keep hitting that ball against the top of the screen until you remove all the pixels. The Q function tells us, you know, the expected total return, or the total reward, that we can expect based on a given state and action pair that we may find ourselves in. The state is basically that the ball is coming slightly at an angle, we're not quite underneath it yet and we need to move towards it and actually hit that ball in a way that, you know, will make it and not miss it, hopefully, right? So hopefully that ball doesn't pass below us then the game would be over. raight down towards us, that's our state, our action is to do nothing and simply reflect that ball back up vertically up. A is a very conservative action. It's just bouncing up, hitting one point at a time from the top and breaking off very slowly the board that you can see here. With B, with B, you're kind of having agency, like one of the answers said. What that implies is that you're sometimes going to actu The algorithm, the agent, can actually learn that hitting the side of the board can have some kind of unexpected consequences. Here you see it trying to enact that policy. Once it reaches a breakout on the side, it found this hack in the solution. We're now breaking off a ton of points. A neural network takes as input two objects: the state of the board and an action. In this case, the actions that a neural network or an agent could take in this game is to move to the right to the left to stay still. The goal here is to estimate the single number output that measures what is the expected value or the expected Q value of this neural network at this particular state action. That way you only need to run your neural network once for the given state that you're in. And then that neural network will tell you for all possible actions, what's the maximum? The question I want to pose here is really, we want to train one of these two networks. Let's stick with the network on the right for simplicity. And the question is, how do we actually train that network? And specifically, I want all of you to think about really the best case scenario. N initial reward plus selecting some action in our action space that maximizes our expected return, then for the next future state, we need to apply that discounting factor and recursively apply the same equation. Now we can ask basically what does our neural network predict, right? So that's our target and recall from previous lectures, if we have a target value. Deep neural network that we're trying to train looks like this, right? It takes us input a state, is trying to output N different numbers, those N different Numbers correspond to the Q value associated to N different actions, one Q value per action. So we have the ground truth labels to train and supervise this model. The policy function is a function that determines what is the best action. It's a very end-to-end way of thinking about the agent's decision-making process. We can determine that policy function directly from the Q function itself simply by maximizing and optimizing all of the different Q values. The optimal action here is simply going to be the maximum of these three Q values, in this case, it's Going to be 20. Now we can send this action back to the environment in the form of the game to execute the next step, right? And as the agent moves through this environment, it will be responded with new pixels that come from the game, and more importantly, some reward signal. Google DeepMind showed that you could train a Q value network for Atari Breakout games. The network learns from actions that don't do very well, that you discourage them and try to do actions that did perform well more frequently. QLearning is naturally applicable to discrete action spaces, because you can think of this output space that we're providin. Now, there are several very important downsides of QLearning, and hopefully these are going to motivate the second part of today's lecture. QLearning is very well suited for discrete action spaces, and we'll talk about ways of overcoming that with other approaches a bit later. The policy that we're learning, the Q function is giving rise to that policy. That policy is determined by, you know, deterministically optimizing that Q function. Policy learning is a different class of reinforcement learning algorithms that are different than QLearning algorithms. In policy grading algorithms, the main difference is that instead of trying to infer the policy from the Q function, we're just going to build a neural network that will directly learn that policy function from the data. L network, outputs these Q values, one value per action, and we determine the policy by looking over this state of Q values. We pick the value that has the highest and looking at its corresponding action. And that represents not only the best action that we should take, but the probability that selecting that action would result in a very desirable outcome. Going left has the probability of being the highest value action with 90% staying in the center. The probability of 10% going right is 0%. So, ideally, what our neural networks should do in this case is 90% of the time in this situation go to the left. The policy network has a very formulated output. All of the numbers here in the output have to sum to one because this is a probability distribution. And that gives it a very rigorous version of how we can train this model. We could also use this same formulation to move beyond discrete action spaces, like you can see here, which are one possible action. Now, we may have a space, which is not what action should I take, go left, right, or stand center, but rather how quickly should I move? Right? That is a continuous variable, as opposed to a discrete variable. If we want to determine the best action to take, we would simply take the mode of this distribution. That would be the speed at which we should move. If we wanted to also try out different things and explore our space, we could sample from this distribution and still obtain some stochasticity. So instead, what if we parameterized our action space by a distribution, right? So let's take, for example, the Gaussian distribution. To parameterize a Gaussian. distribution, we only need two outputs, right. We need a mean and a variance. The state could be obtained through many sensors that could be mounted on the vehicle itself. The action that we could take is a steering-wheel angle. It's actually an angle that could take any real number. And finally, the reward in this very simplistic example is the distance that we travel before we crash. Reinforcement learning is almost like a data acquisition plus learning pipeline combined together. So we start by initializing our agent, right. And in the beginning, of course, it knows nothing about driving. It's never been exposed to any of these rules of the environment or the observation before. The first step of training that we'll do is to take, excuse me, to take the later half of our trajectory that our agent ran and decrease the probability of actions that resulted in low rewards. Now again, there's no reason why there shouldn't necessarily be a good action that we took in the first half of this trajectory. Re-initialize the agent one more time and run it until completion. Now the agent goes a bit farther, right, because you've decreased the probabilities at the end, increased the probabilities of the future. And you keep repeating this over and over again until you notice that the agent learns to perform better and better every time until it finally converges. The loss function for a policy gradient neural network looks like this, and then we'll start by dissecting it to understand why this works the way it does. So here we can see that the loss consists of two terms. The first term is this term in green, which is called the log likelihood. The second term is something that all of you are very familiar with already. This is simply the return at a specific time, right, so that's the expected return on the words that you would get after this time point. If we got a lot of reward for a particular action that had high probability, that means that we want to increase that probability even further. On the other hand, if we obtained a reward that was very low for an action that has high likelihood, we want the inverse effect. We never want to sample that action again in the future because it resulted in a low reward. In our simplified example on the the car example, all the things that had low rewards were exactly those actions that came closest to the termination part of the of the vehicle. Now we can plug this into the loss of gradient descent algorithm to train our neural network when we see, you know, this policy gradient algorithm.  ethod gets its name from this policy gradient piece that you can see here. If you wanted to do this in reality, right, that essentially means that you want to go out, collect your car, crashing it a bunch of times just to learn how to not crash it. And that's, you know, that's simply not feasible. When we're training reinforcement learning algorithms, we ultimately want them to be, you know, not operating in simulation. Modern simulation engines for reinforcement learning and generally, very broadly speaking, modern simulators for vision specifically do not at all capture reality very accurately. MIT lab develops new photorealistic simulation engine. Called Vista, it uses real data from the real world to re-sim environments. This is actually an autonomous agent, not a real car, driving through our virtual simulator. The technology allows you to train these agents entirely using reinforcement learning, no human labels, but importantly, allow them to be transferred into reality because there's no sim-to-real gap anymore. So for example, let's say you take your car, you drive out on Massab, you collect data of Massab. You can now drop a virtual agent into that same simulated environment, observing new viewpoints of what that scene might have looked like. This is the first time ever that reinforcement learning was used to train a policy end-to-end for an autonomous vehicle that could be deployed in reality. This environment completely autonomous in the real world. No transfer learning is done here. There is no augmentation of data from real world data. This is entirely trained using simulation. The objective here was to build a reinforcement learning algorithm to master the game of Go. The number of possible action spaces and possible states that this board could be placed into is greater than the number of atoms in the unit. Google DeepMind rose to this challenge. Using a reinforcement learning pipeline, they were able to defeat the grand champion human players. And the idea that's core was actually very simple. The first step is that you train a neural network to basically watch human level experts. Auxiliary network was almost hallucinating, right? So it would take its human understandings, try to imitate the humans first of all. From that imitation, they would pin these two neural networks against themselves, play a game against themselves. A neural network can learn to outperform the solution that was created by humans. It can also outperform a solution created, which was bootstrapped by humans as well. We saw two difinitely different board states that it could take from this particular state. The deadline for these competitions will be, well, it was originally set to be Thursday, which is tomorrow at 11 p.m. ferent types of reinforcement learning approaches of how we could optimize these solutions. First being Q learning where we're trying to actually estimate, given a state, you know, what is the value that we might expect for any possible action in the second way, was to take a much more end-to-end approach.\n",
      "Topic 5: CNN.com's John Sutter takes a look back at some of the most memorable moments in the history of CNN.com. This week, Sutter looks back at a moment in time when the world was at its most vulnerable. Sutter also looks at a time in the future where the world is at its greatest potential. (Time: 0.0 to 3358.36)\n",
      "Topic 6: Two lectures today are really exciting because they start to  move beyond a lot of what we've talked about in the class so far. And specifically in today, in this lecture right now, I'm going to start to talk about how  we can learn about this very long-standing field of how we can specifically marry two already started learning about as part of this course. In today's lecture, we're going to be focusing exclusively on this third type of learning. It's important to remember that unlike other types of learning that we've covered in this  lecture, it's actually very useful for all of us to consider the sum of all of these. The class was originally set to be Thursday, which is tomorrow at 11 p.m. ,  Now I want to take maybe just a very brief second towards the end of the class here just to talk  about, you know, some of the challenges and keep you in line with the first lecture today. And from this first step,  by humans as well. So with that all summarized very quickly what we've learned today and conclude for  well, it was originallyset to be Wednesday. (Time: 11.200000000000001 to 3451.48)\n",
      "Topic 3: The goal here is to learn a function or a neural network that can learn to predict why. The type of the neural network here naturally, because we have a function that takes us  Let's imagine our neural network will also take as input these two objects. We have the ground truth labels to train and supervise this model. So our deep neural network that we're trying to train looks like this, right?  Now one very popular or very famous approach that showed this was presented by DeepMind. Rks should do in this case is 90% of the time in this situation  And that gives it a very rigorous version of how we can train this model that makes it a bit easier. Remember that we have no training data, right. \"You could actually see that you could, without any.  started pinning themselves against each other,\" he said. \"You could see that  you could.  start pinning yourself against eachother, and you could,\" he added. (Time: 21.04 to 3376.36)\n",
      "Topic 9: The goal of reinforcement learning is to build an agent that can learn how to maximize. This is the third component that is specific to reinforcement learning. In reinforcement learning, we're going to be only given data in the form of what are the contex. Reinforcement learning is almost like a data acquisition plus learning pipeline combined together. No human  this environment completely autonomous in the real world. This represented actually the first time ever that reinforcement learning was used to train of reinforcement learning and policy learning. This is not using reinforcement learning. This is using supervised learning,  bootstrap, in reinforcement learning algorithm, that would play against itself in order to learn  the day. So we've talked a lot about really the foundational algorithms underlying reinforcement  learning. (Time: 32.8 to 3408.92)\n",
      "Topic -1: The marriage of these two fields is actually really fascinating to me, particularly  but that data set is typically fixed in our world, right?  or deep learning algorithm, and then we can evaluate on a brand new data set. Now you can even imagine this combination of robotics  Now training robots to play against us in the real world. Supervised learning is in this domain where we're given data in the form of x's, our inputs,  and our labels y. Unsupervised learning uses latent variables, these hidden features in your data. In today's lecture, we're going to talk about yet another type of learning algorithms. A drone that is making a delivery or, for example, in a game. It could be Super Mario that's navigating inside of your video game. The possible set of all actions that it could take is let's say a set of capital  A. The dampening factor is designed to make future  between zero and one. This is the game of called Atari Breakout. For A, you only have like the maximum you can take off is like one because after you reflect  W. A is a very conservative action. It will solve the game. With B, you're kind of having agency, like one of the answers said. It turns out that the algorithm, the agent, can actually learn that hitting the side of  We're now breaking off a ton of points. The question I want to pose here is really, we want to train one of these. And specifically, I want all of you to think about really the best case scenario just to  This would mean that essentially the target return, right, the predicted or the value that  we're trying to predict, the target is going to always be maximized. The Atari Breakout ecosystem is a network for a variety of different tasks. The reward signals in Pong, or sorry, in Atari  this one network. The really fascinating thing that they showed was for this very simple  from actions that don't do very well, that you discourage them and trying to do actions. In the last lecture, we also saw how we could learn to predict uncertainties. That is a continuous variable, as opposed to a discrete variable. And you could say that now the answer should look like this, right?  over the entire real number line, first of all. The first step of training that we'll do is to take, excuse me, to take the later half of our  trajectory. So we start our vehicle, our agent. ed through many sensors that could  to the vehicle. The action that we could take is a steering-wheel angle. A bad action in the later half. is trajectory  and a bad action  is later. The second term is something that all of you are very familiar with already. This is simply the return at a specific time, right, so that's the expected return on the  that was very low f. MIT researchers have developed a way to make computer programs behave like real cars. The program uses a neural network to predict what will happen in a given situation. It is not possible to deploy this approach into reality, but there are ways around it. All of the training was done entirely in simulation. There is no augmentation of data from real world data. This is entirely trained using simulation. But now that we covered all of these foundations  was a human champion, obviously. So Google DeepMind rose to this challenge. The deadline for these competitions will be, and the deadline for the competition is, March 31. The goal is to help the network learn to not only outperform the solution that,  but to also outperform that solution. (Time: 47.120000000000005 to 3441.88)\n",
      "Topic 17: \"We want to try out different things and explore our space,  at all capture reality very accurately\" \"If we wanted to starve.  space.  real world. value.  data, together out into reality, exploring, interacting with its environment\" (Time: 87.56 to 2941.7200000000003)\n",
      "Topic 4: The first main piece of terminology is that of an agent. An agent is a being, basically, that can take actions. The environment is simply the world where that agent lives and where it operates. The agent can send commands to that environment in the form of what are called actions. The data acquisition pipeline is first toinitial our agent. And as the agent moves through this environment, it's going to be responded with not only  this class with.  and is discovered by the agent. Now we can send this action back to the environment in the form of the game to execute the next. \"We placed agents into our simulator. We trained them  and we deployed them on board our full Massab\" \"The agent learns to perform  better and better every time until it finally converges. And at the end, the agent is able to\" The self-driving car will be the first of its kind in the U.S. It will be a fully-automated, full-scale autonomous vehicle. The car will have a range of up to 100 miles before it will need to be re-programmed. (Time: 126.8 to 3082.12)\n",
      "Topic 12: DeepBind is a game that allows you to train your computer to play games at a higher level. The game is designed to teach you the side of gameplay and strategy-making together with gameplay, right? Go is played on a 19 by 19 board. The objective of Go is to beat the grand master level players. So the number one player in the world of Go  type of player or a human grand master would take based on a given board state. (Time: 136.72 to 3269.4)\n",
      "Topic 1: The objective of the game is to basically eliminate all of those rainbow pixels, so you want to keep hitting  that ball against the top of the screen until you remove all the pixels. I'm hoping for a 5-0 not to lose any games, but I think the realistic goal would be  It's not necessarily just going right or left or straight. The board is bouncing up, hitting one point at a time from the top and breaking off very slowly. Once it reaches a breakout on the side of the board, it found this hack in the solution. The state of the board is the pixels that are on the screen describing that board. You can think of this as simply the pixels   to the right to the left to stay still. Usually it takes a few time steps for that ball to travel back up to the top of the screen. Left has the probability of being the highest value action with 90% staying in the center. 10% of the time it could still try staying it where it is, but never it should go to  the right. The 19 by 19 grid is the size of the board that can be placed into. The number of board pieces is greater than the number of atoms in the unit. The game itself states that this board could be placed in any of 19 positions. (Time: 172.56 to 3180.12)\n",
      "Topic 18: A model could understand and cluster certain parts of these images together. For example, if you consider this example of an apple, observing a bunch of images of  apples. We want to detect in the future if we see a new image of an Apple to detect that this is indeed an apple. (Time: 268.64 to 1341.84)\n",
      "Topic 11: The algorithm itself, it's important to remember that the algorithm is the agent.  agreediness in the algorithm, right? The second class of algorithms, which we'll touch on right at the end of today's that did perform well more frequently, very simple algorithm. We ultimately want them to be, you know, not operating in simulation. We want them  applications that we're seeing. And one very popular application that a lot of people will tell  is an extraordinarily complex game for an artificial algorithm to try and master. (Time: 289.0 to 3390.76)\n",
      "Topic 8: A state is simply a concrete and immediate situation that the agent finds itself in at  time T. In a given state, an agent can send out any form of actions to take some decisions. States are observations. The actions are the behaviors that this agent takes in those particular states. So what if instead you only provided an input of your state.  took all of the ideal actions at any given state?  Given a state, what is the best action?  to take, given any state. Those could be three different actions that could be provided and parameterized. (Time: 347.96 to 3435.6400000000003)\n",
      "Topic 2: The reward is a feedback by which we measure or we can try to measure the success of an action. It's also very important to remember that not all actions result in immediate rewards. You want to maximize all of those rewards over many, many time steps in the future. The discounting factor is essentially multiplied by every future reward that the agent sees. Rewards essentially worth less than rewards that we might see at this instant, at this moment. So for example, if I offered you a reward of $5 today or a reward in 10 years from  now, I think all of you would prefer that $5 now. R of T is discounted number one and number two, we're  take that will maximize this reward. What is the expected reward for that action to be taken?  future reward. And finally, the reward in this very simplistic example is the distance that we travel before we crash. It's very possible that these are noisy rewards as well, because  words that you would get after this time point. Now we can plug this into the  data set  trajectory that our agent ran and decrease the probability of actions that resulted in low rewards. The losers would try to negate all of the actions that they may have acquired from their  that might be very beneficial to achieving superhuman performance. So for those of you who are not  reward. (Time: 370.24 to 3314.5200000000004)\n",
      "Topic 14: In some situations, your action space does not necessarily need to be a finite. Maybe you could take actions in a continuous space. Now, if we have a continuous action space, we have to think about clever ways to work  discrete action spaces. The logical rules are actually quite simple, the number of possible action spaces and possible timal action spaces that we want to take. So instead, what if we parameterized our action space by a distribution, right? (Time: 503.88 to 3174.92)\n",
      "Topic 15: The process that we've just talked about.  And that can be expanded to look exactly like this. And that's exactly how this is captured here as well mathematically. So it's step two, r. In reality, that essentially  like from different types of perturbations or types of angles that it might be exposed to. ight? If you wanted to do this in reality, right. (Time: 643.84 to 3047.2400000000002)\n",
      "Topic 7: Policy learning is the study of how to use data to make better decisions. In policy networks, the idea that we want to keep her safe is a key goal. The next part of today's lecture will be focused on policy learning. A policy gradient neural network can be trained to solve an observation. It runs its policy, which right now is untrained entirely, until it terminates. Then it update that policy as part of this algorithm that I'm showing you. The goal is to build like a policy that would imitate some of the rough patterns that a human would make. The method gets its name from this policy gradient piece that you can see here. Using the exact algorithms that you learned about in today's lecture, these policy gradient  a policy end-to-end for an autonomous vehicle could be deployed in reality. (Time: 688.16 to 3264.44)\n",
      "Topic 0: The Q function takes as input two different things,  at time T, and the Q function of these two pieces is going to denote or capture what the  a really powerful function, right?  If you had access to this type of function, this Q function, I think you could actually use it to infer the highest Q values. The Q function tells us, you know, the expected total return. First of all, how can we construct and learn that Q function from data? And then, of course, the final step is use that Qfunction to take some actions in the code. If we have the Q function, we can directly use it to determine  this Q function?  value or the expected Q value of this neural network at this particular state action. Now for example, we want to formulate a loss function that's going to essentially case our Q value is a continuous variable. The Q function tells us given a state, what is the best, or the value, the return. We can determine that policy function directly from the Q function itself. Google DeepMind several years ago, showed that you could train a Q value network. The policy that we're learning, the Q function is giving  That policy is determined by, you know, deterministically optimizing that Q function. We simply look at the results from the Qfunction and apply our policy. We pick the action that has the best or the highest Q value, and we're just going to build a neural network that will learn. The loss consists of two terms. The first term is this term in green, which is called the log likelihood right? And you'll notice that this loss function right here by including this negative,  optimize these solutions. First being Q learning where we're trying to actually estimate, given a state, (Time: 750.76 to 3414.6000000000004)\n",
      "Topic 10: Ground truth expected return, right?  actions that were executed as part of random selection, for example.  expected total return would be of that agent if it took that action in that particular state. So, not necessarily the value of that action, but rather the probability that selecting that action  would be the highest value. We can't simply predict a single probability for every possible action,  and we can compute a probability over any possible action that we may want to take. So, we can see that if these predicted probabilities here, right, in this example of Atari,  came very close to termination, right. So let's decrease the probability of all of those things right, because you've decreased the probabilities at the end, increased the probabilities of the future. \"What is the value that we might expect for any possible action in the second way,  what is the likelihood that I should take any given action to maximize the potential that I have?\" he asks. (Time: 814.84 to 3430.28)\n",
      "Topic 13: The goal here is to estimate the single number output that measures what is the expected. All of the numbers here in the output have to sum to one because this is a probability distribution. If we do that, if we can obtain this function, right, then we can directly sample from that. To parameterize a Gaussian distribution, we only need two outputs, right.  We need a mean and a variance. Given the mean and the variance, we can actually have a probability mass. So for example, in this image here, we may want to output aGaussian that looks like this. (Time: 1005.8000000000001 to 2384.28)\n",
      "Topic 16: MIT has developed a new type of simulation engine for reinforcement learning. Called Vist, it uses a data-driven approach using real data of the real world. This is actually an autonomous agent,  not a real car, driving through our virtual simulator. \"It allows us to basically use real data that we do  collect in the real world, but then re-simulate those same real roads. So for example, let's say you take  into that same simulated environment, observing new viewpoints of what that scene might have looked  no longer in simulation\" (Time: 2899.3199999999997 to 3089.08)\n"
     ]
    }
   ],
   "source": [
    "# Output the overall summary and topic summaries\n",
    "print(\"Overall Summary of the lecture:\", overall_summary)\n",
    "for topic, summary in topic_summaries.items():\n",
    "    print(f\"Topic {topic}: {summary['summary']} (Time: {summary['start_time']} to {summary['end_time']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "query_with_timeframe() missing 1 required positional argument: 'topic_summaries'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example: Ask a query and get answer with timeframe\u001b[39;00m\n\u001b[0;32m      2\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the main topic discussed?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m answer, timeframe \u001b[38;5;241m=\u001b[39m \u001b[43mquery_with_timeframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverall_summary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_summaries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Relevant Timeframe: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeframe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: query_with_timeframe() missing 1 required positional argument: 'topic_summaries'"
     ]
    }
   ],
   "source": [
    "# Example: Ask a query and get answer with timeframe\n",
    "question = \"What is the main topic discussed?\"\n",
    "answer, timeframe = query_with_timeframe(question, overall_summary, topic_summaries)\n",
    "print(f\"Answer: {answer}, Relevant Timeframe: {timeframe}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
