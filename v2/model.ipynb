{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import whisper\n",
    "from transformers import pipeline\n",
    "import yt_dlp as youtube_dl\n",
    "from bertopic import BERTopic\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the audio using yt-dlp\n",
    "def download_audio(video_link, output_audio_path):\n",
    "    ydl_opts = {\n",
    "        \"format\": \"bestaudio/best\",\n",
    "        \"postprocessors\": [{\"key\": \"FFmpegExtractAudio\", \"preferredcodec\": \"wav\", \"preferredquality\": \"192\"}],\n",
    "        \"postprocessor_args\": [\"-ar\", \"16000\"],\n",
    "        \"prefer_ffmpeg\": True,\n",
    "        \"keepvideo\": False,\n",
    "        \"outtmpl\": output_audio_path + \".%(ext)s\",\n",
    "    }\n",
    "    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([video_link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transcribe audio and extract segments using Whisper\n",
    "def transcribe_audio_with_timestamps(audio_path, model_name=\"base\"):\n",
    "    model = whisper.load_model(model_name)\n",
    "    print(\"Transcribing audio...\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    transcription = result[\"text\"]\n",
    "    segments = result[\"segments\"]  # Contains timestamps per segment\n",
    "    return transcription, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization using transformer-based summarization (e.g., BART)\n",
    "def summarize_text_in_chunks(text, chunk_size=1024):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
    "\n",
    "    # Split text into chunks\n",
    "    text_chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    summaries = []\n",
    "\n",
    "    print(\"Summarizing text in chunks...\")\n",
    "    for chunk in tqdm(text_chunks, desc=\"Summarizing\"):\n",
    "        summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0][\"summary_text\"]\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to dynamically segment the transcription into topics using BERTopic\n",
    "def topic_segmentation(segments):\n",
    "    texts = [segment[\"text\"] for segment in segments]\n",
    "    topic_model = BERTopic()  # Initialize BERTopic model\n",
    "\n",
    "    print(\"Segmenting topics...\")\n",
    "    topics, _ = topic_model.fit_transform(texts)\n",
    "\n",
    "    # Group segments by topics with timeframes\n",
    "    topic_segments = {}\n",
    "    for idx, topic in enumerate(topics):\n",
    "        if topic not in topic_segments:\n",
    "            topic_segments[topic] = {\n",
    "                \"text\": [],\n",
    "                \"start_time\": segments[idx][\"start\"],\n",
    "                \"end_time\": segments[idx][\"end\"],\n",
    "            }\n",
    "        topic_segments[topic][\"text\"].append(segments[idx][\"text\"])\n",
    "        topic_segments[topic][\"end_time\"] = segments[idx][\"end\"]  # Update the end time\n",
    "\n",
    "    return topic_segments, topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries for each segmented topic\n",
    "def summarize_topics(topic_segments, chunk_size=1024):\n",
    "    topic_summaries = {}\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
    "\n",
    "    print(\"Summarizing topics...\")\n",
    "    for topic, data in tqdm(topic_segments.items(), desc=\"Summarizing each topic\"):\n",
    "        full_text = \" \".join(data[\"text\"])  # Combine all texts for this topic\n",
    "\n",
    "        # Split the full text into chunks to handle large inputs\n",
    "        text_chunks = [full_text[i : i + chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
    "        summaries = []\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0][\"summary_text\"]\n",
    "            summaries.append(summary)\n",
    "\n",
    "        # Combine chunk summaries and store them\n",
    "        topic_summaries[topic] = {\n",
    "            \"summary\": \" \".join(summaries),\n",
    "            \"start_time\": data[\"start_time\"],\n",
    "            \"end_time\": data[\"end_time\"],\n",
    "            \"full_text\": full_text,  # Added for RAG retrieval\n",
    "        }\n",
    "\n",
    "    return topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to process video, transcribe, segment by topic, summarize, and enable Q&A with ChatGPT\n",
    "def process_lecture_video(video_link, audio_output_path):\n",
    "    # Step 1: Download the audio from the video\n",
    "    print(\"STEP 1: Downloading audio...\")\n",
    "    download_audio(video_link, audio_output_path)\n",
    "\n",
    "    # Step 2: Transcribe the audio and get timestamps for each segment\n",
    "    print(\"\\nSTEP 2: Transcribing audio...\")\n",
    "    transcription, segments = transcribe_audio_with_timestamps(audio_output_path + \".wav\")\n",
    "\n",
    "    # Step 3: Generate overall summary of the lecture\n",
    "    print(\"\\nSTEP 3: Generating overall summary...\")\n",
    "    overall_summary = summarize_text_in_chunks(transcription, chunk_size=1024)\n",
    "\n",
    "    # Step 4: Segment the transcription by topics dynamically\n",
    "    print(\"\\nSTEP 4: Segmenting transcription into topics...\")\n",
    "    topic_segments, topic_model = topic_segmentation(segments)\n",
    "\n",
    "    # Step 5: Summarize each segmented topic\n",
    "    print(\"\\nSTEP 5: Summarizing each topic...\")\n",
    "    topic_summaries = summarize_topics(topic_segments)\n",
    "\n",
    "    return overall_summary, topic_summaries, topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API Key for ChatGPT\n",
    "client = OpenAI(\n",
    "    api_key=\"OPENAI_API_KEY\",\n",
    "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),  # Ensure your API key is set in the environment variable\n",
    ")\n",
    "\n",
    "\n",
    "# Function to create embeddings for segments\n",
    "def create_embeddings(topic_summaries):\n",
    "    print(\"Creating embeddings for topic summaries...\")\n",
    "    embeddings = {}\n",
    "    for topic, data in tqdm(topic_summaries.items(), desc=\"Embedding topics\"):\n",
    "        response = client.embeddings.create(\n",
    "            input=data[\"full_text\"],\n",
    "            model=\"text-embedding-ada-002\",\n",
    "        )\n",
    "        embeddings[topic] = {\n",
    "            \"embedding\": response[\"data\"][0][\"embedding\"],\n",
    "            \"start_time\": data[\"start_time\"],\n",
    "            \"end_time\": data[\"end_time\"],\n",
    "            \"text\": data[\"full_text\"],\n",
    "            \"summary\": data[\"summary\"],\n",
    "        }\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Function to handle queries using RAG\n",
    "def query_with_rag(question, embeddings):\n",
    "    print(\"Processing query with RAG...\")\n",
    "    # Create embedding for the question\n",
    "    question_embedding_response = client.embeddings.create(\n",
    "        input=question,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "    )\n",
    "    question_embedding = question_embedding_response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    # Compute similarities\n",
    "    similarities = []\n",
    "    for topic, data in embeddings.items():\n",
    "        sim = cosine_similarity(\n",
    "            [question_embedding],\n",
    "            [data[\"embedding\"]],\n",
    "        )[\n",
    "            0\n",
    "        ][0]\n",
    "        similarities.append((sim, topic))\n",
    "\n",
    "    # Get the most relevant topics\n",
    "    similarities.sort(reverse=True)\n",
    "    top_topics = [topic for _, topic in similarities[:3]]  # Get top 3 relevant topics\n",
    "\n",
    "    # Combine the texts of the most relevant topics\n",
    "    context = \" \".join([embeddings[topic][\"text\"] for topic in top_topics])\n",
    "\n",
    "    # Use the context to answer the question\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\"},\n",
    "        ],\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    answer = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "    # Get relevant timeframes\n",
    "    relevant_timeframes = [(embeddings[topic][\"start_time\"], embeddings[topic][\"end_time\"]) for topic in top_topics]\n",
    "\n",
    "    return answer, relevant_timeframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "video_link = \"https://www.youtube.com/watch?v=AhyznRSDjw8\"\n",
    "audio_output_path = \"downloaded_audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process the lecture video to get summaries and topics\n",
    "# overall_summary, topic_summaries, topic_model = process_lecture_video(video_link, audio_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1:....\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=AhyznRSDjw8\n",
      "[youtube] AhyznRSDjw8: Downloading webpage\n",
      "[youtube] AhyznRSDjw8: Downloading ios player API JSON\n",
      "[youtube] AhyznRSDjw8: Downloading web creator player API JSON\n",
      "[youtube] AhyznRSDjw8: Downloading m3u8 information\n",
      "[info] AhyznRSDjw8: Downloading 1 format(s): 251\n",
      "[download] Destination: downloaded_audio.webm\n",
      "[download] 100% of   45.54MiB in 00:00:06 at 7.10MiB/s     \n",
      "[ExtractAudio] Destination: downloaded_audio.wav\n",
      "Deleting original file downloaded_audio.webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download the audio from the video\n",
    "print(\"STEP 1:....\")\n",
    "download_audio(video_link, audio_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2:....\n",
      "Transcribing audio...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Transcribe the audio and get timestamps for each segment\n",
    "print(\"STEP 2:....\")\n",
    "transcription, segments = transcribe_audio_with_timestamps(audio_output_path + \".wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3:....\n",
      "Summarizing text in chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing: 100%|██████████| 54/54 [04:41<00:00,  5.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate overall summary of the lecture\n",
    "print(\"STEP 3:....\")\n",
    "overall_summary = summarize_text_in_chunks(transcription, chunk_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4:....\n",
      "Segmenting topics...\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Segment the transcription by topics dynamically\n",
    "print(\"STEP 4:....\")\n",
    "topic_segments, topic_model = topic_segmentation(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5:....\n",
      "Summarizing topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing each topic:   5%|▌         | 1/20 [00:03<01:09,  3.65s/it]Your max_length is set to 150, but your input_length is only 57. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Summarizing each topic:  20%|██        | 4/20 [00:30<02:05,  7.82s/it]Your max_length is set to 150, but your input_length is only 61. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "Summarizing each topic:  30%|███       | 6/20 [01:51<06:51, 29.38s/it]Your max_length is set to 150, but your input_length is only 93. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n",
      "Summarizing each topic:  35%|███▌      | 7/20 [02:05<05:18, 24.48s/it]Your max_length is set to 150, but your input_length is only 57. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Summarizing each topic:  45%|████▌     | 9/20 [02:21<02:53, 15.75s/it]Your max_length is set to 150, but your input_length is only 58. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Summarizing each topic:  65%|██████▌   | 13/20 [03:08<01:21, 11.64s/it]Your max_length is set to 150, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Summarizing each topic:  70%|███████   | 14/20 [03:33<01:34, 15.71s/it]Your max_length is set to 150, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Summarizing each topic:  75%|███████▌  | 15/20 [03:46<01:13, 14.74s/it]Your max_length is set to 150, but your input_length is only 83. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
      "Summarizing each topic:  80%|████████  | 16/20 [03:53<00:50, 12.55s/it]Your max_length is set to 150, but your input_length is only 140. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n",
      "Summarizing each topic:  90%|█████████ | 18/20 [04:36<00:35, 17.70s/it]Your max_length is set to 150, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Summarizing each topic:  95%|█████████▌| 19/20 [04:43<00:14, 14.48s/it]Your max_length is set to 150, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Summarizing each topic: 100%|██████████| 20/20 [04:50<00:00, 14.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Summarize each segmented topic\n",
    "print(\"STEP 5:....\")\n",
    "topic_summaries = summarize_topics(topic_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Summary of the lecture: Two lectures today are really exciting because they start to move beyond a lot of what we've talked about in the class so far, which is focusing a lot on really static data sets. In today, in this lecture right now, I'm going to start to talk about how we can learn about this very long-standing field of reinforcement learning. In the real world, you have your deep learning model actually deployed together with the data. This is the key motivation of reinforcement learning. You're going to try and learn through reinforcement, making mistakes in your world, and then collecting data on those mistakes to learn how to improve. \"I'm hoping for a 5-0 not to lose any games, but I think the realistic goal would be 4-1 in my favour\" \"I wasn't expecting that good. Everything that he did was proper. It was calculated and it was done well\" We've really covered two different types of learning in this course to date. Supervised learning is in this domain where we're given data in the form of x's, our inputs, and our labels y. Our goal here is to learn a function or a neural network that can learn to predict why. Unsupervised learning is a type of learning that has no notion of labels. In reinforcement learning, we're going to be only given data in the form of black and white sketches. In this lecture, we'll talk about yet another type oflearning algorithms. The goal of reinforcement learning is to build an agent that can learn how to maximize what are called rewards. This is the third component that is specific to reinforcement learning. You want to maximize all of those rewards over many, many time steps in the future. We're going to be focusing exclusively on this third type of learning paradigm. An agent is a being, basically, that can take actions. For example, you can think of an agent as a machine that is, let's say, an autonomous drone. The environment is simply the world where that agent lives and where it operates. An agent can send commands to that environment in the form of what are called actions. It can take actions in that environment and let's say the possible set of all actions that it could take is a set of capital A. This is essentially how the environment responds back to the agent. Reinforcement learning is a bit unique because it has one more component here in addition to these other components, which is called the reward. Now, the reward is a feedback by which we measure or we can try to measure the success of a particular agent in its environment. When we look at the total reward that an agent accumulates over the course of its lifetime, we can simply sum up all of the rewards that an agents gets after a certain time t. Its capital R of t is the sum of all rewards from that point on into the future into infinity. The algorithm uses a discounting factor to make rewards worth less than what they might be in the future. This factor is like multiple, like I said, multiplied it every single future reward exponentially. It's important to understand that also typically this discounting factors is, you know, between zero and one. The Q function takes as input two different things. The first is the state that you're currently in. The second is a possible action that you could execute in this particular state. The Q function of these two pieces is going to denote or. t today. The Q function tells us for any possible action, right? What is the expected reward for that action to be taken? So if we wanted to take a specific action given a specific state, ultimately we ultimately we n The way we do that from a Q function is simply to pick the action that will maximize our future reward. And we can simply pry out number one. If we have a continuous action space, maybe we do something a bit more intelligent. There are two classes of reinforcement learning algorithms that we're going to briefly touch on as part of today's lecture. The first class is what's going to be called value learning. And that's exactly this process that we've just talked about. The second class of algorithms is kind of a different framing of the same approach. First optimizing the Q function and finding the Q value and then using t. We'll build up that intuition and that knowledge onto the second part of policy learning. So maybe let's start by just digging a bit deeper into the Q function specifically, just to start to understand how we could estimate this in the beginning. The objective of the game is to basically eliminate all of those rainbow pixels, so you want to keep hitting that ball against the top of the screen until you remove all the pixels. The Q function tells us, you know, the expected total return, or the total reward, that we can expect based on a given state and action pair that we may find ourselves in. The state is basically that the ball is coming slightly at an angle, we're not quite underneath it yet and we need to move towards it and actually hit that ball in a way that, you know, will make it and not miss it, hopefully, right? So hopefully that ball doesn't pass below us then the game would be over. raight down towards us, that's our state, our action is to do nothing and simply reflect that ball back up vertically up. A is a very conservative action. It's just bouncing up, hitting one point at a time from the top and breaking off very slowly the board that you can see here. With B, with B, you're kind of having agency, like one of the answers said. What that implies is that you're sometimes going to actu The algorithm, the agent, can actually learn that hitting the side of the board can have some kind of unexpected consequences. Here you see it trying to enact that policy. Once it reaches a breakout on the side, it found this hack in the solution. We're now breaking off a ton of points. A neural network takes as input two objects: the state of the board and an action. In this case, the actions that a neural network or an agent could take in this game is to move to the right to the left to stay still. The goal here is to estimate the single number output that measures what is the expected value or the expected Q value of this neural network at this particular state action. That way you only need to run your neural network once for the given state that you're in. And then that neural network will tell you for all possible actions, what's the maximum? The question I want to pose here is really, we want to train one of these two networks. Let's stick with the network on the right for simplicity. And the question is, how do we actually train that network? And specifically, I want all of you to think about really the best case scenario. N initial reward plus selecting some action in our action space that maximizes our expected return, then for the next future state, we need to apply that discounting factor and recursively apply the same equation. Now we can ask basically what does our neural network predict, right? So that's our target and recall from previous lectures, if we have a target value. Deep neural network that we're trying to train looks like this, right? It takes us input a state, is trying to output N different numbers, those N different Numbers correspond to the Q value associated to N different actions, one Q value per action. So we have the ground truth labels to train and supervise this model. The policy function is a function that determines what is the best action. It's a very end-to-end way of thinking about the agent's decision-making process. We can determine that policy function directly from the Q function itself simply by maximizing and optimizing all of the different Q values. The optimal action here is simply going to be the maximum of these three Q values, in this case, it's Going to be 20. Now we can send this action back to the environment in the form of the game to execute the next step, right? And as the agent moves through this environment, it will be responded with new pixels that come from the game, and more importantly, some reward signal. Google DeepMind showed that you could train a Q value network for Atari Breakout games. The network learns from actions that don't do very well, that you discourage them and try to do actions that did perform well more frequently. QLearning is naturally applicable to discrete action spaces, because you can think of this output space that we're providin. Now, there are several very important downsides of QLearning, and hopefully these are going to motivate the second part of today's lecture. QLearning is very well suited for discrete action spaces, and we'll talk about ways of overcoming that with other approaches a bit later. The policy that we're learning, the Q function is giving rise to that policy. That policy is determined by, you know, deterministically optimizing that Q function. Policy learning is a different class of reinforcement learning algorithms that are different than QLearning algorithms. In policy grading algorithms, the main difference is that instead of trying to infer the policy from the Q function, we're just going to build a neural network that will directly learn that policy function from the data. L network, outputs these Q values, one value per action, and we determine the policy by looking over this state of Q values. We pick the value that has the highest and looking at its corresponding action. And that represents not only the best action that we should take, but the probability that selecting that action would result in a very desirable outcome. Going left has the probability of being the highest value action with 90% staying in the center. The probability of 10% going right is 0%. So, ideally, what our neural networks should do in this case is 90% of the time in this situation go to the left. The policy network has a very formulated output. All of the numbers here in the output have to sum to one because this is a probability distribution. And that gives it a very rigorous version of how we can train this model. We could also use this same formulation to move beyond discrete action spaces, like you can see here, which are one possible action. Now, we may have a space, which is not what action should I take, go left, right, or stand center, but rather how quickly should I move? Right? That is a continuous variable, as opposed to a discrete variable. If we want to determine the best action to take, we would simply take the mode of this distribution. That would be the speed at which we should move. If we wanted to also try out different things and explore our space, we could sample from this distribution and still obtain some stochasticity. So instead, what if we parameterized our action space by a distribution, right? So let's take, for example, the Gaussian distribution. To parameterize a Gaussian. distribution, we only need two outputs, right. We need a mean and a variance. The state could be obtained through many sensors that could be mounted on the vehicle itself. The action that we could take is a steering-wheel angle. It's actually an angle that could take any real number. And finally, the reward in this very simplistic example is the distance that we travel before we crash. Reinforcement learning is almost like a data acquisition plus learning pipeline combined together. So we start by initializing our agent, right. And in the beginning, of course, it knows nothing about driving. It's never been exposed to any of these rules of the environment or the observation before. The first step of training that we'll do is to take, excuse me, to take the later half of our trajectory that our agent ran and decrease the probability of actions that resulted in low rewards. Now again, there's no reason why there shouldn't necessarily be a good action that we took in the first half of this trajectory. Re-initialize the agent one more time and run it until completion. Now the agent goes a bit farther, right, because you've decreased the probabilities at the end, increased the probabilities of the future. And you keep repeating this over and over again until you notice that the agent learns to perform better and better every time until it finally converges. The loss function for a policy gradient neural network looks like this, and then we'll start by dissecting it to understand why this works the way it does. So here we can see that the loss consists of two terms. The first term is this term in green, which is called the log likelihood. The second term is something that all of you are very familiar with already. This is simply the return at a specific time, right, so that's the expected return on the words that you would get after this time point. If we got a lot of reward for a particular action that had high probability, that means that we want to increase that probability even further. On the other hand, if we obtained a reward that was very low for an action that has high likelihood, we want the inverse effect. We never want to sample that action again in the future because it resulted in a low reward. In our simplified example on the the car example, all the things that had low rewards were exactly those actions that came closest to the termination part of the of the vehicle. Now we can plug this into the loss of gradient descent algorithm to train our neural network when we see, you know, this policy gradient algorithm.  ethod gets its name from this policy gradient piece that you can see here. If you wanted to do this in reality, right, that essentially means that you want to go out, collect your car, crashing it a bunch of times just to learn how to not crash it. And that's, you know, that's simply not feasible. When we're training reinforcement learning algorithms, we ultimately want them to be, you know, not operating in simulation. Modern simulation engines for reinforcement learning and generally, very broadly speaking, modern simulators for vision specifically do not at all capture reality very accurately. MIT lab develops new photorealistic simulation engine. Called Vista, it uses real data from the real world to re-sim environments. This is actually an autonomous agent, not a real car, driving through our virtual simulator. The technology allows you to train these agents entirely using reinforcement learning, no human labels, but importantly, allow them to be transferred into reality because there's no sim-to-real gap anymore. So for example, let's say you take your car, you drive out on Massab, you collect data of Massab. You can now drop a virtual agent into that same simulated environment, observing new viewpoints of what that scene might have looked like. This is the first time ever that reinforcement learning was used to train a policy end-to-end for an autonomous vehicle that could be deployed in reality. This environment completely autonomous in the real world. No transfer learning is done here. There is no augmentation of data from real world data. This is entirely trained using simulation. The objective here was to build a reinforcement learning algorithm to master the game of Go. The number of possible action spaces and possible states that this board could be placed into is greater than the number of atoms in the unit. Google DeepMind rose to this challenge. Using a reinforcement learning pipeline, they were able to defeat the grand champion human players. And the idea that's core was actually very simple. The first step is that you train a neural network to basically watch human level experts. Auxiliary network was almost hallucinating, right? So it would take its human understandings, try to imitate the humans first of all. From that imitation, they would pin these two neural networks against themselves, play a game against themselves. A neural network can learn to outperform the solution that was created by humans. It can also outperform a solution created, which was bootstrapped by humans as well. We saw two difinitely different board states that it could take from this particular state. The deadline for these competitions will be, well, it was originally set to be Thursday, which is tomorrow at 11 p.m. ferent types of reinforcement learning approaches of how we could optimize these solutions. First being Q learning where we're trying to actually estimate, given a state, you know, what is the value that we might expect for any possible action in the second way, was to take a much more end-to-end approach.\n",
      "Topic 3: CNN.com's John Sutter takes a look back at the first half of the show. We'll take a look at what happened in the second half and see what happens in the third half. (Time: 0.0 to 2564.68)\n",
      "Topic 6: Two lectures today are really exciting because they start to  move beyond a lot of what we've talked about in the class so far. And specifically in today, in this lecture right now, I'm going to start to talk about how  already started learning about as part of this course. Today's lecture is going to be talking about primarily how we can obtain this. We'll talk more details about that later in the lecture. In fact, we've seen this in the last two lectures, right?  And actually, we're going to briefly touch on as part of today's lecture. The first lecture was originally set to be Thursday, which is tomorrow at 11 p.m. The first step is to use the techniques that we covered in lectures one, two, and three. Thank you. (Time: 11.200000000000001 to 3451.48)\n",
      "Topic 17: A lot on really static data sets.  We collect, we go out and collect that data set, we deploy it on our machine learning  and our labels y. There's no notion of labels. So actually, the question I want to pose here is really, we want to train one of these. (Time: 21.04 to 3274.92)\n",
      "Topic 11: The process is very different than the way things work in the real world. And for each of these tasks, the really fascinating thing that they showed was for this very simple  And you could say that now the answer should look like this, right?  you can see basical. The deadline for these competitions will be, and the deadline for the competition is December 1. We'll be posting the winners on CNN.com and CNN. ly. The winner will be announced on December 2. (Time: 27.68 to 3441.88)\n",
      "Topic 9: The goal of reinforcement learning is to build an agent that can learn how to maximize. This is the third component that is specific to reinforcement learning. In reinforcement learning, we're going to be only given data in the form of what are. Reinforcement learning is almost like a data acquisition plus learning pipeline combined together. And that allows us to train these agents now entirely using reinforcement learning, no human. This represented actually the first time ever that reinforcement learning was used to train  of reinforcement learning and policy learning. This is using supervised learning,  bootstrap, in reinforcement learning algorithm, that would play against itself in order to learn  the day. We saw two different types of reinforcement learning approaches of how we could learn. ing. (Time: 32.8 to 3408.92)\n",
      "Topic -1: The marriage of these two fields is actually really fascinating to me, particularly  but that data set is typically fixed in our world, right?  or deep learning algorithm, and then we can evaluate on a brand new data set. Now this is obviously a huge field or a huge topic in the field of robotics and autonomy. Supervised learning is in this domain where we're given data in the form of x's, our inputs,  given our inputs x. What we were calling basically these latent variables, these hidden features in your data. For example, in this apple example, using unsupervised learning, the analogous example would basically have the same semantic meaning as this black and white outline sketch. We're going to talk about yet another type of learning algorithms.  This is what the agent, the neural network is going to observe.  It's what it sees.  Again, in this apple example, we might now see that the agent doesn't necessarily learn  starve. The first class is what's going to be called value learning. This is how to understand how to what actions to take in a particular  what action should be taken. For example, in a video game, when Mario grabs a coin, for example, he wins points. Atari Breakout is a game where players try to break off points by hitting the side of the screen. The game was created by Atari in the 1980s. It's now called the Atari Breakout game. In this case, the actions that a neural network or an agent could take in this game is to move. Those could be three different actions that could be provided and parameterized to the player. We minimize their distance over two, over many, many different iterations of flying our neural network in this environment. After the action is committed or executed, we can see exactly the  ground truth expected return, right? DeepMind's algorithm relies on random choice of selection of actions, and then learning from actions that don't do very well. As we'll see, this was really such an exciting advance because of the simplicity of data. In the last lecture, we also saw how we could learn to predict uncertainties,  over the entire real number line, first of all. So if we want to determine the best action to take, we would simply take the mode of this. If we wanted to also try out different things and explore our space,  so instead of predicting this probability of taking an action, giving all possible states, which in this case, there's now an infinite number of. We start our vehicle, our agent. And in the beginning, of course,  it knows nothing about driving. It's never been exposed to any of these rules of the environment. Now, because the vehicle, we know the vehicle terminated, we can assume that all of the actions  that occurred in the later half of this trajectory were probably not very good actions. In our simplified example on the the car   this was a car that learned entirely just by going out, crashing a lot, and trying to figure out  what to do to not keep doing that in the future. And the remaining question is actually how we can  likelihoods of all these good events. MIT researchers have developed a way to train algorithms in simulation and they don't  in reality. They say the approach is safe because they're not going to actually be damaging anything real. It's still  of times just learn how not to crash, but at least now, at least from a safety point of view,  at all capture reality very accurately. Google DeepMind has created an autonomous car that can drive itself. The technology is based on the game of Go, which is played on a 19 by 19 board. Google DeepMind created a couple of virtual agents to test the technology. The idea that's core was actually  how to improve even beyond the human levels. So it would take its human understandings, try to  human counterparts and try to actually learn new types of rules. And finally,  human supervision at all, have a neural network learn to not only outperform the solution that,  by humans as well. (Time: 38.16 to 3399.1600000000003)\n",
      "Topic 5: In the real world, you have your deep learning model actually deployed together with the neural network. Our goal here is to learn a function or a neural network that can learn to predict why  the network, which one would result in a greater reward. Our deep neural network that we're trying to train looks like this, right?  You only needed a very little amount of prior knowledge to impose onto this neural network. So, ideally, what our neural networks should do in this case is 90% of the time in this situation. Auxiliary network took as input the state of the board and tried to predict, you know,  network was an auxiliary network that was almost hallucinating, right? Different board states that  started entirely from scratch and just had two neural networks never trained before, they  started pinning themselves against each other, and you could actually see that you could, without any help. (Time: 61.2 to 3376.36)\n",
      "Topic 14: DeepBind is a computer game where the player controls an agent. The agent is able to move left or right, this  screen, all the way to the actions of a controller on the right-hand side. DeepBind allows you to train  of games. The rules are very simple in their logical definitions,  result was to beat the grand master level players. So the number one player in the world of Go would take based on a given board state, the type of actions. (Time: 136.72 to 3269.4)\n",
      "Topic 10: The goal here is to estimate the single number output that measures what is the expected. All of the numbers here in the output have to sum to one because this is a probability distribution. And remember, those distributions can also take continuous forms. There's an infinite number of continuous distributions. To parameterize a Gaussian distribution, we only need two outputs, right. We need a mean and a variance. Given the mean and the variance, we can actually have a probability mass. (Time: 169.60000000000002 to 2384.28)\n",
      "Topic 1: The objective of the game is to basically eliminate all of those rainbow pixels, so you want to keep hitting  that ball against the top of the screen until you remove all the pixels. I'm hoping for a 5-0 not to lose any games, but I think the realistic goal would be  It's not necessarily just going right or left or straight. The board is just bouncing up, hitting one point at a time from the top and breaking off very slowly. It's targeting the sides of the board. Once it reaches a breakout on, then B can bounce around and there's more than just up and down. The state of the board is simply the pixels that are on the screen describing that board. You can see that the ball is  coming to the left of it, it moves to the right, the game is over, right? So it needs to move to theleft in order to do that, and  action that comes from that 20, which is moving left. The input on the left-hand side is simply the raw pixels coming from the  going left has the probability of being the highest value action with 90% staying in the center. That would be the speed at which we should move and the direction that we should moving in. The 19 by 19 grid is the size of the number of atoms in the unit. The board could be placed into any of the 19 different shapes. The game itself states that this board can be placed in any of these shapes. (Time: 172.56 to 3180.12)\n",
      "Topic 18: A model could understand and cluster certain parts of these images together. For example, if you consider this example of an apple, observing a bunch of images of  apples. We want to detect in the future if we see a new image of an Apple to detect that this is indeed an apple. (Time: 268.64 to 1341.84)\n",
      "Topic 12: The algorithm itself, it's important to remember that the algorithm is the agent.  agreediness in the algorithm, right? The second class of algorithms, which we'll touch on right at the end of today's that did perform well more frequently, very simple algorithm. We ultimately want them to be, you know, not operating in simulation. We want them  applications that we're seeing. And one very popular application that a lot of people will tell  is an extraordinarily complex game for an artificial algorithm to try and master. (Time: 289.0 to 3390.76)\n",
      "Topic 7: A state is simply a concrete and immediate situation that the agent finds itself in at  time T. In a given state, an agent can send out any form of actions to take some decisions. States are observations. The actions are the behaviors that this agent takes in those particular states. Today we have a new tool that lets you see what the ideal action would be in any given state. The tool is called the board state tool, and it shows you the possible board states that could emerge from a state. (Time: 347.96 to 3435.6400000000003)\n",
      "Topic 2: The reward is a feedback by which we measure or we can try to measure the success of an action. It's also very important to remember that not all actions result in immediate rewards. You want to maximize all of those rewards over many, many time steps in the future. The discounting factor is essentially multiplied by every future reward that the agent sees. Rewards essentially worth less than rewards that we might see at this instant, at this moment. So for example, if I offered you a reward of $5 today or a reward in 10 years from  now, I think all of you would prefer that $5 now. R of T is discounted number one and number two, we're  take that will maximize this reward. What is the expected reward for that action to be taken?  future reward. And finally, the reward in this very simplistic example is the distance that we travel before we crash. It's possible that these are noisy rewards as well, because  it's such a sparse signal. It's very possible that you had some good actions at the end, and you  words that you would get after this time point. Now let's assume that we got a lot of reward for a  reward. That means that we want to increase  again into the future. The losers would try to negate all of the actions that they may have acquired from their  that might be very beneficial to achieving superhuman performance. Now we can plug this into the  at the time, achieved incredibly impressive results. So for those of you who are not reward. (Time: 370.24 to 3314.5200000000004)\n",
      "Topic 8: An agent is a being, basically, that can take actions. The environment is simply the world where that agent lives and where it operates. The agent can send commands to that environment in the form of what are called actions. The game is a very end-to-end way of thinking about the agent's decision-making process. As the agent moves through this environment, it's going to be responded with not only  this class with.  How an agent would perform ideally in a particular situation or what would happen if an agent  did something. . g car that we never taught ourselves how to drive. We learned to drive a car in a way that we didn't know how to before. We also learned how to control the car with a steering wheel and a clutch. (Time: 427.03999999999996 to 2640.44)\n",
      "Topic 13: In some situations, your action space does not necessarily need to be a finite. Maybe you could take actions in a continuous space. And just like that, we could also use this same formulation to move beyond discrete action  spaces, like yo. The logical rules are actually quite simple, the number of possible action spaces and possible actions. The probability associated to one  possible action in a discrete set of possible actions has most of its mass right in the optimal action space. (Time: 503.88 to 3174.92)\n",
      "Topic 4: Policy learning is the study of how to use data to make better decisions. In policy networks, the idea that we want to keep her safe is a key goal. The next part of today's lecture will be focused on policy learning. A policy gradient neural network can be trained to solve an observation. It runs its policy, which right now is untrained entirely, until it terminates. Then it update that policy as part of this algorithm that I'm showing you. The goal is to build like a policy that would imitate some of the rough patterns that a human would make. The method gets its name from this policy gradient piece that you can see here. Using the exact algorithms that you learned about in today's lecture, these policy gradient  a policy end-to-end for an autonomous vehicle could be deployed in reality. (Time: 688.16 to 3264.44)\n",
      "Topic 0: The Q function takes as input two different things,  at time T, and the Q function of these two pieces is going to denote or capture what the  a really powerful function, right?  If you had access to this type of function, this Q function, I think you could actually use it to infer the highest Q values. The Q function tells us, you know, the expected total return. First of all, how can we construct and learn that Q function from data? And then, of course, the final step is use that Qfunction to take some actions in the code. If we have the Q function, we can directly use it to determine  this Q function?  value or the expected Q value of this neural network at this particular state action. Now for example, we want to formulate a loss function that's going to essentially case our Q value is a continuous variable. The Q function tells us given a state, what is the best, or the value, the return. We can determine that policy function directly from the Q function itself. Google DeepMind several years ago, showed that you could train a Q value network. The policy that we're learning, the Q function is giving  That policy is determined by, you know, deterministically optimizing that Q function. We simply look at the results from the Qfunction and apply our policy. We pick the action that has the best or the highest Q value, and we're just going to build a neural network that will learn. The loss consists of two terms. The first term is this term in green, which is called the log likelihood right? And you'll notice that this loss function right here by including this negative,  optimize these solutions. First being Q learning where we're trying to actually estimate, given a state, (Time: 750.76 to 3414.6000000000004)\n",
      "Topic 15: The probability that selecting that action would result in a very desirable outcome. So, not necessarily the value of that action, but rather the probability that selected that action. You don't care exactly about what is the numerical value that selecting this action takes. \"What is the value that we might expect for any possible action?\" he asks. \"And what is the likelihood that I should take any given action to maximize the potential that I have?\" (Time: 1562.92 to 3430.28)\n",
      "Topic 16: MIT has developed a new type of simulation engine for reinforcement learning. Called Vista, it uses real data from the real world to create synthetic environments. MIT is also developing an autonomous car that can drive through a virtual simulator. The car is driving through a simulation of a real road. On the left-hand side, you can see basically this car driving through the road. The car is travelling at speeds of up to 100km/h. (Time: 2899.3199999999997 to 3089.08)\n"
     ]
    }
   ],
   "source": [
    "# Output the overall summary and topic summaries\n",
    "print(\"Overall Summary of the lecture:\", overall_summary)\n",
    "for topic, summary in topic_summaries.items():\n",
    "    print(f\"Topic {topic}: {summary['summary']} (Time: {summary['start_time']} to {summary['end_time']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for topic summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding topics:   0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: OPENAI_A**_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create embeddings for RAG\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_summaries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Example: Ask a query and get answer with timeframe\u001b[39;00m\n\u001b[0;32m      5\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is reward function?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[56], line 13\u001b[0m, in \u001b[0;36mcreate_embeddings\u001b[1;34m(topic_summaries)\u001b[0m\n\u001b[0;32m     11\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic, data \u001b[38;5;129;01min\u001b[39;00m tqdm(topic_summaries\u001b[38;5;241m.\u001b[39mitems(), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding topics\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-ada-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     embeddings[topic] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     23\u001b[0m     }\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mc:\\Users\\farha\\miniconda3\\Lib\\site-packages\\openai\\resources\\embeddings.py:124\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    118\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    119\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\farha\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1276\u001b[0m     )\n\u001b[1;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\farha\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\farha\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1067\u001b[0m )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: OPENAI_A**_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# Create embeddings for RAG\n",
    "embeddings = create_embeddings(topic_summaries)\n",
    "    \n",
    "# Example: Ask a query and get answer with timeframe\n",
    "question = \"What is reward function?\"\n",
    "answer, timeframe = query_with_rag(question, embeddings)\n",
    "print(f\"Answer: {answer}, Relevant Timeframe: {timeframe}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
