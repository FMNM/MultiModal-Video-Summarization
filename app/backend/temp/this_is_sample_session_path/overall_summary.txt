The first topic is reinforcement learning, which has existed for many, many decades together with a lot of the very recent advances in deep learning. In the real world, you have your deep learning model actually deployed together with the data, together out into reality, exploring, interacting with its environment. And typically, we want to be able to do this without explicit human supervision, right? This is the key motivation of reinforcement learning.
We've started seeing incredible advances of deeper reinforcement learning specifically also on the side of gameplay and strategy-making. Now training robots to play against us in the real world, and I'll just play this very short video on Starcraft and DeepBind. It also requires long-term planning and the ability to choose what action to take from millions and millions of possibilities. I'm hoping for a 5-0 not to lose any games, but I think the realistic goal would be 4-1 in my favour.
The second class of learning approaches that we've discovered yesterday in yesterday's lecture was that of unsupervised learning. These algorithms, you have only access to the data. There's no notion of labels. This is what we learned about yesterday. In today's lecture, we're going to talk about yet another type of learning algorithms. In reinforcement learning, we are going to be only given data in the form of what are called state action pairs.
In today's lecture, we're going to be focusing exclusively on this third type of learning paradigm, which is reinforcement learning. An agent is a being, basically, that can take actions. For example, you can think of an agent as a machine that is, let's say, an autonomous drone that is making a delivery or, for example, in a game, it could be Super Mario that's navigating inside of your video game.
The environment can tell the agent what it should be seeing based on those actions that it just took. It responds in the form of what is called a state. A state is simply a concrete and immediate situation that the agent finds itself in. From a given state, an agent can send out any form of actions to take some decisions. Those actions may or may not result in rewards being collected and accumulated over time.
This dampening factor is designed to make future rewards essentially worth less than rewards that we might see at this instant, right at this moment right now. This discounting factor is like multiple, like I said, multiplied it every single future reward exponentially. Now finally, it's very important in reinforcement learning, this special function called the Q function, which ties in a lot of these different components.
The Q function takes as input two different things. The first is the state that you're currently in. And the second is a possible action that you could execute in this particular state. The Q function of these two pieces is going to denote or capture what the expected total return would be of that agent if it took that action in that state. If you had access to this type of function, this Q function, I think you could actually perform a lot of tasks right off the bat.
In this lecture, what I want to focus on is actually how we can obtain this Q function to start with, right? And that's what today's lecture is going to be talking about primarily. First of all, how can we construct and learn that Q function from data? And then, of course, the final step is use that Qfunction to take some actions in the real world.
The Q function tells us, you know, the expected total return, or the total reward, that we can expect based on a given state and action pair that we may find ourselves in this game. Sometimes, even for us, as humans, to understand what the Q value should be is sometimes quite unintuitive, right? So here's one example. Let's say we find these two state action pairs. Here is A and B. That's our state. Our action is to do nothing and simply reflect that ball back up vertically up.
A is a very conservative action. You're kind of only going up and down. It will achieve a good reward. In fact, it solves the game exactly like this right here. B is kind of having agency, like one of the answers said. It's targeting the sides of the board. And it turns out that the algorithm, the agent, can actually learn that hitting the side of theBoard can have some kind of unexpected consequences.
The Q function can be used to determine what is the best action that we can take in any given state that we find ourselves in. So now the question naturally is, how can we train a neural network that can, indeed, learn this Q function? So the type of the neural network here naturally, because we have a function that takes us input two things, let's imagine our neural network will also take as input these two objects as well.
If you wanted to evaluate, let's suppose, a very large action space, it's going to be very inefficient to try the approach on the left. One time for every single element of your action space. So what if instead you only provided an input of your state. And as output, you gave it,let's say, all n different q values. That way you only need to run your neural network once for the given state that you're in.
Our deep neural network that we're trying to train looks like this, right. It takes us input a state is trying to output and different numbers, those end different numbers correspond to the q value associated to end different actions, one q value per action. We minimize their distance over two over many, many different iterations of flying our neural network in this environment, observing actions and observing not only the actions, but most importantly, after the action is committed or executed, we can see exactly the ground truth expected return.
The policy function is a function that given a state, it determines what is the best action. It's a very end to end way of thinking about, you know, the agents decision making process based on what I see right now. And we can determine that policy function directly from the q function itself simply by maximizing and optimizing all of the different actions that we see here. So for example, here we can see that given this state, theQ function has the results of these three different values.
Google DeepMind showed that you could train a q value network to play Atari games. They were able to surpass human level performance on over half of the games. And how you know, clean the formulation of the training was you only needed a very little amount of prior knowledge to impose on to this neural network. You never had to teach you any of the rules of the game, right? You only had to let it explore its environment play the game.
QLearning is naturally applicable to discrete action spaces, right? Because you can think of this output space that we're providing is kind of like one number per action that could be taken. Now, if we have a continuous action space, we have to think about clever ways to work around that. That is very dangerous in many cases because of the fact that it's always going to pick the best value for a given state.
We're going to build a neural network that will directly learn that policy function from the data, right. The main difference is that instead of trying to infer the policy from the Q function, we're just going to building a neuralNetwork. We're calling the policy function pi of s, right, so pi is the policy s is our state. So the outputs here give us the desired action that we should take in any given state that we find ourselves in.
The policy network has a very formulated output all of the numbers here in the output have to sum to one because this is a probability distribution, right. That gives it a very rigorous version of how we can train this model that makes it a bit easier to train than q functions as well. In fact, we've seen this in the last two lectures, we saw how VIEs could be used to predict Gaussian distributions over their latent space.
In the continuous domain, we can't simply predict a single probability for every possible action, because there is an infinite number of them. So instead, what if we parameterized our action space by distribution, right, so let's take for example the Gaussian distribution. We can actually have a probability mass and we can compute a probability over any possible action that we may want to take just from those two numbers.
Reinforcement learning is almost like a data acquisition plus learning pipeline combined together. So for example, in this image here, we may want to output a Gaussian that looks like this, right, its mean is centered at, let's see, negative 0.8. And again, we can see that because this is a probability distribution, because of the format of policy networks, right. So now let's take a look at how we could train a policy gradient neural network to solve this task.
We start our vehicle, our agent, and in the beginning, of course, it knows nothing about driving. It's never been exposed to any of these rules of the environment or the observation before. So it runs its policy, which right now is untrained entirely until it terminates. We measure basically the reward as the distance that we traveled before it terminated. This becomes our mini data set that we'll use for the first round of training.
The loss function for a policy gradient neural network looks like this. So that's the expected return on the words that you would get after this time point. Now, repeat this process again. Re-initialize the agent one more time and run it until completion. Now the agent goes a bit farther, right? Because you've decreased the probabilities at the end, increased the probabilities of the future. And you keep repeating this over and over again until you notice that the agent learns to perform better and better.
This method gets its name from this policy gradient piece that you can see here. This gradient is exactly of the policy part of the neural network. That's the probability of selecting an action given a specific state. Now, we can plug this into the loss of gradient descent algorithm to train our neural network when we see thispolicy gradient algorithm. Right, and you'll notice that this loss function right here by including this negative, we're going to minimize the likelihood of achieving any action that had low rewards.
When we're training reinforcement learning algorithms, we ultimately want them to be, you know, not operating in simulation. And we want to be in reality. The number one way around this is that people try to train these types of models in simulation, right? Simulation is very safe because we're not going to actually be damaging anything real. It's still very inefficient. But at least now, at least from a safety point of view, it's much safer.
MIT has developed a new photorealistic simulation engine. Called Vista, it allows us to use real data that we do collect in the real world, but then re-simulate those same real roads. This is actually an autonomous agent, not a real car, driving through our virtual simulator in a bunch of different types of different scenarios. It's a very, very cool result that we created here at MIT.
MIT researchers used reinforcement learning to train a policy end to end for an autonomous vehicle that could be deployed in reality. On the left hand side, you can see basically this car driving through this environment completely autonomous in the real world. No transfer learning is done here. There is no augmentation of data from real world data. This is entirely trained using simulation and this represented actually the first time ever that reinforcement learning was used to train an autonomous car.
Google developed a way to defeat the grand champion human players. The idea that's core was actually very simple. The first step is that you train a neural network to basically watch human level experts. The losers would try to negate all of the actions that they may have acquired from their human counterparts and try to actually learn new types of rules and new type of actions that might be very beneficial to achieving super human performance.
The deadline for these competitions will be well it was originally set to be Thursday, which is tomorrow at 11 p.m. ET. We saw two different types of reinforcement learning approaches of how we could optimize these solutions first being Q learning where we're trying to actually estimate given a state, you know, what is the value that we might expect for any possible action. The second way was to take a much more end to end approach and say how given a. state that we see ourselves in what's the likelihood that I should take any given action to maximize. the potential that I have in this particular state.