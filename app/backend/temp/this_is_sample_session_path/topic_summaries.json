{
  "Robotics and Reinforcement Learning": {
    "summary": "Lectures focus on how we can specifically marry two topics. The first topic is reinforcement learning, which has existed for many, many  decades together with a lot of the very recent advances in deep learning. This marriage of these two fields is really fascinating to me. In the real world, you have your deep learning model deployed together with the data, together out into reality, exploring, interacting with its environment. This is the key motivation of reinforcement learning. You're going to try and learn through reinforcement, making mistakes in your world, and then  collecting data on those mistakes to learn how to improve. \"We've started seeing incredible advances of deeper reinforcement learning specifically also on  the side of gameplay and strategy-making as well. So one really cool thing is that now you can even imagine this combination of robotics and cars,\" he said.",
    "start_time": 0.0,
    "end_time": 149.12,
    "full_text": " Hi everyone, welcome back.  Today I think that these two lectures today are really exciting because they start to  move beyond a lot of what we've talked about in the class so far, which is focusing a  lot on really static data sets.  And specifically in today, in this lecture right now, I'm going to start to talk about how  we can learn about this very long-standing field of how we can specifically marry two  topics, the first topic being reinforcement learning, which has existed for many, many  decades together with a lot of the very recent advances in deep learning, which you've  already started learning about as part of this course.  Now this marriage of these two fields is actually really fascinating to me, particularly  because like I said, it moves away from this whole paradigm of, or really this whole  paradigm that we've been exposed to in the class thus far.  And that paradigm is really how we can build a deep learning model using some data set,  but that data set is typically fixed in our world, right?  We collect, we go out and collect that data set, we deploy it on our machine learning  or deep learning algorithm, and then we can evaluate on a brand new data set.  And that is very different than the way things work in the real world.  In the real world, you have your deep learning model actually deployed together with the  data, together out into reality, exploring, interacting with its environment, and trying  out a whole bunch of different actions and different things in that environment in order  to be able to learn how to best perform any particular task that it may need to accomplish.  And typically, we want to be able to do this without explicit human supervision, right?  This is the key motivation of reinforcement learning.  You're going to try and learn through reinforcement, making mistakes in your world, and then  collecting data on those mistakes to learn how to improve.  Now this is obviously a huge field or a huge topic in the field of robotics and autonomy.  You can think of self-driving cars and robot manipulation, but also very recently we've  started seeing incredible advances of deeper reinforcement learning specifically also on  the side of gameplay and strategy-making as well.  So one really cool thing is that now you can even imagine this combination of robotics"
  },
  "Reinforcement Learning": {
    "summary": "In today's lecture, we're going to be focusing exclusively on this third type of learning  paradigm, which is reinforcement learning. I just want to start by building up some very key terminology and basically background for all of you so that we're all on the same page. The environment is simply the world where that agent lives and where it operates. The agent can send commands to that environment in the form of what are called actions. It should be noted that agents at any point in time could choose amongst this list of possible actions.",
    "start_time": 381.44,
    "end_time": 511.32,
    "full_text": " Again, in this apple example, we might now see that the agent doesn't necessarily learn  that this is an apple or it looks like these other apples.  Now it has to learn to, let's say, eat the apple, take an action, eat that apple because  it has learned that eating that apple makes it live longer or survive because it doesn't  starve.  In today, like I said, we're going to be focusing exclusively on this third type of learning  paradigm, which is reinforcement learning.  Before we go any further, I just want to start by building up some very key terminology  and basically background for all of you so that we're all on the same page when we  start discussing some of the more complex components of today's lecture.  Let's start by building up some of this terminology.  The first main piece of terminology is that of an agent.  An agent is a being, basically, that can take actions.  For example, you can think of an agent as a machine that is, let's say, an autonomous  drone that is making a delivery or, for example, in a game, it could be Super Mario that's  navigating inside of your video game.  The algorithm itself, it's important to remember that the algorithm is the agent.  We're trying to build an agent that can do these tasks and the algorithm is that agent.  In life, for example, all of you are agents in life.  The environment is the other kind of contrary approach or the contrary perspective to the  agent.  The environment is simply the world where that agent lives and where it operates.  Where it exists and it moves around in.  The agent can send commands to that environment in the form of what are called actions.  It can take actions in that environment and let's call for notation purposes.  Let's say the possible set of all actions that it could take is, let's say, a set of  capital A. It should be noted that agents at any point in time could choose amongst this,  let's say, list of possible actions.  Of course, in some situations, your action space does not necessarily need to be a finite  space."
  },
  "Reinforcement learning": {
    "summary": "Reinforcement learning is a bit unique because it has one more component here in addition  to these other components, which is called the reward. The reward is a feedback by which we measure or we can try to measure the success of a particular agent in its environment. From a given state, an agent can send out any form of actions to take some decisions. Those actions may or may not result in rewards being collected and accumulated over time. It's also very important to remember that not all actions result in immediate rewards.",
    "start_time": 511.32,
    "end_time": 643.82,
    "full_text": " Maybe you could take actions in a continuous space.  For example, when you're driving a car, you're taking actions on a continuous angle space  of what angle you want to steer that car.  It's not necessarily just going right or left or straight.  You may steer at any continuous degree.  Observations is essentially how the environment responds back to the agent.  The environment can tell the agent what it should be seeing based on those actions that  it just took.  It responds in the form of what is called a state.  A state is simply a concrete and immediate situation that the agent finds itself in.  At that particular moment.  It's important to remember that unlike other types of learning that we've covered in this  course, reinforcement learning is a bit unique because it has one more component here in addition  to these other components, which is called the reward.  Now the reward is a feedback by which we measure or we can try to measure the success of  a particular agent in its environment.  For example, in a video game, when Mario grabs a coin, for example, he wins points.  From a given state, an agent can send out any form of actions to take some decisions.  Those actions may or may not result in rewards being collected and accumulated over time.  It's also very important to remember that not all actions result in immediate rewards.  You may take some actions that will result in a reward in a delayed fashion.  Maybe in a few time steps down the future or maybe in life, maybe years.  You may take an action today that results in a reward many some time from now.  Essentially, all of these try to effectively evaluate some way of measuring the success  of a particular action that an agent takes.  For example, when we look at the total reward that an agent accumulates over the course of  its lifetime, we can simply sum up all of the rewards that an agent gets after a certain  time t.  So this capital R of t is the sum of all rewards from that point on into the future, into  infinity."
  },
  "Reinforcement Learning and Discounted Sum": {
    "summary": "The discounted sum is the sum of all of the rewards. It's rewarded time t plus the reward time  t plus one plus t plus two and so on and so  forth. And that can be expanded to look exactly like this. In reinforcement learning, this special function called the  Q function, which ties in a lot of these different components that I've just shared with  you all together. Now let's look at what this Q function is, right?",
    "start_time": 643.82,
    "end_time": 759.5799999999999,
    "full_text": " And that can be expanded to look exactly like this.  It's rewarded time t plus the reward time t plus one plus t plus two and so on and so  forth.  Often it's actually very useful for all of us to consider not only the sum of all of  these rewards, but instead what's called the discounted sum.  So you can see here, I've added this gamma factor in front of all of the rewards and that  discounting factor is essentially multiplied by every future reward that the agent sees  and is discovered by the agent.  And the reason that we want to do this is actually this dampening factor is designed to make  future rewards essentially worth less than rewards that we might see at this instant,  right at this moment right now.  You can think of this as basically enforcing some kind of short term, uh, uh,  agreediness in the algorithm, right?  So for example, if I offered you a reward of $5 today or a reward of $5 in 10 years  from now, I think all of you would prefer that $5 today simply because we have that same  discounting factor applied to this processing, right?  We have that factor that that $5 is not worth as much to us if it's given to us 10 years  in the future.  And that's exactly how this is captured here as well mathematically.  This discounting factor is like multiple, like I said, multiplied it every single future  reward exponentially.  And it's important to understand that also typically this discounting factor is, you know,  between zero and one.  There are some exceptional cases where maybe you want some strange behaviors and they  have a discounting factor greater than one, but in general, that's not something we're  going to be talking about today.  Now finally, it's very important in reinforcement learning, this special function called the  Q function, which ties in a lot of these different components that I've just shared with  you all together.  Now let's look at what this Q function is, right?"
  },
  "How to build a Q function": {
    "summary": "The Q function takes as input two different things. The first is the state that you're currently in. The second is a possible action that you could execute in this particular state. The Q function of these two pieces is going to denote or capture what  the expected total return would be of that agent. If you had access to this type of function, this Q function, I think you could actually  perform a lot of tasks right off the bat, right?  So if you wanted to, for example, understand how to what actions to take in a particular state.  Does anyone have any ideas of how you could transform that Q function to directly infer  what action should be taken?",
    "start_time": 759.5799999999999,
    "end_time": 871.3000000000001,
    "full_text": " So we already covered this R of T function, right?  R of T is the discounted sum of rewards from time T all the way into the future, time  infinity.  But remember that this R of T, right, it's discounted number one and number two, we're going  to try and build a Q function that captures the maximum or the best action that we could  take that will maximize this reward.  So let me say that one more time in a different way.  The Q function takes as input two different things.  The first is the state that you're currently in.  And the second is a possible action that you could execute in this particular state.  So here, S of T is that state at time T, A of T is that action that you may want to take  at time T, and the Q function of these two pieces is going to denote or capture what  the expected total return would be of that agent if it took that action in that particular  state.  Now one thing that I think maybe we should all be asking ourselves now is this seems like  a really powerful function, right?  If you had access to this type of function, this Q function, I think you could actually  perform a lot of tasks right off the bat, right?  So if you wanted to, for example, understand how to what actions to take in a particular  state.  And let's suppose I gave you this magical Q function.  Does anyone have any ideas of how you could transform that Q function to directly infer  what action should be taken?  Yep.  Given a state, you can look at your possible action space and pick the one that gives you  the highest Q values.  Exactly.  So that's exactly right.  So just to repeat that one more time, the Q function tells us for any possible action,  right?"
  },
  "What is a Q function?": {
    "summary": "If we wanted to take a specific action given a specific state, ultimately we need to figure out which action is the best action. The way we do that from a Q function is simply to pick the action that will maximize our  future reward. And we can simply pry out number one. In reality, we're not given a Q function. We have to learn that Q function using deep learning. And broadly speaking, there are two classes of reinforcement learning algorithms that we're going to briefly touch on as part of today's lecture.",
    "start_time": 871.3000000000001,
    "end_time": 973.9,
    "full_text": " What is the expected reward for that action to be taken?  So if we wanted to take a specific action given a specific state, ultimately we need to  figure out which action is the best action.  The way we do that from a Q function is simply to pick the action that will maximize our  future reward.  And we can simply pry out number one.  If we have a discrete action space, we can simply try out all possible actions, compute  their Q value for every single possible action based on the state that we currently find  ourselves in.  And then we pick the action that is going to result in the highest Q value.  If we have a continuous action space, maybe we do something a bit more intelligent, maybe  following the gradients along this Q value curve and maximizing it as part of an optimization  procedure.  But generally, in this lecture, what I want to focus on is actually how we can obtain this  Q function to start with, right?  I kind of skipped a lot of steps in that last slide where I just said, let's suppose I  give you this magical Q function.  How can you determine what action to take?  But in reality, we're not given that Q function.  We have to learn that Q function using deep learning.  And that's what today's lecture is going to be talking about primarily.  First of all, how can we construct and learn that Q function from data?  And then, of course, the final step is use that Q function to take some actions in the  real world.  And broadly speaking, there are two classes of reinforcement learning algorithms that  we're going to briefly touch on as part of today's lecture.  The first class is what's going to be called value learning.  And that's exactly this process that we've just talked about.  The second class of algorithms, which we'll touch on right at the end of today's lecture,  is kind of a different framing of the same approach."
  },
  "Q learning and policy learning": {
    "summary": "Instead of first optimizing the Q function and finding the Q value and then using that  Q function to optimize our value, we're going to find the first class of algorithms. If we do that, if we can obtain this function, right, then we can directly sample from that  policy distribution to obtain the optimal action. Atari Breakout is a game in which the player tries to break off all of the rainbow pixels on the screen. The Q function tells us, you know, the expected total return, or the total reward,  that we can expect based on a given state. Even for us, as humans, to understand what the Q value should be is sometimes quite unintuitive, right?  So here's one example. Let's say we find these two state action pairs. on pair that we may find ourselves in this game.",
    "start_time": 973.9,
    "end_time": 1109.9,
    "full_text": " But instead of first optimizing the Q function and finding the Q value and then using that  Q function to optimize our value, we're going to find the first class of algorithms.  And then, we're going to find the first class of algorithms that we're going to find  to take, but instead of first optimizing the Q function and finding the Q value and then  using that Q function to optimize our actions, what if we just try to directly optimize our  policy, which is what action to take based on a particular state that we find ourselves  in?  If we do that, if we can obtain this function, right, then we can directly sample from that  policy distribution to obtain the optimal action.  We'll talk more details about that later in the lecture.  First, let's cover this first class of approaches, which is Q learning approaches, and we'll build up  that intuition and that knowledge onto the second part of policy learning.  So maybe let's start by just digging a bit deeper into the Q function specifically,  just to start to understand how we could estimate this in the beginning.  So first, let me introduce this game.  Maybe some of you recognize this.  This is the game of called Atari Breakout.  The game here is essentially one where the agent is able to move left or right, this paddle on the  bottom, left or right, and the objective is to move it in a way that this ball that's coming down  towards the bottom of the screen can be bounced off of your paddle, reflected back up, and essentially  you want to break out, right, reflect that ball back up to the top of the screen towards the rainbow  portion, and keep breaking off.  Every time you hit a pixel on the top of the screen, you break off that pixel.  The objective of the game is to basically eliminate all of those rainbow pixels, right?  So you want to keep hitting that ball against the top of the screen until you remove all the pixels.  Now, the Q function tells us, you know, the expected total return, or the total reward,  that we can expect based on a given state and action pair that we may find ourselves in this game.  Now, the first point I want to make here is that sometimes, even for us, as humans,  to understand what the Q value should be is sometimes quite unintuitive, right?  So here's one example. Let's say we find these two state action pairs, right?"
  },
  "What's the difference between A and B?": {
    "summary": "A and B are two different options that we can be presented with in this game. Can you imagine, you know, which of these two options might have a higher Q value for the network?  Which one would result in a greater reward for the neural network or for the agent? B bounces up, hitting one point at a time from the top,  and breaking off very slowly the board that you can see here. A is a very conservative action. It will achieve a good reward. In fact, it solves the game exactly like this right here.",
    "start_time": 1109.9,
    "end_time": 1234.9,
    "full_text": " Here is A and B, two different options that we can be presented with in this game.  A, the ball is coming straight down towards us. That's our state.  Our action is to do nothing and simply reflect that ball back up vertically up.  The second situation, the state is basically that the ball is coming slightly at an angle,  we're not quite underneath it yet, and we need to move towards it and actually hit that ball in a way that, you know,  we'll make it and not miss it, hopefully, right?  So hopefully that ball doesn't pass below us, then the game would be over.  Can you imagine, you know, which of these two options might have a higher Q value for the network?  Which one would result in a greater reward for the neural network or for the agent?  So how many people believe A would result in a higher return?  How about B?  How about someone who picked B? Can you tell me why B?  Why an agency? You're actually doing something?  Okay. Yeah. Have a more?  For A, you only have like the maximum you can take off is like one,  because after you reflect your automatic comes back down,  but then B can bounce around, and there's more than that, what happens?  Exactly. And actually, there's a very interesting thing.  So when I first saw this, actually, it was very unintuitive for me.  Why A is actually working much worse than B, but in general,  this very conservative action of B, you're kind of exactly like you said,  the two answers we're implying, is that A is a very conservative action.  You're kind of only going up and down.  It will achieve a good reward. It will solve the game, right?  In fact, it solves the game exactly like this right here.  You can see, in general, this action is going to be quite conservative.  It's just bouncing up, hitting one point at a time from the top,  and breaking off very slowly the board that you can see here.  But in general, you see the part of the board that's being broken off  is towards the center of the board, right?"
  },
  "How to train a neural network to learn the Q": {
    "summary": "The algorithm, the agent, can actually learn that hitting the side of the board can have some kind of unexpected consequences. So that was a kind of a trick that this neural network learned,  which was a way that it even moves away from the ball as it's coming down. If we have the Q function,  we can directly use it to determine what is the best action that we can take  in any given state that we find ourselves in. So the question naturally is, how can we train a neural network that can, indeed,  learn this Q function?",
    "start_time": 1234.9,
    "end_time": 1333.9,
    "full_text": " Not much on the edges of the board.  If you look at B now, with B, you're kind of having agency,  like one of the answers said.  You're coming towards the ball, and what that implies is that you're sometimes  going to actually hit the corner of your paddle and have a very extreme angle  on your paddle and hit the sides of the board as well.  And it turns out that the algorithm, the agent, can actually learn that  hitting the side of the board can have some kind of unexpected consequences  that look like this.  So here you see it trying to enact that policy.  It's targeting the sides of the board.  But once it reaches a breakout on the side of the board,  it found this hack in the solution.  We're now breaking off a ton of points.  So that was a kind of a trick that this neural network learned,  which was a way that it even moves away from the ball as it's coming down  just so it could move back towards it, just to hit it on the corner  and execute on those corner parts of the board and break out a lot of pieces  for free, almost.  So now that we can see that sometimes obtaining the Q function can be a little bit  unintuitive, but the key point here is that if we have the Q function,  we can directly use it to determine what is the best action that we can take  in any given state that we find ourselves in.  So now the question naturally is, how can we train a neural network that can, indeed,  learn this Q function?  So the type of the neural network here naturally, because we have a function  that takes us input two things, let's imagine our neural network will also take as input  these two objects as well.  One object is going to be the state of the board.  You can think of this as simply the pixels that are on the screen describing that board."
  },
  "How to train a Neural Network": {
    "summary": "The goal here is to estimate the single number output that measures what is the expected value  or the expected q value of this neural network at this particular state action pair. So in this case, the actions that a neural network or an agent could take in this game is to move to the right to the left to stay still. Those could be three different actions that could be provided and parameterized  to the input of a neuralNetwork. If you only provided an input of your state, and as output, you gave it, let's say, all n different q values, one q value for every single possible action. That way you only need to run your neural network once for the given state. And then that neural network will tell you for all possible actions, what's the maximum? Best case scenario just to start with. How an agent would perform ideally in a particular situation. This would mean that essentially the target return, right, the predicted or the value that we're trying to predict, the target is going to always be maximized.",
    "start_time": 1333.9,
    "end_time": 1487.9,
    "full_text": " So it's an image of the board at a particular time.  Maybe you want to even provide two or three images to give it some sense of temporal information  and some past history as well, but all of that information can be combined together  and provided to the network in the form of a state.  And in addition to that, you may also want to provide it some actions as well.  So in this case, the actions that a neural network or an agent could take in this game  is to move to the right to the left to stay still.  And those could be three different actions that could be provided and parameterized  to the input of a neural network.  The goal here is to estimate the single number output that measures what is the expected value  or the expected q value of this neural network at this particular state action pair.  Now, oftentimes what you'll see is that if you wanted to evaluate, let's suppose,  a very large action space.  It's going to be very inefficient to try the approach on the left with a very large action space.  Because what it would mean is that you'd have to run your neural network forward many different times.  One time for every single element of your action space.  So what if instead you only provided an input of your state.  And as output, you gave it, let's say, all n different q values, one q value for every single possible action.  That way you only need to run your neural network once for the given state that you're in.  And then that neural network will tell you for all possible actions, what's the maximum?  You'd simply then look at that output and pick the action that has the kai sq value.  Now, what would happen? Right. So actually, the question I want to pose here is really, you know,  we want to train one of these two networks. Let's stick with the network on the right for simplicity,  just since it's a much more efficient version of the network on the left.  And the question is, you know, how do we actually train that network on the right?  And specifically, I want all of you to think about really the best case scenario just to start with.  How an agent would perform ideally in a particular situation or what would happen, right, if an agent took all of the ideal actions at any given state.  This would mean that essentially the target return, right, the predicted or the value that we're trying to predict, the target is going to always be maximized.  And this can serve as essentially the ground truth to the agent.  Now, for example, to do this, we want to formulate a loss function that's going to essentially represent our expected return if we're able to take all of the best actions."
  },
  "How to train a deep neural network": {
    "summary": "If we select an initial reward plus selecting some action in our action space that maximizes our expected return,  then for the next future state, we need to apply that discounting factor and recursively apply the same equation. That simply turns into our target, right. Now we can ask basically what does our neural network predict. Deep neural network that we're trying to train looks like this. It takes us input a state is trying to output and different numbers. Different numbers correspond to the q value associated to end different actions, one q value per action. Policy function is a function that given a state, it determines what is the best action. It's a very end to end way of thinking about, you know, the agents decision making process based on what I see. The optimal action is simply going to be the maximum of these three q values. value of zero, it's going to basically die after this iteration. If it moves to the right, because you can see that the ball is coming to the left of it, the most right, the game is over, right. Google DeepMind showed that you could train a q value network for Atari games. kes a few time steps for that ball to travel back up to the top of the screen. Usually your rewards will be quite delayed, maybe at least by several time steps. The algorithm was able to surpass human level performance on over half of the games. It was a very simple algorithm. You only had to let it explore its environment play the game many, many times against itself and learn directly from that data.",
    "start_time": 1487.9,
    "end_time": 1846.9,
    "full_text": " Right. So, for example, if we select an initial reward plus selecting some action in our action space that maximizes our expected return,  then for the next future state, we need to apply that discounting factor and recursively apply the same equation.  And that simply turns into our target, right. Now we can ask basically what does our neural network predict, right.  So that's our target and recall from previous lectures, if we have a target value, in this case, our q value is a continuous variable,  we have also a predicted variable that is going to come as part of the output of every single one of these potential actions that could be taken.  We can define what's called a q loss, which is essentially just a very simple mean squared error loss between these two continuous variables.  We minimize their distance over two over many, many different iterations of flying our neural network in this environment, observing actions and observing not only the actions, but most importantly,  after the action is committed or executed, we can see exactly the ground truth expected return, right.  So we have the ground truth labels to train and supervise this model directly from the actions that were executed as part of random selection, for example.  Now, let me just stop right there and maybe summarize the whole process one more time and maybe a bit different terminology just to give everyone kind of a different perspective on this same problem.  So our deep neural network that we're trying to train looks like this, right. It takes us input a state is trying to output and different numbers, those end different numbers correspond to the q value associated to end different actions, one q value per action.  Here, the actions in the tary breakout, for example, should be three actions, we can either go left, we can go right, or we can do nothing, we can stay where we are.  Right. So the next step from this, we saw if we have this q value output, what we can do with it is we can make an action, or we can even, let me be more formal about it, we can develop what's called a policy function.  Policy function is a function that given a state, it determines what is the best action. So that's different than the q function, right. The q function tells us given a state, what is the best, or what is the value, the return of every action that we could take.  The policy function tells us one step more than that given given a state, what is the best action, right. So it's a very end to end way of thinking about, you know, the agents decision making process based on what I see right now, what is the action that I should take.  And we can determine that policy function directly from the q function itself simply by maximizing and optimizing all of the different q values for all of the different actions that we see here.  So for example, here we can see that given this state, the q function has the results of these three different values as a q value of 20, if it goes to the left, as a q value of three, if it stays in the same place, and it has a q value of zero, it's going to basically die after this iteration.  If it moves to the right, because you can see that the ball is coming to the left of it, the most right, the game is over, right. So it needs to move to the left in order to do that in order to continue the game and the q value reflects that.  The optimal action here is simply going to be the maximum of these three q values. In this case, it's going to be 20 and then the action is going to be the corresponding action that comes from that 20, which is moving left.  Now, we can send this action back to the environment in the form of the game to execute the next step, right. And as the agent moves through this environment, it's going to be responded with not only by new pixels that come from the game, but more importantly, some reward signal.  Now, it's very important to remember that the reward signals in a Tari breakout are very sparse, right. You get a reward not necessarily based on the action that you take at this exact moment.  It usually takes a few time steps for that ball to travel back up to the top of the screen. So usually your rewards will be quite delayed, maybe at least by several time steps, sometimes even more if you're bouncing off of the corners of the screen.  Now, one very popular or very famous approach that showed this was presented by DeepMind, Google DeepMind several years ago, where they showed that you could train a q value network.  And you can see the input on the left hand side is simply the raw pixels coming from the screen all the way to the actions of a controller on the right hand side.  And you could train this one network for a variety of different tasks all across the Atari breakout ecosystem of games. And for each of these tasks, the really fascinating thing that they showed was for this very simple algorithm, which really relies on random choice of selection of actions.  And then, you know, learning from, you know, actions that don't do very well that you discourage them and trying to do actions that did perform well more frequently, very simple algorithm.  But what they found was even with that type of algorithm, they were able to surpass human level performance on over half of the game.  There were some games that you can see here were still below human level performance. But as we'll see, this was really like such an exciting advance because of the simplicity of the algorithm.  And how you know, clean the formulation of the training was you only needed a very little amount of prior knowledge to impose on to this neural network where to be able to learn how to play these games.  You never had to teach you any of the rules of the game, right? You only had to let it explore its environment play the game many, many times against itself and learn directly from that data."
  },
  "QLearning and Policy Learning": {
    "summary": "QLearning is naturally applicable to discrete action spaces, right?  Because you can think of this output space that we're providing is kind of like one number per action that could be taken. Now, if we have a continuous action space, we have to think about clever ways to work around that. The next part of today's lecture is going to be focused on policy learning. Policy learning is a different class of reinforcement learning algorithms that are different than QLearning algorithms. The main difference is that instead of trashing the results of the Q function, policy grading algorithms. We're trying to build a neural network that will directly learn that policy function from the data. We're calling the policy function pi of s, right.  So pi is the policy s is our state. It's a function that takes as input only the state and it's going to directly output the action. The probability that selecting that action would be the highest value, right. Exact value itself doesn't matter. You only care about if selecting this action is going to give you with high likelihood the best one. The policy network has a very formulated output all of the numbers here in the output have to sum to one because this is a probability distribution. That gives it a very rigorous version of how we can train this model that makes it a bit easier to train than q functions. In the generative lecture, we saw how VIEs could be used to predict Gaussian distributions over their latent space. And just like that, we could also use this same formulation to move beyond discrete action spaces, like you can see here.",
    "start_time": 1846.9,
    "end_time": 2216.9,
    "full_text": " Now, there are several very important downsides of QLearning and hopefully these are going to motivate the second part of today's lecture, which we'll talk about.  But the first one that I want to really convey to everyone here is that QLearning is naturally applicable to discrete action spaces, right?  Because you can think of this output space that we're providing is kind of like one number per action that could be taken.  Now, if we have a continuous action space, we have to think about clever ways to work around that. In fact, there are now more recently, there are some solutions to achieve QLearning and continuous action spaces.  But for the most part, QLearning is very well suited for discrete action spaces and we'll talk about ways of overcoming that with other approaches a bit later.  And the second component here is that the policy that we're learning, right, the Q function is giving rise to that policy, which is the thing that we're actually using to determine what action to take given any state.  That policy is determined by, you know, deterministically optimizing that Q function. We simply look at the results from the Q function and apply our, or we look at the results of the Q function and we pick the action that has the best or the highest Q value.  That is very dangerous in many cases because of the fact that it's always going to pick the best value for a given state. There's no stochasticity in that pipeline.  So you can very frequently get caught in situations where you keep repeating the same actions and you don't learn to explore potentially different options that you may be thinking of.  So to address these very important challenges, that's hopefully going to motivate now the next part of today's lecture, which is going to be focused on policy learning, which is a different class of reinforcement learning algorithms that are different than QLearning algorithms.  And like I said, those are called policy grading algorithms and policy grading algorithms. The main difference is that instead of trying to infer the policy from the Q function, we're just going to build a neural network that will directly learn that policy function from the data, right.  So it kind of skips one step and we'll see how we can train those networks.  So before we get there, let me just revisit one more time the Q function illustration that we're looking at, right.  Q function, we're trying to build a neural network outputs these Q values, one value per action, and we determine the policy by looking over this state of Q values, picking the value that has the highest and looking at its corresponding action.  Now with policy networks, the idea that we want to keep here is that instead of predicting the Q values themselves, let's directly try to optimize this policy function here. We're calling the policy function pi of s, right.  So pi is the policy s is our state. So it's a function that takes as input only the state and it's going to directly output the action.  So the outputs here give us the desired action that we should take in any given state that we find ourselves in.  And that represents not only the best action that we should take, but let's denote this as basically the probability that selecting that action would result in a very desirable outcome for our network.  So not necessarily the value of that that action, but rather the probability that selecting that action would be the highest value, right.  So you don't care exactly about what is the numerical value that selecting this action takes or gives rise to rather, but rather what is the likelihood that selecting this action will give you the best performing value that you could expect.  Exact value itself doesn't matter. You only care about if selecting this action is going to give you with high likelihood the best one.  So we can see that if these predicted probabilities here, right, in this example of Atari, right, going left has the probability of being the highest value action with 90% staying in the center.  That's a probability of 10% going right is 0%. So ideally, what our neural networks should do in this case is 90% of the time in this situation go to the left 10% of the time, it could still try staying where it is, but never it should go to the right.  Now note that this now is a probability distribution. This is very different than a q function. A q function has actually no structure, right. The q values themselves can take any real number, right.  But here the policy network has a very formulated output all of the numbers here in the output have to sum to one because this is a probability distribution, right.  And that gives it a very rigorous version of how we can train this model that makes it a bit easier to train than q functions as well.  So one other very important advantage of having an output that is a probability distribution is actually going to tie back to this other issue of q functions and q neural networks that we saw before.  And that is the fact that q functions are naturally suited towards discrete action spaces. Now when we're looking at this policy network, we're outputting a distribution, right.  And remember those distributions can also take continuous forms. In fact, we've seen this in the last two lectures, right. In the generative lecture, we saw how VIEs could be used to predict Gaussian distributions over their latent space.  And the last lecture, we also saw how we could learn to predict uncertainties, which are continuous probability distributions using data. And just like that, we could also use this same formulation to move beyond discrete action spaces, like you can see here, which are one possible action, a probability associated to one possible action in a discrete set of possible actions."
  },
  "Learning to Parameterize Action Spaces": {
    "summary": "The question is not what action should I take go left, right, or send center, but rather how quickly should I move. That is a continuous variable as opposed to a discrete variable. And you could say that now the answer should look like this, right. Moving very fast to the right versus very slow to the left has this continuous spectrum that we may want to model. In the continuous domain, we can't simply predict a single probability for every possible action, because there is an infinite number of them. So instead, what if we parameterized our action space by distribution, right, so let's take for example the Gaussian distribution. To parameterize a Gaussian. distribution, we only need two outputs,right, we need a mean and a variance. Given the mean and a variance, we can actually have a probability mass and we can compute a probability over any possible action that we may want to take just from those two numbers. So for example, in this image here, we may. want to output a Gaussian that looks like this, right, its mean is centered at, let's see, negative 0.8 indicating that we should move basically left with a speed of 0. 8 meters per second. The agent here is the vehicle, right, the state could be obtained through many sensors that could be mounted on the vehicle itself. The action that we could take is a steer angle, this is a continuous variable, it's actually an angle that could take any real number. The reward in this very simplistic example is the distance that we travel before we crash. Reinforcement learning is almost like a data acquisition plus learning pipeline combined together. So we start our vehicle, our agent, and in the beginning, of course, it knows nothing about driving. It's never been exposed to any of these rules of the environment. We can assume that all of the actions that occurred in the later half of this trajectory were probably not very good actions because they came very close to termination. So let's decrease the probability of all of those things happening again in the future. And we'll increase the probabilities of actions that resulted in low rewards. Re-initialize the agent one more time and run it until completion. Now the agent goes a bit farther, right? Because you've decreased the probabilities at the end, increased the probabilities of the future.",
    "start_time": 2216.9,
    "end_time": 2614.9,
    "full_text": " Now we may have a space, which is not what action should I take go left, right, or send center, but rather how quickly should I move and what direction should I move, right. That is a continuous variable as opposed to a discrete variable. And you could say that now the answer should look like this, right.  Moving very fast to the right versus very slow to the or excuse me, very fast to the left versus very slow to the left has this continuous spectrum that we may want to model.  Now when we plot this entire distribution of taking an action, giving a state, you can see basically a very simple illustration of that right here. This distribution has most of its mass over, or sorry, it has all of its mass over the entire real number line, first of all, it has most of its mass, right, in the optimal action space that we want to take.  So if we want to determine the best action to take, we would simply take the mode of this distribution, right, the highest point, that would be the speed at which we should move and the direction that we should move in.  If we wanted to also try out different things and explore our space, we could sample from this distribution and still obtain some stochasticity.  Now let's look at an example of how we could actually model these continuous distributions and actually we've already seen some examples of this in the previous two lectures, like I mentioned, but let's take a look specifically in the context of reinforcement learning and policy gradient learning.  So instead of predicting this probability of taking an action, giving all possible states, which in this case there is now an infinite number of, because we're in the continuous domain, we can't simply predict a single probability for every possible action, because there is an infinite number of them.  So instead, what if we parameterized our action space by distribution, right, so let's take for example the Gaussian distribution.  To parameterize a Gaussian distribution, we only need two outputs, right, we need a mean and a variance, given the mean and a variance, we can actually have a probability mass and we can compute a probability over any possible action that we may want to take just from those two numbers.  So for example, in this image here, we may want to output a Gaussian that looks like this, right, its mean is centered at, let's see, negative 0.8 indicating that we should move basically left with a speed of 0.8 meters per second, for example.  And again, we can see that because this is a probability distribution, because of the format of policy networks, right, we're enforcing that this is a probability distribution, that means that the integral now of this of this outputs, right, by definition of it being a Gaussian must also integrate to 1.  Okay, great. So now let's maybe take a look at how policy gradient networks can be trained and you know step through that process as well as we look at a very concrete example and maybe let's start by just revisiting this reinforcement learning loop that we've started this class with.  Now, let's specifically consider the example of training an autonomous vehicle since I think that this is a particularly very intuitive example that we can walk through the agent here is the vehicle, right, the state could be obtained through many sensors that could be mounted on the vehicle itself.  So, for example, autonomous vehicles are typically equipped with sensors like cameras, light hours, radars, etc. All of these are giving observational inputs to the, to the vehicle.  The action that we could take is a steer angle, this is not a discrete variable, this is a continuous variable, it's actually an angle that could take any real number.  And finally, the reward in this very simplistic example is the distance that we travel before we crash.  So now let's take a look at how we could train a policy gradient neural network to solve this task of self driving cars as a concrete example.  So we start by initializing our agent, right, remember that we have no training data, right. So we have to think about actually reinforcement learning is almost like a data acquisition plus learning pipeline combined together.  So the first part of that data acquisition pipeline is first to initialize our agent to go out and collect some data.  So we start our vehicle, our agent, and in the beginning, of course, it knows nothing about driving. It's never been exposed to any of these rules of the environment or the observation before.  So it runs its policy, which right now is untrained entirely until it terminates, right, until it goes outside of some bounds that we define, we measure basically the reward as the distance that we traveled before it terminated.  And we record all of the states, all of the actions, and the final reward that it obtained until that termination, right.  This becomes our mini data set that we'll use for the first round of training.  Let's take those data sets and now we'll do one step of training. The first step of training that we'll do is to take, excuse me, to take the later half of our trajectory that our agent ran and decrease the probability of actions that resulted in low rewards.  Now, because the vehicle, we know the vehicle terminated, we can assume that all of the actions that occurred in the later half of this trajectory were probably not very good actions because they came very close to termination.  So let's decrease the probability of all of those things happening again in the future and we'll take all of the things that happened in the beginning half of our training episode.  And we will increase their probabilities. Now, again, there's no reason why there shouldn't necessarily be a good action that we took in the first half of this trajectory and a bad action in the later half.  But it's simply because actions that are in the later half were closer to a failure and closer determination that we can assume, for example, that these were probably suboptimal actions.  But it's very possible that these are noisy rewards as well because it's such a sparse signal. It's very possible that you had some good actions at the end and you were actually trying to recover your car, but you were just too late.  Now, repeat this process again. Re-initialize the agent one more time and run it until completion. Now the agent goes a bit farther, right? Because you've decreased the probabilities at the end, increased the probabilities of the future."
  },
  "How to update a policy gradient neural network": {
    "summary": "Self-driving car learned entirely just by going out, crashing a lot, and trying to figure out what to do to not keep doing that in the future. How can we basically formulate that same algorithm and specifically the update equation steps four and five right here? The loss function for a policy gradient neural network looks like this. The loss consists of two terms. The first term is this term in green, just called the log likelihood of selecting a particular action. The second term is something that all of you are very familiar with already. This is simply the return on the words that you would get after this time point. In our simplified example on the car example, all the things that had low rewards were exactly those actions that came closest to the termination part of the of the vehicle. That's just the assumption that we make when defining our reward structure. We can plug this into the loss of gradient descent algorithm to train our neural network. This method gets its name from this policy gradient piece that you can see here. That's the probability of selecting an action given a specific state. Now, I want to take maybe just a very brief second towards the end of the class here just to talk about some of the challenges. In reality, crashing a car a bunch of times just to learn how to not crash it is not feasible. Simulation is very safe because, you know, we're not going to actually be damaging anything real. Modern simulation engines for reinforcement learning and generally very broadly speaking, modern simulators for vision specifically do not at all capture reality. MIT researchers have been developing a new type of simulation engine. Called photorealistic, it can simulate the phenomena that we see in the real world. It can be used to train reinforcement learning algorithms in a more realistic way. MIT develops photorealistic simulation engine. This is actually an autonomous agent, not a real car, driving through our virtual simulator. It allows us to basically use real data that we do collect in the real world, but then re-simulate those same real roads.",
    "start_time": 2614.9,
    "end_time": 3026.9,
    "full_text": " And you keep repeating this over and over again until you notice that the agent learns to perform better and better every time until it finally converges. And at the end, the agent is able to basically follow lanes, usually swarving a bit side to side while it does that, without crashing.  And this is actually really fascinating because this is a self-driving car that we never taught anything about what a lane marker means or what are the rules of the road, anything about that, right?  This was a car that learned entirely just by going out, crashing a lot, and trying to figure out what to do to not keep doing that in the future.  And the remaining question is actually how we can update that policy as part of this algorithm that I'm showing you on the left-hand side. How can we basically formulate that same algorithm and specifically the update equation steps four and five right here? These are the two really important steps of how we can use those two steps to train our policy and decrease the probability of bad events while promoting these likelihoods of all these good events.  So let's assume the, let's look at the loss function, first of all, the loss function for a policy gradient neural network looks like this and then we'll start by dissecting it to understand why this works the way it does.  So here we can see that the loss consists of two terms. The first term is this term in green, just called the log likelihood of selecting a particular action.  The second term is something that all of you are very familiar with already. This is simply the return at a specific time, right? So that's the expected return on the words that you would get after this time point.  Now, let's assume that we got a lot of reward for a particular action that had a high log probability or a high probability.  Right, if we got a lot of reward for a particular action that had high probability, that means that we want to increase that probability even further. So we do it even more or even more likelihood, we sampled that action again into the future.  On the other hand, if we selected or let's say if we obtained a reward that was very low for an action that had high likelihood, we want the inverse effect, right?  We never want to sample that action again in the future because it resulted in a low reward. Right, and you'll notice that this loss function right here by including this negative, we're going to minimize the likelihood of achieving any action that had low rewards in this trajectory.  Now, in our simplified example on the car example, all the things that had low rewards were exactly those actions that came closest to the termination part of the of the vehicle, right? All the things that had high rewards were the things that came in the beginning.  That's just the assumption that we make when defining our reward structure.  Now, we can plug this into the loss of gradient descent algorithm to train our neural network when we see this policy gradient algorithm, which you can see highlighted here.  This gradient is exactly of the policy part of the neural network. That's the probability of selecting an action given a specific state.  And if you remember before when we defined, what does it mean to be a policy function? That's exactly what it means, right? Given a particular state that you find yourself in, what is the probability of selecting a particular action with the highest likelihood?  And that's exactly where this method gets its name from this policy gradient piece that you can see here.  Now, I want to take maybe just a very brief second towards the end of the class here just to talk about some of the challenges and keep you in line with the first lecture today, some of the challenges of deploying these types of algorithms in the context of the real world, right?  What do you think when you look at this training algorithm that you can see here, right? What do you think are the shortcomings of this training algorithm? And which step, I guess specifically, if we wanted to deploy this approach into reality?  Yeah, exactly. So it's step two, right? If you wanted to do this in reality, right? That essentially means that you want to go out, collect your car, crashing it a bunch of times just to learn how to not crash it.  Right? And that's, you know, that's simply not feasible, right? Number one, it's also, you know, very dangerous. Number two.  So there are ways around this, right? The number one way around this is that people try to train these types of models in simulation, right?  Simulation is very safe because, you know, we're not going to actually be damaging anything real. It's still very inefficient because we have to run these algorithms a bunch of times and crash them a bunch of times just learn how not to crash.  But at least now, at least from a safety point of view, it's much safer. But, you know, the problem is that modern simulation engines for reinforcement learning and generally very broadly speaking, modern simulators for vision specifically do not at all capture reality very accurately.  In fact, there's a very famous notion called the sim-to-real gap, which is a gap that exists when you train algorithms in simulation, and they don't extend to a lot of the phenomena that we see and the patterns that we see in reality.  And one really cool result that I want to just highlight here is that when we're training reinforcement learning algorithms, we ultimately want them to be, you know, not operating in simulation.  And we want them to be in reality. And as part of our lab here at MIT, we've been developing this very, very cool brand new photorealistic simulation engine that goes beyond basically the paradigm of how simulators work today, which is basically defining a model of their environment and trying to, you know, synthesize that model.  These simulators are like glorified game engines, right? They all look very game-like when you look at them. But one thing that we've done is taken a data-driven approach using real data of the real world, can we build up synthetic environments that are super photorealistic and look like this?  So this is a cool result that we created here at MIT, developing this photorealistic simulation engine. This is actually an autonomous agent, not a real car, driving through our virtual simulator in a bunch of different types of different scenarios.  So this simulator is called Vista. It allows us to basically use real data that we do collect in the real world, but then re-simulate those same real roads."
  },
  "How Reinforcement Learning Can Be Used in Go": {
    "summary": "We can now train these agents now entirely using reinforcement learning, no human labels, but importantly allow them to be transferred into reality because there's no sim to real gap anymore. So in fact, we did exactly this. We placed agents into our simulator. We trained them using the exact algorithms that you learned about in today's lecture, these policy gradient algorithms. And all of the training was done entirely in simulation. Then we took these policies and we deployed them on board our full scale autonomous vehicle. This is now in the real world, no longer in Simulation. MIT researchers used reinforcement learning to train a policy end to end for an autonomous vehicle that could be deployed in reality. No transfer learning is done here. There is no augmentation of data from real world data. This is entirely trained using simulation. Go is an extraordinarily complex game for an artificial algorithm to try and master. The number of possible action spaces and possible states that this board could be placed into is greater than the number of atoms in the universe. The objective here was to build a reinforcement learning algorithm to master the game. Using a reinforcement learning pipeline, they were able to defeat the grand champion human players. The idea that's core was actually very simple. The first step is that you train a neural network to basically watch human level experts. Auxiliary network was almost hallucinating, right?  Different board  different board states. The losers would try to negate all of the actions that they may have acquired from their human counterparts and try to actually learn new types of rules. A neural network can learn to not only outperform the solution that is created by humans, but also outperforms the solution created by the humans as well. So with that all summarized very quickly what we've learned today and conclude for the day. CNN's John Defterios talks about how to use reinforcement learning to find solutions to problems. He says there are two types of approaches: Q learning and end-to-end approach.",
    "start_time": 3026.9,
    "end_time": 3431.9,
    "full_text": " So for example, let's say you take your car, you drive out on Massab, you collect data of Massab, you can now drop a virtual agent into that same simulated environment observing new viewpoints of what that scene might have looked like from different types of perturbations or types of angles that it might be exposed to.  And that allows us to train these agents now entirely using reinforcement learning, no human labels, but importantly allow them to be transferred into reality because there's no sim to real gap anymore.  So in fact, we did exactly this. We placed agents into our simulator. We trained them using the exact algorithms that you learned about in today's lecture, these policy gradient algorithms.  And all of the training was done entirely in simulation. Then we took these policies and we deployed them on board our full scale autonomous vehicle. This is now in the real world, no longer in simulation.  And on the left hand side, you can see basically this car driving through this environment completely autonomous in the real world. No transfer learning is done here. There is no augmentation of data from real world data.  This is entirely trained using simulation and this represented actually the first time ever that reinforcement learning was used to train a policy end to end for an autonomous vehicle that could be deployed in reality.  So that was something really cool that we created here at MIT. But now that we covered all of this foundations of reinforcement learning and policy learning, I want to touch on some other maybe very exciting applications that we're seeing.  And one very popular application that a lot of people will tell you about and talk about is the game of go. So here reinforcement learning agents could be actually tried to put against the test against, you know, grand master level go players.  And you know, at the time, achieved incredibly impressive results. So for those of you who are not familiar with the game of go, the game of go is played on a 19 by 19 board.  The rough objective of go is to claim basically more board pieces than your opponent, right. And through the grid of, sorry, through the grid that you can see here, this 19 by 19 grid.  And while the game itself, the logical rules are actually quite simple, the number of possible action spaces and possible states that this board could be placed into is greater than the number of atoms in the universe.  So this game, even though the rules are very simple in their logical definitions, is an extraordinarily complex game for an artificial algorithm to try and master.  So the objective here was to build a reinforcement learning algorithm to master the game of go, not only beating, you know, these gold standard softwares, but also what was at the time like an amazing result was to beat the grand master level player.  So the number one player in the world of go was a human, the human champion, obviously.  So Google deep mind rose to this challenge. They created a couple years ago, developing this solution, which is very much based in the exact same algorithms that you learned about in today's lecture, combining both the value part of this network with residual layers, which will cover in the next lecture tomorrow.  And using a reinforcement learning pipeline, they were able to defeat the grand champion human players. And the idea that's core was actually very simple.  The first step is that you train a neural network to basically watch human level experts.  So this is not using reinforcement learning is using supervised learning using the techniques that we covered in lectures one, two, and three.  And from this first step, the goal is to build like a policy that would imitate some of the rough patterns that a human type of player or human grand master would take based on given board state, the type of actions that they might execute.  But then given this pre trained model, essentially, you could use it to bootstrap in reinforcement learning algorithm that would play against itself in order to learn how to improve even beyond the human levels.  So it would take its human understandings, try to imitate the humans first of all, but then from that imitation, they would pin these two neural networks against themselves, play a game against themselves, and the winners would be receiving a reward.  The losers would try to negate all of the actions that they may have acquired from their human counterparts and try to actually learn new types of rules and new types of actions basically that might be very beneficial to achieving super human performance.  And one of the very important auxiliary tricks that brought this idea to be possible was the usage of this second network, this auxiliary network, which took as input the state of the board and tried to predict, you know, what are all of the different possible board states that might emerge from this particular state, and what would their values be?  What would their potential returns and their outcomes be? So this network was an auxiliary network that was almost hallucinating, right?  Different board states that it could take from this particular state and using those predicted values to guide its planning of, you know, what action should it take into the future.  And finally, very much more recently, they extended this algorithm and showed that they could not even use the human grandmasters in the beginning to imitate from in the beginning and bootstrapped these algorithms.  What if they just started entirely from scratch and just had two neural networks never trained before they start pinning themselves against each other, and you could actually see that you could, without any human supervision at all, have a neural network, learn to not only outperform the solution that  or outperform the humans, but also outperform the solution that was created, which was bootstrapped by humans as well.  So with that all summarized very quickly what we've learned today and conclude for the day. So we've talked a lot about really the foundational algorithms underlying reinforcement learning.  We saw two different types of reinforcement learning approaches of how we could optimize these solutions first being Q learning where we're trying to actually estimate given a state, you know, what is the value that we might expect for any possible action in the second way was to take a much more end to end approach and say how given a state that we see ourselves in what is the likelihood that I should take any given action to maximize the potential that I have in this particular state."
  },
  "Lab and Competitions": {
    "summary": "The deadline for these competitions will be well it was originally set to be Thursday, which is tomorrow at 11 p.m.  And I hope that all of this was very exciting to you today we have a very exciting lab.",
    "start_time": 3431.9,
    "end_time": 3449.9,
    "full_text": " And I hope that all of this was very exciting to you today we have a very exciting lab and kick off for the competition and the deadline for these competitions will be well it was originally set to be Thursday, which is tomorrow at 11 p.m. Thank you."
  }
}
